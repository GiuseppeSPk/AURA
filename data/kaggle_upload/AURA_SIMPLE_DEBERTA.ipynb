{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AURA Simple: DeBERTa-v3-base Reporting-Aware Toxicity Detection\n",
    "\n",
    "**Philosophy**: A more powerful model can learn nuances directly.\n",
    "\n",
    "**Architecture**: `microsoft/deberta-v3-base` ‚Üí Binary Classification Head\n",
    "\n",
    "**Dataset**: Unified Toxicity + Reporting (18k samples)\n",
    "- Label 0: Non-toxic OR Reporting (safe content)\n",
    "- Label 1: Toxic direct speech\n",
    "\n",
    "**Goal**: The model should learn that *reporting* toxic content is NOT toxic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & GPU Check\n",
    "import torch\n",
    "print(\"üîß Checking GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU! Enable in Settings ‚Üí Accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üîß Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'microsoft/deberta-v3-base',\n",
    "    'max_length': 128,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5,\n",
    "    'lr': 2e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'patience': 3,\n",
    "}\n",
    "\n",
    "DATA_DIR = '/kaggle/input/aura-deberta-data'\n",
    "\n",
    "print('üìã Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'   {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset Class\n",
    "class ToxicityDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len):\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc = self.tokenizer(\n",
    "            str(row['text']),\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].flatten(),\n",
    "            'attention_mask': enc['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(int(row['label']), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print('üì¶ Dataset class defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Data\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "\n",
    "train_ds = ToxicityDataset(f\"{DATA_DIR}/deberta_unified_train.csv\", tokenizer, CONFIG['max_length'])\n",
    "val_ds = ToxicityDataset(f\"{DATA_DIR}/deberta_unified_val.csv\", tokenizer, CONFIG['max_length'])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print('='*60)\n",
    "print('üìä DATASET SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Training samples: {len(train_ds):,}')\n",
    "print(f'Validation samples: {len(val_ds):,}')\n",
    "\n",
    "# Check distribution\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/deberta_unified_train.csv\")\n",
    "print(f'\\nLabel distribution (train):')\n",
    "print(f'   0 (Safe/Reporting): {(train_df[\"label\"] == 0).sum():,}')\n",
    "print(f'   1 (Toxic Direct):   {(train_df[\"label\"] == 1).sum():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "# Class weights for imbalance\n",
    "n_safe = (train_df['label'] == 0).sum()\n",
    "n_toxic = (train_df['label'] == 1).sum()\n",
    "class_weights = torch.tensor([n_toxic / n_safe, 1.0], device=device, dtype=torch.float32)\n",
    "print(f'‚öñÔ∏è Class weights: {class_weights}')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps * CONFIG['warmup_ratio']),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'\\nüèóÔ∏è Model loaded: {CONFIG[\"model_name\"]}')\n",
    "print(f'   Parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Functions\n",
    "def train_epoch(model, loader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return f1, all_preds, all_labels\n",
    "\n",
    "print('‚úÖ Training functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Loop\n",
    "print('='*60)\n",
    "print('üöÄ TRAINING START')\n",
    "print('='*60)\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    print(f'\\n--- Epoch {epoch}/{CONFIG[\"epochs\"]} ---')\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
    "    val_f1, _, _ = evaluate(model, val_loader)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f} | Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), '/kaggle/working/deberta_best.pt')\n",
    "        print('>>> BEST MODEL SAVED <<<')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'(Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print('üõë Early stopping.')\n",
    "            break\n",
    "\n",
    "print(f'\\nüèÜ Best F1: {best_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Evaluation\n",
    "print('='*60)\n",
    "print('üî¨ FINAL EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "model.load_state_dict(torch.load('/kaggle/working/deberta_best.pt'))\n",
    "f1, preds, labels = evaluate(model, val_loader)\n",
    "\n",
    "print('\\n--- Classification Report ---')\n",
    "print(classification_report(labels, preds, target_names=['Safe/Reporting', 'Toxic']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Safe', 'Toxic'], yticklabels=['Safe', 'Toxic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Qualitative Stress Test - Does it understand Reporting?\n",
    "print('='*80)\n",
    "print('üß™ QUALITATIVE STRESS TEST: REPORTING AWARENESS')\n",
    "print('='*80)\n",
    "\n",
    "test_cases = [\n",
    "    # Reporting (should be SAFE)\n",
    "    (\"He said you are a disgusting person\", \"Safe\", \"Reporting\"),\n",
    "    (\"According to the witness, 'you are an idiot'\", \"Safe\", \"Reporting\"),\n",
    "    (\"She claims he called her a failure\", \"Safe\", \"Reporting\"),\n",
    "    (\"The document records the insult: 'you are worthless'\", \"Safe\", \"Reporting\"),\n",
    "    \n",
    "    # Direct Toxic (should be TOXIC)\n",
    "    (\"You are a disgusting person\", \"Toxic\", \"Direct\"),\n",
    "    (\"I am telling you that you are an idiot\", \"Toxic\", \"Direct\"),\n",
    "    (\"You are a failure and everyone knows it\", \"Toxic\", \"Direct\"),\n",
    "    (\"You are worthless garbage\", \"Toxic\", \"Direct\"),\n",
    "    \n",
    "    # Implicit Hate (should be TOXIC)\n",
    "    (\"People like you belong in a cage\", \"Toxic\", \"Implicit\"),\n",
    "    (\"Your kind is inferior\", \"Toxic\", \"Implicit\"),\n",
    "    \n",
    "    # Non-toxic (should be SAFE)\n",
    "    (\"I hate rainy Mondays\", \"Safe\", \"General\"),\n",
    "    (\"This soup is disgusting\", \"Safe\", \"Object\"),\n",
    "    (\"The movie was terrible\", \"Safe\", \"Opinion\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Text':<50} | {'Expected':<8} | {'Pred':<8} | {'Status'}\")\n",
    "print('-'*80)\n",
    "\n",
    "correct = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text, expected, category in test_cases:\n",
    "        enc = tokenizer(text, max_length=CONFIG['max_length'], padding='max_length',\n",
    "                        truncation=True, return_tensors='pt')\n",
    "        outputs = model(enc['input_ids'].to(device), attention_mask=enc['attention_mask'].to(device))\n",
    "        pred_idx = outputs.logits.argmax(dim=1).item()\n",
    "        pred_label = 'Toxic' if pred_idx == 1 else 'Safe'\n",
    "        \n",
    "        status = '‚úÖ' if pred_label == expected else '‚ùå'\n",
    "        if pred_label == expected:\n",
    "            correct += 1\n",
    "        \n",
    "        print(f\"{text[:48]:<50} | {expected:<8} | {pred_label:<8} | {status}\")\n",
    "\n",
    "print('-'*80)\n",
    "print(f'\\nüéØ Stress Test Accuracy: {correct}/{len(test_cases)} ({correct/len(test_cases)*100:.1f}%)')\n",
    "print('\\nüìù Key Question: Does it correctly identify REPORTING as SAFE?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Save Artifacts\n",
    "print('üíæ Saving artifacts...')\n",
    "\n",
    "import json\n",
    "with open('/kaggle/working/deberta_history.json', 'w') as f:\n",
    "    json.dump(history, f)\n",
    "\n",
    "print('‚úÖ Model: /kaggle/working/deberta_best.pt')\n",
    "print('‚úÖ History: /kaggle/working/deberta_history.json')\n",
    "print(f'\\nüèÜ Final Best F1: {best_f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true,
   "isInternetEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
