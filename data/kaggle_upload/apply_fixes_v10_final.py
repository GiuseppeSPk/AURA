import json
import os

NOTEBOOK_PATH = r'c:\Users\spicc\Desktop\Multimodal\AURA\kaggle_upload\AURA_V10_KAGGLE.ipynb'

# 1. NEW SOURCE FOR CELL 11 (Fixed with Loss Tracking + Robust Metrics)
CELL_11_SOURCE = [
    "# Cell 11: Training & Validation Loop (FIXED V10.2)\n",
    "def evaluate(loader, task_id, task_name):\n",
    "    \"\"\"Evaluate with detailed per-class metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids, mask = batch['ids'].to(device), batch['mask'].to(device)\n",
    "            out = model(ids, mask)\n",
    "            \n",
    "            if task_id == 0:  # Toxicity\n",
    "                preds = torch.argmax(out['toxicity'], dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(batch['tox'].numpy())\n",
    "            elif task_id == 3:  # Reporting\n",
    "                preds = (torch.sigmoid(out['reporting'].squeeze(-1)) > 0.5).int()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_targets.extend(batch['rep'].numpy())\n",
    "    \n",
    "    if not all_preds: return 0.0\n",
    "    \n",
    "    f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Detailed per-class breakdown\n",
    "    p, r, f, s = precision_recall_fscore_support(all_targets, all_preds, average=None, labels=[0, 1], zero_division=0)\n",
    "    print(f'   üìä {task_name} F1: {f1:.4f}')\n",
    "    print(f'      [Neg] P: {p[0]:.2f} R: {r[0]:.2f} F1: {f[0]:.2f}')\n",
    "    print(f'      [Pos] P: {p[1]:.2f} R: {r[1]:.2f} F1: {f[1]:.2f}')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    if epoch <= CONFIG['freezing_epochs']:\n",
    "        for p in model.roberta.parameters(): p.requires_grad = False\n",
    "    else:\n",
    "        for p in model.roberta.parameters(): p.requires_grad = True\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', mininterval=10.0)\n",
    "    \n",
    "    epoch_losses = {'tox': [], 'emo': [], 'sent': [], 'rep': [], 'total': []}\n",
    "    \n",
    "    for step, batch in enumerate(pbar):\n",
    "        ids, mask = batch['ids'].to(device), batch['mask'].to(device)\n",
    "        tasks = batch['tasks']\n",
    "        out = model(ids, mask)\n",
    "        \n",
    "        losses = []\n",
    "        task_mask = []  # V10.2: Mask for Uncertainty Loss\n",
    "        \n",
    "        # 0. Toxicity\n",
    "        if (tasks == 0).sum() > 0:\n",
    "            l = focal_loss(out['toxicity'][tasks==0], batch['tox'].to(device), weight=tox_weights, smoothing=CONFIG['label_smoothing'])\n",
    "            losses.append(l)\n",
    "            task_mask.append(1.0)\n",
    "            epoch_losses['tox'].append(l.item())\n",
    "        else:\n",
    "            losses.append(torch.tensor(0., device=device))\n",
    "            task_mask.append(0.0) # ABSENT\n",
    "            \n",
    "        # 1. Emotion\n",
    "        if (tasks == 1).sum() > 0:\n",
    "            l = F.binary_cross_entropy_with_logits(out['emotion'][tasks==1], batch['emo'].to(device))\n",
    "            losses.append(l)\n",
    "            task_mask.append(1.0)\n",
    "            epoch_losses['emo'].append(l.item())\n",
    "        else:\n",
    "            losses.append(torch.tensor(0., device=device))\n",
    "            task_mask.append(0.0)\n",
    "            \n",
    "        # 2. Sentiment\n",
    "        if (tasks == 2).sum() > 0:\n",
    "            l = focal_loss(out['sentiment'][tasks==2], batch['sent'].to(device), smoothing=CONFIG['label_smoothing'])\n",
    "            losses.append(l)\n",
    "            task_mask.append(1.0)\n",
    "            epoch_losses['sent'].append(l.item())\n",
    "        else:\n",
    "            losses.append(torch.tensor(0., device=device))\n",
    "            task_mask.append(0.0)\n",
    "            \n",
    "        # 3. Reporting\n",
    "        if (tasks == 3).sum() > 0:\n",
    "            l = F.binary_cross_entropy_with_logits(out['reporting'][tasks==3].squeeze(-1), batch['rep'].float().to(device))\n",
    "            losses.append(l)\n",
    "            task_mask.append(1.0)\n",
    "            epoch_losses['rep'].append(l.item())\n",
    "        else:\n",
    "            losses.append(torch.tensor(0., device=device))\n",
    "            task_mask.append(0.0)\n",
    "\n",
    "        # Skip empty batches (rare now with balancing)\n",
    "        if sum(task_mask) == 0: continue\n",
    "\n",
    "        # V10.2: Pass mask to loss\n",
    "        loss = loss_fn(losses, task_mask) / CONFIG['gradient_accumulation']\n",
    "        \n",
    "        if torch.isnan(loss): continue\n",
    "        \n",
    "        epoch_losses['total'].append(loss.item() * CONFIG['gradient_accumulation'])\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % CONFIG['gradient_accumulation'] == 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "    avg_loss = np.mean(epoch_losses['total'] or [0])\n",
    "    print(f'\\nüìâ Losses: Avg Total {avg_loss:.4f} | Tox {np.mean(epoch_losses[\"tox\"] or [0]):.4f} | '\n",
    "          f'Rep {np.mean(epoch_losses[\"rep\"] or [0]):.4f}')\n",
    "\n",
    "    print(f'\\nüìù Epoch {epoch} Validation:')\n",
    "    val_f1_tox = evaluate(val_loader_tox, 0, 'Toxicity')\n",
    "    val_f1_rep = evaluate(val_loader_rep, 3, 'Reporting')\n",
    "    \n",
    "    return {'train_loss': avg_loss, 'val_f1': val_f1_tox, 'val_f1_rep': val_f1_rep}\n"
]

# 2. NEW SOURCE FOR CELL 12 (Main Loop)
CELL_12_SOURCE = [
    "# Cell 12: Main Training Loop\n",
    "print('='*60)\n",
    "print('üöÄ AURA V10.2 - TRAINING START (FIXED)')\n",
    "print('='*60)\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_f1': [], 'val_f1_rep': [], 'task_weights': []}\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    result = train_epoch(epoch)\n",
    "    val_f1 = result['val_f1']\n",
    "    weights = loss_fn.get_weights()\n",
    "    \n",
    "    history['train_loss'].append(result['train_loss'])\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_f1_rep'].append(result['val_f1_rep'])\n",
    "    history['task_weights'].append(weights.copy())\n",
    "    \n",
    "    print(f'\\nSummary Epoch {epoch}:')\n",
    "    print(f'  Avg Tox F1: {val_f1:.4f}')\n",
    "    print(f'  Weights: {weights.round(3)} (Tox/Emo/Sent/Rep)')\n",
    "    \n",
    "    # Early stopping (Strict)\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        # Save locally for safety\n",
    "        torch.save(model.state_dict(), '/kaggle/working/aura_v10_best.pt')\n",
    "        print('  >>> BEST MODEL SAVED <<<')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  (Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print('üõë Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "print(f'\\nüèÜ Final Best F1: {best_f1:.4f}')"
]

def apply_patches():
    with open(NOTEBOOK_PATH, 'r', encoding='utf-8') as f:\n        nb = json.load(f)\n\n    cells = nb['cells']\n\n    # Map IDs to Source\n    patches = {\n        '55719f9c': CELL_11_SOURCE,\n        '5fc621ca': CELL_12_SOURCE\n    }\n\n    patched_count = 0\n    for cell in cells:\n        if 'id' in cell and cell['id'] in patches:\n            cell['source'] = patches[cell['id']]\n            patched_count += 1\n            print(f"‚úÖ Patched cell {cell['id']}")\n\n    if patched_count != len(patches):\n        print(f"‚ö†Ô∏è Warning: Only patched {patched_count}/{len(patches)} cells. Check IDs.")\n\n    with open(NOTEBOOK_PATH, 'w', encoding='utf-8') as f:\n        json.dump(nb, f, indent=1)\n    \n    print(f"üíæ Saved fixed notebook to {NOTEBOOK_PATH}")\n\nif __name__ == \"__main__\":\n    apply_patches()\n
