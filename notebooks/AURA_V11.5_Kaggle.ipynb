{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# AURA V11.5 — DB-MTL: Dual-Balancing Multi-Task Learning\n\n**Base:** V11.3 (RoBERTa-base + all V11 fixes retained)\n\n## New in V11.5\n- **DB-MTL** (Lin et al., 2023) replaces GradNorm for gradient balancing\n- Two-pronged approach:\n  1. **Log-transform losses** → normalizes loss scales across tasks\n  2. **Gradient-magnitude balancing** → normalizes all task gradients to same magnitude\n- **No learnable task weights** → eliminates zero-sum weight divergence entirely\n\n## V11 Fixes Retained\n1. Proper Emotion/Sentiment Validation (official dev splits)\n2. Task Mask in Multi-Task Loss\n3. Optimizer State Reset on Unfreeze\n4. No Data Leak in Emotion Evaluation\n\n## Why DB-MTL over GradNorm?\nV11.3 (α=1.5) and V11.4 (α=1.0) both showed catastrophic weight divergence:\n- V11.3: Sentiment monopolized (weight→1.67), Reporting collapsed (→0.10)\n- V11.4: Reporting monopolized (weight→3.51), Toxicity collapsed (→0.10)\n- DB-MTL uses no learnable weights — divergence is structurally impossible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_seed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Seed — IDENTICAL TO V10.2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import (\n    f1_score, classification_report, confusion_matrix, \n    multilabel_confusion_matrix, precision_recall_fscore_support\n)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility — SAME SEED AS V10.2\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'\\U0001f527 Device: {device}')\nif device.type == 'cuda':\n    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration — V11.5: DB-MTL (no GradNorm hyperparameters needed)\nCONFIG = {\n    # Model\n    'encoder': 'roberta-base',\n    'hidden_dim': 768,\n    'n_heads': 8,\n    'num_emotion_classes': 7,\n    'max_length': 128,\n    'dropout': 0.3,\n    \n    # Training\n    'batch_size': 16,\n    'gradient_accumulation': 4,  # Effective batch = 64\n    'epochs': 10,  # SAME AS V10.2 FINAL RUN\n    'lr_encoder': 1e-5,\n    'lr_heads': 5e-5,\n    'weight_decay': 0.01,\n    'max_grad_norm': 1.0,\n    'warmup_ratio': 0.1,\n    \n    # Regularization (Module 3)\n    'focal_gamma': 2.0,\n    'label_smoothing': 0.1,\n    'patience': 5,\n    'freezing_epochs': 1,\n    # V11.5: DB-MTL — no learnable weight hyperparameters needed\n    # (GradNorm's gradnorm_alpha and gradnorm_lr are removed)\n    'balancing': 'db-mtl',  # Marker for logging clarity\n}\n\nDATA_DIR = '/kaggle/input/aura-v11-data'\nEMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n\nprint('\\U0001f4cb V11.5 Configuration (DB-MTL):')\nfor k, v in CONFIG.items():\n    print(f'   {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Functions — IDENTICAL TO V10.2\ndef plot_class_distribution(df, label_col, title, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 4))\n    counts = df[label_col].value_counts().sort_index()\n    bars = ax.bar(counts.index.astype(str), counts.values, color=['#66c2a5', '#fc8d62'])\n    ax.set_title(title)\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Count')\n    for bar, count in zip(bars, counts.values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n                str(count), ha='center', fontsize=10)\n    return ax\n\ndef plot_confusion_matrix_heatmap(y_true, y_pred, labels, title='Confusion Matrix', ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 5))\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=labels, yticklabels=labels, ax=ax,\n                cbar_kws={'label': 'Count'})\n    ax.set_title(title)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    return ax\n\ndef plot_multilabel_confusion_matrices(y_true, y_pred, labels, normalize=True):\n    cms = multilabel_confusion_matrix(y_true, y_pred)\n    n_labels = len(labels)\n    cols = min(4, n_labels)\n    rows = (n_labels + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axes = axes.flatten() if n_labels > 1 else [axes]\n    \n    for i, (cm, label) in enumerate(zip(cms, labels)):\n        ax = axes[i]\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n            fmt = '.2f'\n        else:\n            fmt = 'd'\n        sns.heatmap(cm, annot=True, fmt=fmt, cmap='YlGnBu', ax=ax,\n                    xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'],\n                    vmin=0, vmax=1 if normalize else None, cbar=False)\n        ax.set_title(label, fontsize=10)\n        ax.set_ylabel('Actual')\n        ax.set_xlabel('Predicted')\n    \n    for i in range(n_labels, len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Multilabel Confusion Matrices (Normalized)', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_history(history):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    axes[0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-o', label='Train')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].plot(range(1, len(history['val_f1'])+1), history['val_f1'], 'g-o', label='Val F1')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Macro F1')\n    axes[1].set_title('Validation F1 Score')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    weights = np.array(history['task_weights'])\n    for i, name in enumerate(['Toxicity', 'Emotion', 'Sentiment', 'Reporting']):\n        axes[2].plot(range(1, len(weights)+1), weights[:, i], '-o', label=name)\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Weight')\n    axes[2].set_title('DB-MTL Effective Weights')\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint('\\U0001f4ca Visualization functions loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mha_module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Task-Specific Multi-Head Attention Module — IDENTICAL TO V10.2\nclass TaskSpecificMHA(nn.Module):\n    \"\"\"Multi-Head Self-Attention per task (Module 2: Redundancy Principle).\n    \n    Each task gets its own attention mechanism to learn WHERE to look.\n    - Toxicity: looks for 'You' + insults\n    - Reporting: looks for 'said', 'claims'\n    - Sentiment: looks for adjectives\n    \"\"\"\n    def __init__(self, hidden_dim, n_heads, dropout=0.1):\n        super().__init__()\n        self.mha = nn.MultiheadAttention(\n            embed_dim=hidden_dim, \n            num_heads=n_heads, \n            batch_first=True, \n            dropout=dropout\n        )\n        self.layernorm = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states, attention_mask):\n        key_padding_mask = (attention_mask == 0)\n        attn_output, attn_weights = self.mha(\n            query=hidden_states, \n            key=hidden_states, \n            value=hidden_states,\n            key_padding_mask=key_padding_mask\n        )\n        output = self.layernorm(hidden_states + self.dropout(attn_output))\n        return output, attn_weights\n\nprint('\\U0001f9e0 TaskSpecificMHA module defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: AURA V10 Model — shared_rep for gradient balancing (V11.3+)\nclass AURA_V10(nn.Module):\n    \"\"\"AURA V10: RoBERTa + 4 Parallel Task-Specific MHSA Blocks.\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained(config['encoder'])\n        hidden = config['hidden_dim']\n        \n        # 4 Parallel MHSA Blocks (Feature Disentanglement)\n        self.tox_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.emo_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.sent_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.report_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        \n        self.dropout = nn.Dropout(config['dropout'])\n        \n        # Classification Heads\n        self.toxicity_head = nn.Linear(hidden, 2)\n        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n        self.sentiment_head = nn.Linear(hidden, 2)\n        self.reporting_head = nn.Linear(hidden, 1)\n        \n        # Bias Initialization (NB11: Imbalanced Datasets)\n        with torch.no_grad():\n            self.toxicity_head.bias[0] = 2.5   # Non-Toxic\n            self.toxicity_head.bias[1] = -2.5  # Toxic\n\n    def _mean_pool(self, seq, mask):\n        mask_exp = mask.unsqueeze(-1).expand(seq.size()).float()\n        return (seq * mask_exp).sum(dim=1) / mask_exp.sum(dim=1).clamp(min=1e-9)\n\n    def forward(self, input_ids, attention_mask):\n        shared = self.roberta(input_ids, attention_mask).last_hidden_state\n        \n        tox_seq, _ = self.tox_mha(shared, attention_mask)\n        emo_seq, _ = self.emo_mha(shared, attention_mask)\n        sent_seq, _ = self.sent_mha(shared, attention_mask)\n        rep_seq, _ = self.report_mha(shared, attention_mask)\n        \n        return {\n            'toxicity': self.toxicity_head(self.dropout(self._mean_pool(tox_seq, attention_mask))),\n            'emotion': self.emotion_head(self.dropout(self._mean_pool(emo_seq, attention_mask))),\n            'sentiment': self.sentiment_head(self.dropout(self._mean_pool(sent_seq, attention_mask))),\n            'reporting': self.reporting_head(self.dropout(self._mean_pool(rep_seq, attention_mask))).squeeze(-1),\n            'shared_rep': shared  # V11.5: encoder output for DB-MTL gradient balancing\n        }\n\nprint('\\U0001f985 AURA_V10 model defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Loss Functions\n# V11.5: DB-MTL replaces GradNorm (Lin et al., 2023)\n#\n# DB-MTL: Dual-Balancing Multi-Task Learning\n# Two mechanisms to prevent any task from dominating:\n#   1. Log-transform each task loss -> normalizes loss scales\n#   2. Normalize gradient magnitudes -> all tasks contribute equally to encoder\n#\n# Unlike GradNorm, DB-MTL has NO learnable task weights.\n# This eliminates the zero-sum weight divergence seen in V11.3/V11.4.\n\ndef focal_loss(logits, targets, weight=None, gamma=2.0, smoothing=0.0):\n    \"\"\"Focal loss for binary/multiclass classification — IDENTICAL to V10.2.\"\"\"\n    n_classes = logits.size(-1)\n    if smoothing > 0:\n        with torch.no_grad():\n            smooth = torch.full_like(logits, smoothing / (n_classes - 1))\n            smooth.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n        log_probs = F.log_softmax(logits, dim=-1)\n        ce = -(smooth * log_probs).sum(dim=-1)\n    else:\n        ce = F.cross_entropy(logits, targets, weight=weight, reduction='none')\n    \n    pt = torch.exp(-ce)\n    focal = ((1 - pt) ** gamma) * ce\n    return focal.mean()\n\n\nclass DBMTLLoss(nn.Module):\n    \"\"\"DB-MTL: Dual-Balancing Multi-Task Learning (Lin et al., 2023).\n    \n    Eliminates task weight divergence with two simple mechanisms:\n    \n    1. Loss-scale balancing:\n       - Apply log() to each task loss before summing\n       - This compresses large losses and expands small ones,\n         ensuring all tasks have similar-scale contributions\n       - Example: if L_tox=0.25 and L_sent=0.14,\n         log(0.25)=-1.39 and log(0.14)=-1.97 -> much closer in scale\n    \n    2. Gradient-magnitude balancing:\n       - Compute gradient norm of each log-loss w.r.t. shared encoder output\n       - Scale each loss so all gradients have the same magnitude (max norm)\n       - This prevents any single task from dominating shared encoder updates\n    \n    No learnable parameters -> no divergence possible.\n    \"\"\"\n    def __init__(self, n_tasks):\n        super().__init__()\n        self.n_tasks = n_tasks\n        # Track effective weights for logging (not learned, just observed)\n        self._last_effective_weights = np.ones(n_tasks)\n    \n    def forward(self, task_losses, shared_rep, task_mask=None):\n        \"\"\"Compute DB-MTL balanced loss.\n        \n        Args:\n            task_losses: list of per-task loss tensors\n            shared_rep: encoder output tensor (for gradient computation)\n            task_mask: list of bools — which tasks are present in this batch\n        \n        Returns:\n            Balanced total loss (scalar tensor)\n        \"\"\"\n        # Collect active (present in batch) task losses\n        active_losses = []\n        active_indices = []\n        for i in range(self.n_tasks):\n            if task_mask is not None and not task_mask[i]:\n                continue\n            if not task_losses[i].requires_grad:\n                continue\n            active_losses.append(task_losses[i])\n            active_indices.append(i)\n        \n        if len(active_losses) == 0:\n            return torch.tensor(0.0, device=shared_rep.device, requires_grad=True)\n        \n        # If only one task in batch, just return its log-loss (no balancing needed)\n        if len(active_losses) == 1:\n            return torch.log(active_losses[0].clamp(min=1e-8))\n        \n        # ── Step 1: Log-transform losses (loss-scale balancing) ──\n        log_losses = [torch.log(L.clamp(min=1e-8)) for L in active_losses]\n        \n        # ── Step 2: Gradient-magnitude balancing ──\n        # Only possible when shared_rep has gradient (encoder unfrozen).\n        # During frozen epochs, shared_rep.requires_grad=False, so we\n        # fall back to equal-weight log-loss sum (log balancing only).\n        can_balance_grads = shared_rep.requires_grad\n        \n        if can_balance_grads:\n            grad_norms = []\n            for ll in log_losses:\n                grad = torch.autograd.grad(\n                    ll, shared_rep, retain_graph=True, create_graph=False,\n                    allow_unused=True\n                )[0]\n                if grad is not None:\n                    grad_norms.append(grad.norm().item())\n                else:\n                    grad_norms.append(0.0)\n            \n            # Compute normalization scales (to max norm)\n            max_norm = max(grad_norms) if max(grad_norms) > 1e-12 else 1.0\n            scales = [max_norm / max(gn, 1e-12) for gn in grad_norms]\n        else:\n            # Frozen encoder: equal weights (log-loss balancing only)\n            scales = [1.0] * len(log_losses)\n        \n        # ── Step 3: Weighted sum of log-losses ──\n        # Each log-loss is scaled so its gradient magnitude matches the max,\n        # ensuring all tasks contribute equally to shared encoder updates.\n        total = torch.tensor(0.0, device=shared_rep.device)\n        for ll, s in zip(log_losses, scales):\n            total = total + s * ll\n        \n        # Track effective weights for visualization\n        eff_weights = np.ones(self.n_tasks)\n        for j, idx in enumerate(active_indices):\n            eff_weights[idx] = scales[j]\n        self._last_effective_weights = eff_weights\n        \n        return total\n    \n    def get_weights(self):\n        \"\"\"Return last observed effective weights (for logging/viz only).\"\"\"\n        return self._last_effective_weights.copy()\n\n\nprint('\\u2696\\ufe0f Loss functions defined (Focal + DB-MTL).')\nprint('   V11.5: DB-MTL replaces GradNorm for gradient balancing.')\nprint('   Two mechanisms: (1) log-loss scale balancing, (2) gradient magnitude normalization.')\nprint('   No learnable task weights — zero-sum divergence eliminated.')\nprint('   V11 FIX retained: task_mask support (absent tasks skipped).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Dataset Classes — IDENTICAL TO V10.2\nclass BaseDataset(Dataset):\n    def __init__(self, path, tokenizer, max_len):\n        self.df = pd.read_csv(path)\n        self.tok = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self): \n        return len(self.df)\n    \n    def encode(self, text):\n        return self.tok(\n            str(text), max_length=self.max_len, \n            padding='max_length', truncation=True, return_tensors='pt'\n        )\n\nclass ToxicityDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'tox': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 0\n        }\n\nclass EmotionDataset(BaseDataset):\n    def __init__(self, path, tokenizer, max_len, cols):\n        super().__init__(path, tokenizer, max_len)\n        self.cols = cols\n        if 'label_sum' in self.df.columns:\n            self.df = self.df[self.df['label_sum'] > 0].reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'emo': torch.tensor([float(row[c]) for c in self.cols], dtype=torch.float), \n            'task': 1\n        }\n\nclass SentimentDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'sent': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 2\n        }\n\nclass ReportingDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'rep': torch.tensor(int(row['is_reporting']), dtype=torch.long), \n            'task': 3\n        }\n\ndef collate_fn(batch):\n    ids = torch.stack([x['ids'] for x in batch])\n    mask = torch.stack([x['mask'] for x in batch])\n    tasks = torch.tensor([x['task'] for x in batch])\n    \n    tox_items = [x['tox'] for x in batch if x['task'] == 0]\n    emo_items = [x['emo'] for x in batch if x['task'] == 1]\n    sent_items = [x['sent'] for x in batch if x['task'] == 2]\n    rep_items = [x['rep'] for x in batch if x['task'] == 3]\n    \n    return {\n        'ids': ids, 'mask': mask, 'tasks': tasks,\n        'tox': torch.stack(tox_items) if tox_items else None,\n        'emo': torch.stack(emo_items) if emo_items else None,\n        'sent': torch.stack(sent_items) if sent_items else None,\n        'rep': torch.stack(rep_items) if rep_items else None\n    }\n\nprint('\\U0001f4e6 Dataset classes defined \\u2014 identical to V10.2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load Data\n# V11 FIX #1: Load proper held-out validation sets for emotion and sentiment.\n# These come from official GoEmotions dev and SST-2 dev splits,\n# generated by prepare_v11_datasets.py. No data leak.\n\ntokenizer = RobertaTokenizer.from_pretrained(CONFIG['encoder'])\n\n# Training sets\ntox_train = ToxicityDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'])\nemo_train = EmotionDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\nsent_train = SentimentDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'])\nrep_train = ReportingDataset(f'{DATA_DIR}/reporting_examples_augmented.csv', tokenizer, CONFIG['max_length'])\n\n# Validation sets — V11: ALL from official held-out splits\ntox_val = ToxicityDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, CONFIG['max_length'])\nemo_val = EmotionDataset(f'{DATA_DIR}/emotions_val.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\nsent_val = SentimentDataset(f'{DATA_DIR}/sentiment_val.csv', tokenizer, CONFIG['max_length'])\n\n# Combined training loader\ntrain_ds = ConcatDataset([tox_train, emo_train, sent_train, rep_train])\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, \n                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n\n# Separate validation loaders per task\ntox_val_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\nemo_val_loader = DataLoader(emo_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\nsent_val_loader = DataLoader(sent_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\n\nprint('='*60)\nprint('\\U0001f4ca DATASET SUMMARY')\nprint('='*60)\nprint(f'Training Samples: {len(train_ds):,}')\nprint(f'  \\u251c\\u2500 Toxicity:  {len(tox_train):,}')\nprint(f'  \\u251c\\u2500 Emotion:   {len(emo_train):,}')\nprint(f'  \\u251c\\u2500 Sentiment: {len(sent_train):,}')\nprint(f'  \\u2514\\u2500 Reporting: {len(rep_train):,}')\nprint(f'Validation Samples:')\nprint(f'  \\u251c\\u2500 Toxicity:  {len(tox_val):,} (OLID official test)')\nprint(f'  \\u251c\\u2500 Emotion:   {len(emo_val):,} (GoEmotions official dev)')\nprint(f'  \\u2514\\u2500 Sentiment: {len(sent_val):,} (SST-2 official dev)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Distribution Analysis (NB11 Pattern)\nprint('='*60)\nprint('\\U0001f4c8 CLASS DISTRIBUTION ANALYSIS (NB11)')\nprint('='*60)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 1. Toxicity Distribution\ntox_df = pd.read_csv(f'{DATA_DIR}/toxicity_train.csv')\nplot_class_distribution(tox_df, 'label', 'Toxicity: Class Distribution', axes[0, 0])\naxes[0, 0].set_xticklabels(['Non-Toxic (0)', 'Toxic (1)'])\n\n# 2. Task Sample Distribution\ntask_counts = {'Toxicity': len(tox_train), 'Emotion': len(emo_train), \n               'Sentiment': len(sent_train), 'Reporting': len(rep_train)}\ncolors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3']\nbars = axes[0, 1].bar(task_counts.keys(), task_counts.values(), color=colors)\naxes[0, 1].set_title('Task Sample Distribution')\naxes[0, 1].set_ylabel('Count')\nfor bar, count in zip(bars, task_counts.values()):\n    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500, \n                    f'{count:,}', ha='center', fontsize=9)\n\n# 3. Emotion Label Distribution (Multilabel)\nemo_df = pd.read_csv(f'{DATA_DIR}/emotions_train.csv')\nif 'label_sum' in emo_df.columns:\n    emo_df = emo_df[emo_df['label_sum'] > 0]\nemo_counts = emo_df[EMO_COLS].sum().sort_values(ascending=True)\nemo_counts.plot(kind='barh', ax=axes[1, 0], color='#8da0cb')\naxes[1, 0].set_title('Emotion Label Distribution')\naxes[1, 0].set_xlabel('Count')\n\n# 4. # of Labels per Sample\nif 'label_sum' in emo_df.columns:\n    label_counts = emo_df['label_sum'].value_counts().sort_index()\nelse:\n    label_counts = emo_df[EMO_COLS].sum(axis=1).value_counts().sort_index()\nlabel_counts.plot(kind='bar', ax=axes[1, 1], color='#fc8d62')\naxes[1, 1].set_title('Emotion: # of Labels per Sample')\naxes[1, 1].set_xlabel('Number of Emotion Labels')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].tick_params(axis='x', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\nneg, pos = tox_df['label'].value_counts().sort_index()\nprint(f'\\n\\u26a0\\ufe0f Toxicity Imbalance: {neg:,} Non-Toxic vs {pos:,} Toxic ({pos/(neg+pos)*100:.1f}% minority class)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model & Optimizer Setup\n# V11.5: DBMTLLoss replaces GradNormLoss — no learnable weights\n\nmodel = AURA_V10(CONFIG).to(device)\n\n# V11.5: DB-MTL loss (replaces GradNorm)\nloss_fn = DBMTLLoss(n_tasks=4).to(device)\n\n# Separate encoder and head parameters\nencoder_params = set(model.roberta.parameters())\nhead_params = [p for p in model.parameters() if p not in encoder_params]\n\noptimizer = torch.optim.AdamW([\n    {'params': model.roberta.parameters(), 'lr': CONFIG['lr_encoder']},\n    {'params': head_params, 'lr': CONFIG['lr_heads']}\n], weight_decay=CONFIG['weight_decay'])\n\n# Toxicity class weights — IDENTICAL to V10.2/V11\ntox_weights = torch.tensor([0.5, 2.0]).to(device)\n\ntotal_steps = (len(train_loader) // CONFIG['gradient_accumulation']) * CONFIG['epochs']\nwarmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n)\n\nn_params = sum(p.numel() for p in model.parameters())\nn_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('='*60)\nprint('\\U0001f3d7\\ufe0f MODEL SETUP')\nprint('='*60)\nprint(f'Encoder: {CONFIG[\"encoder\"]}')\nprint(f'Total parameters:     {n_params:,}')\nprint(f'Trainable parameters: {n_train:,}')\nprint(f'Total optimization steps: {total_steps}')\nprint(f'Warmup steps: {warmup_steps}')\nprint(f'Effective batch size: {CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation\"]}')\nprint(f'V11.5: DB-MTL (log-loss + gradient-norm balancing, no learnable weights)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training & Evaluation Functions\n# V11.5: DB-MTL gradient balancing replaces GradNorm.\n# All V11 fixes retained: task_mask, optimizer state reset.\n# Key difference: loss_fn now takes shared_rep as input for gradient computation.\n# No initial_losses tracking needed — DB-MTL is stateless.\n\ndef train_epoch(epoch):\n    model.train()\n    \n    # Progressive freezing — IDENTICAL to V11\n    if epoch <= CONFIG['freezing_epochs']:\n        print(f'\\u2744\\ufe0f Epoch {epoch}: RoBERTa FROZEN')\n        for p in model.roberta.parameters(): \n             p.requires_grad = False\n    else:\n        # --- V11 FIX #4: Reset Adam states on first unfreeze ---\n        if epoch == CONFIG['freezing_epochs'] + 1:\n            print(f'\\U0001f525 Epoch {epoch}: RoBERTa UNFROZEN (resetting optimizer states)')\n            for p in model.roberta.parameters():\n                p.requires_grad = True\n                # Clear stale Adam momentum/variance from frozen epochs\n                if p in optimizer.state:\n                    del optimizer.state[p]\n        else:\n            print(f'\\U0001f525 Epoch {epoch}: RoBERTa UNFROZEN')\n            for p in model.roberta.parameters(): \n                 p.requires_grad = True\n    \n    total_loss = 0\n    step_weights = []  # Track DB-MTL effective weights per step\n    optimizer.zero_grad()\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', mininterval=10.0)\n    \n    for step, batch in enumerate(pbar):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        tasks = batch['tasks']\n        \n        out = model(ids, mask)\n        shared_rep = out['shared_rep']  # V11.5: for DB-MTL gradient balancing\n        \n        # Compute per-task losses — IDENTICAL to V10.2/V11\n        losses = []\n        task_mask = []  # V11 FIX #3: track which tasks are present\n        \n        # Toxicity\n        if batch['tox'] is not None and (tasks == 0).sum() > 0:\n            losses.append(focal_loss(\n                out['toxicity'][tasks == 0], batch['tox'].to(device), \n                weight=tox_weights, smoothing=CONFIG['label_smoothing']\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Emotion (Multilabel BCE)\n        if batch['emo'] is not None and (tasks == 1).sum() > 0:\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['emotion'][tasks == 1], batch['emo'].to(device)\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Sentiment\n        if batch['sent'] is not None and (tasks == 2).sum() > 0:\n            losses.append(focal_loss(\n                out['sentiment'][tasks == 2], batch['sent'].to(device), \n                smoothing=CONFIG['label_smoothing']\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Reporting\n        if batch['rep'] is not None and (tasks == 3).sum() > 0:\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['reporting'][tasks == 3], batch['rep'].float().to(device)\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Skip fully empty batches\n        if not any(task_mask):\n            print(f\"\\u26a0\\ufe0f Warning: Empty batch at step {step}, skipping\")\n            optimizer.zero_grad()\n            continue\n\n        # V11.5: DB-MTL loss — passes shared_rep for gradient balancing\n        # No separate gradnorm_step needed — balancing is built into the loss\n        loss = loss_fn(losses, shared_rep, task_mask=task_mask) / CONFIG['gradient_accumulation']\n        \n        # Track effective weights for visualization\n        step_weights.append(loss_fn.get_weights())\n        \n        # NaN/Inf safety check\n        # Note: log-losses are negative (log of values < 1), this is expected\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"\\u26a0\\ufe0f Warning: Invalid loss {loss.item():.4f} at step {step}, skipping batch\")\n            optimizer.zero_grad()\n            continue\n\n        loss.backward()\n        \n        # Gradient Accumulation\n        is_accum_step = (step + 1) % CONFIG['gradient_accumulation'] == 0\n        if is_accum_step:\n            nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n        total_loss += loss.item() * CONFIG['gradient_accumulation']\n        if step % 50 == 0: pbar.set_postfix({'loss': f'{loss.item() * CONFIG[\"gradient_accumulation\"]:.3f}'})\n    \n    # Average effective weights across all steps this epoch\n    avg_weights = np.mean(step_weights, axis=0) if step_weights else np.ones(4)\n    return total_loss / len(train_loader), avg_weights\n\n@torch.no_grad()\ndef evaluate_toxicity():\n    \"\"\"Evaluate toxicity on held-out OLID test set.\"\"\"\n    model.eval()\n    preds, trues = [], []\n    for batch in tox_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n    return f1_score(trues, preds, average='macro', zero_division=0)\n\n@torch.no_grad()\ndef evaluate_emotion():\n    \"\"\"Evaluate emotion on held-out GoEmotions dev set (V11 FIX #2).\"\"\"\n    model.eval()\n    all_preds, all_trues = [], []\n    for batch in emo_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        pred = (torch.sigmoid(out['emotion']) > 0.5).cpu().numpy()\n        all_preds.append(pred)\n        all_trues.append(batch['emo'].numpy())\n    all_preds = np.concatenate(all_preds)\n    all_trues = np.concatenate(all_trues)\n    # Per-emotion F1, then average\n    f1s = []\n    for i in range(len(EMO_COLS)):\n        f1s.append(f1_score(all_trues[:, i], all_preds[:, i], average='binary', zero_division=0))\n    return np.mean(f1s)\n\n@torch.no_grad()\ndef evaluate_sentiment():\n    \"\"\"Evaluate sentiment on held-out SST-2 dev set (V11 FIX #2).\"\"\"\n    model.eval()\n    preds, trues = [], []\n    for batch in sent_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['sentiment'].argmax(1).cpu().numpy())\n        trues.extend(batch['sent'].numpy())\n    return f1_score(trues, preds, average='macro', zero_division=0)\n\nprint('\\U0001f3af Training & evaluation functions defined.')\nprint('   V11.5: DB-MTL gradient balancing (log-loss + grad-norm).')\nprint('   No initial_losses / gradnorm_step needed — DB-MTL is stateless.')\nprint('   V11 FIX #3: task_mask for absent tasks (retained).')\nprint('   V11 FIX #4: Optimizer states reset on RoBERTa unfreeze (retained).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Main Training Loop\n# V11.5: DB-MTL effective weights tracked per epoch (observed, not learned).\n\nprint('='*60)\nprint('\\U0001f680 AURA V11.5 \\u2014 TRAINING START (RoBERTa + DB-MTL)')\nprint('='*60)\n\nbest_f1 = 0\npatience_counter = 0\nhistory = {\n    'train_loss': [], \n    'val_f1': [],            # Toxicity (primary)\n    'val_emo_f1': [],        # Emotion (V11)\n    'val_sent_f1': [],       # Sentiment (V11)\n    'task_weights': []       # DB-MTL effective weights (observed per epoch)\n}\n\nfor epoch in range(1, CONFIG['epochs'] + 1):\n    train_loss, epoch_weights = train_epoch(epoch)\n    val_f1 = evaluate_toxicity()\n    emo_f1 = evaluate_emotion()\n    sent_f1 = evaluate_sentiment()\n    \n    history['train_loss'].append(train_loss)\n    history['val_f1'].append(val_f1)\n    history['val_emo_f1'].append(emo_f1)\n    history['val_sent_f1'].append(sent_f1)\n    history['task_weights'].append(epoch_weights.tolist())\n    \n    print(f'\\nEpoch {epoch} Summary:')\n    print(f'  Train Loss:     {train_loss:.4f}')\n    print(f'  Toxicity Val F1:  {val_f1:.4f}')\n    print(f'  Emotion Val F1:   {emo_f1:.4f}')\n    print(f'  Sentiment Val F1: {sent_f1:.4f}')\n    print(f'  DB-MTL Eff. Weights [Tox/Emo/Sent/Rep]: {np.array(epoch_weights).round(3)}')\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_emo_f1 = emo_f1\n        best_sent_f1 = sent_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'aura_v11.5_best.pt')\n        print('  >>> BEST MODEL SAVED <<<')\n    else:\n        patience_counter += 1\n        print(f'  (No improvement. Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n        if patience_counter >= CONFIG['patience']:\n            print(f'\\n\\u26a0\\ufe0f Early stopping at epoch {epoch}')\n            break\n\nprint('\\n' + '='*60)\nprint(f'\\u2705 Training Complete.')\nprint(f'   Best Toxicity F1:  {best_f1:.4f}')\nprint(f'   Best Emotion F1:   {best_emo_f1:.4f}')\nprint(f'   Best Sentiment F1: {best_sent_f1:.4f}')\nprint('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training History Visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Loss\naxes[0, 0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-o', label='Train')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Toxicity F1\naxes[0, 1].plot(range(1, len(history['val_f1'])+1), history['val_f1'], 'g-o', label='Toxicity Val F1')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Macro F1')\naxes[0, 1].set_title('Toxicity Validation F1')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Emotion + Sentiment F1\naxes[1, 0].plot(range(1, len(history['val_emo_f1'])+1), history['val_emo_f1'], 'r-o', label='Emotion Val F1')\naxes[1, 0].plot(range(1, len(history['val_sent_f1'])+1), history['val_sent_f1'], 'm-o', label='Sentiment Val F1')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('F1')\naxes[1, 0].set_title('Auxiliary Task Validation F1')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# DB-MTL Effective Weights\nweights = np.array(history['task_weights'])\nfor i, name in enumerate(['Toxicity', 'Emotion', 'Sentiment', 'Reporting']):\n    axes[1, 1].plot(range(1, len(weights)+1), weights[:, i], '-o', label=name)\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Effective Weight (observed)')\naxes[1, 1].set_title('DB-MTL Effective Weights (not learned)')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_toxicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Final Evaluation — Toxicity\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: TOXICITY')\nprint('='*60)\n\nmodel.load_state_dict(torch.load('aura_v11.5_best.pt'))\nmodel.eval()\n\npreds, trues = [], []\nwith torch.no_grad():\n    for batch in tox_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n\nprint('\\n--- Classification Report ---')\nprint(classification_report(trues, preds, target_names=['Non-Toxic', 'Toxic']))\n\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_confusion_matrix_heatmap(trues, preds, ['Non-Toxic', 'Toxic'], 'Toxicity Confusion Matrix', ax)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_emotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Final Evaluation — Emotion\n# V11 FIX #2: Uses official GoEmotions dev split instead of training data tail.\n# This eliminates the data leak present in V10.2.\n\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: EMOTION (GoEmotions Dev)')\nprint('='*60)\n\nemo_preds, emo_trues = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(emo_val_loader, desc='Evaluating Emotions'):\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        pred = (torch.sigmoid(out['emotion']) > 0.5).cpu().numpy()\n        emo_preds.append(pred)\n        emo_trues.append(batch['emo'].numpy())\n\nemo_preds = np.concatenate(emo_preds)\nemo_trues = np.concatenate(emo_trues)\n\n# Per-emotion metrics\nprint('\\n--- Per-Emotion Metrics ---')\nfor i, emo in enumerate(EMO_COLS):\n    p, r, f1, _ = precision_recall_fscore_support(emo_trues[:, i], emo_preds[:, i], average='binary', zero_division=0)\n    print(f'{emo:10s}: P={p:.3f}, R={r:.3f}, F1={f1:.3f}')\n\n# Multilabel Confusion Matrices\nprint('\\n--- Multilabel Confusion Matrices ---')\nplot_multilabel_confusion_matrices(emo_trues, emo_preds, EMO_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Evaluation — Sentiment (V11 NEW)\n# Uses official SST-2 dev split for evaluation.\n\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: SENTIMENT (SST-2 Dev)')\nprint('='*60)\n\nsent_preds, sent_trues = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(sent_val_loader, desc='Evaluating Sentiment'):\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        sent_preds.extend(out['sentiment'].argmax(1).cpu().numpy())\n        sent_trues.extend(batch['sent'].numpy())\n\nprint('\\n--- Classification Report ---')\nprint(classification_report(sent_trues, sent_preds, target_names=['Negative', 'Positive']))\n\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_confusion_matrix_heatmap(sent_trues, sent_preds, ['Negative', 'Positive'], 'Sentiment Confusion Matrix', ax)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stress_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Qualitative Stress Test — IDENTICAL TO V10.2\nprint('='*60)\nprint('\\U0001f9ea QUALITATIVE STRESS TEST')\nprint('='*60)\n\ntest_cases = [\n    (\"I hate rainy Mondays\", \"Non-Toxic\"),\n    (\"This soup is disgusting\", \"Non-Toxic\"),\n    (\"I am so angry at the traffic\", \"Non-Toxic\"),\n    (\"This movie was terrible\", \"Non-Toxic\"),\n    \n    (\"You are an idiot\", \"Toxic\"),\n    (\"I hate you so much\", \"Toxic\"),\n    (\"Go kill yourself\", \"Toxic\"),\n    (\"You are worthless garbage\", \"Toxic\"),\n    \n    (\"He said you are an idiot\", \"Non-Toxic\"),\n    (\"The article discusses hate speech\", \"Non-Toxic\"),\n    (\"Someone wrote 'go die' in the comments\", \"Non-Toxic\"),\n]\n\nprint(f\"{'Text':<50} {'Expected':<12} {'Predicted':<12} {'Status'}\")\nprint('-'*80)\n\ncorrect = 0\nmodel.eval()\nwith torch.no_grad():\n    for text, expected in test_cases:\n        enc = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n        out = model(enc['input_ids'].to(device), enc['attention_mask'].to(device))\n        pred_idx = out['toxicity'].argmax(1).item()\n        pred_label = 'Toxic' if pred_idx == 1 else 'Non-Toxic'\n        status = '\\u2705' if pred_label == expected else '\\u274c'\n        if pred_label == expected:\n            correct += 1\n        print(f\"{text[:48]:<50} {expected:<12} {pred_label:<12} {status}\")\n\nprint('-'*80)\nprint(f'Stress Test Accuracy: {correct}/{len(test_cases)} ({correct/len(test_cases)*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: V11.5 Summary & Comparison\nprint('='*60)\nprint('\\u2697\\ufe0f V11.5 RESULTS SUMMARY')\nprint('='*60)\n\n# Known previous results\nV10_TOX_F1 = 0.7572\nBASELINE_TOX_F1 = 0.7378\nV11_TOX_F1 = 0.7418\nV112_TOX_F1 = 0.7368\nV113_TOX_F1 = 0.7830\nV114_TOX_F1 = 0.7836\n\nV11_EMO_F1 = 0.6202\nV11_SENT_F1 = 0.9403\nV112_EMO_F1 = 0.6095\nV112_SENT_F1 = 0.9357\nV113_EMO_F1 = 0.6112\nV113_SENT_F1 = 0.9334\nV114_EMO_F1 = 0.5969\nV114_SENT_F1 = 0.9426\n\nV115_TOX_F1 = best_f1\n\nprint(f'\\n{\"Metric\":<25} {\"V10.2\":<10} {\"V11\":<10} {\"V11.3\":<10} {\"V11.4\":<10} {\"V11.5\":<10}')\nprint('-'*75)\nprint(f'{\"Toxicity Val F1\":<25} {V10_TOX_F1:<10.4f} {V11_TOX_F1:<10.4f} {V113_TOX_F1:<10.4f} {V114_TOX_F1:<10.4f} {V115_TOX_F1:<10.4f}')\nprint(f'{\"Emotion Val F1\":<25} {\"N/A\":<10} {V11_EMO_F1:<10.4f} {V113_EMO_F1:<10.4f} {V114_EMO_F1:<10.4f} {best_emo_f1:<10.4f}')\nprint(f'{\"Sentiment Val F1\":<25} {\"N/A\":<10} {V11_SENT_F1:<10.4f} {V113_SENT_F1:<10.4f} {V114_SENT_F1:<10.4f} {best_sent_f1:<10.4f}')\nprint(f'{\"Gradient Balance\":<25} {\"Kendall\":<10} {\"Kendall\":<10} {\"GradNorm\":<10} {\"GradNorm\":<10} {\"DB-MTL\":<10}')\nprint(f'{\"Learnable Weights\":<25} {\"Yes\":<10} {\"Yes\":<10} {\"Yes\":<10} {\"Yes\":<10} {\"No\":<10}')\nprint('-'*75)\n\ndelta_v113 = V115_TOX_F1 - V113_TOX_F1\ndelta_v10 = V115_TOX_F1 - V10_TOX_F1\ndelta_v114 = V115_TOX_F1 - V114_TOX_F1\nprint(f'\\n\\u0394 vs V11.3 (GradNorm \\u03b1=1.5): {delta_v113:+.4f} F1')\nprint(f'\\u0394 vs V11.4 (GradNorm \\u03b1=1.0): {delta_v114:+.4f} F1')\nprint(f'\\u0394 vs V10.2 (Kendall):         {delta_v10:+.4f} F1')\n\nprint('\\n--- V11.5 Changes ---')\nprint('  V11 fixes retained:')\nprint('    1. \\u2705 Proper emotion/sentiment validation')\nprint('    2. \\u2705 Emotion eval data leak fixed')\nprint('    3. \\u2705 Task mask in multi-task loss')\nprint('    4. \\u2705 Optimizer state reset on unfreeze')\nprint('  V11.5 new:')\nprint('    5. \\u2705 DB-MTL gradient balancing (replaces GradNorm)')\nprint('    6. \\u2705 No learnable weights (zero divergence risk)')\nprint('  V11.2 reverted:')\nprint('    \\u274c Task-Weighted Sampling (back to uniform shuffle)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Save Artifacts\nprint('='*60)\nprint('\\U0001f4be SAVING V11.5 ARTIFACTS')\nprint('='*60)\n\nimport json as json_save\nhistory_serializable = {\n    'train_loss': history['train_loss'],\n    'val_f1': history['val_f1'],\n    'val_emo_f1': history['val_emo_f1'],\n    'val_sent_f1': history['val_sent_f1'],\n    'task_weights': history['task_weights'],  # Already list (DB-MTL observed weights)\n    'best_f1': best_f1,\n    'best_emo_f1': best_emo_f1,\n    'best_sent_f1': best_sent_f1,\n    'config': CONFIG,\n    'model_type': 'aura_v11.5_dbmtl',\n    'balancing_method': 'DB-MTL (Lin et al., 2023)',\n    'v11_fixes': [\n        'proper_emotion_sentiment_validation',\n        'task_mask_in_loss',\n        'optimizer_state_reset_on_unfreeze',\n        'emotion_eval_data_leak_fixed'\n    ],\n    'v115_improvements': [\n        'db_mtl_gradient_balancing',\n        'no_learnable_task_weights'\n    ]\n}\nwith open('aura_v11.5_history.json', 'w') as f:\n    json_save.dump(history_serializable, f, indent=2)\n\nprint('\\u2705 Model saved: aura_v11.5_best.pt')\nprint('\\u2705 History saved: aura_v11.5_history.json')\nprint(f'\\n\\U0001f3c6 Best Toxicity F1:  {best_f1:.4f}')\nprint(f'\\U0001f3c6 Best Emotion F1:   {best_emo_f1:.4f}')\nprint(f'\\U0001f3c6 Best Sentiment F1: {best_sent_f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}