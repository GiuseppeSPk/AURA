{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AURA V5: Production Model\n",
                "\n",
                "---\n",
                "## PRIMA DI ESEGUIRE:\n",
                "1. **Settings** -> **Accelerator** -> **GPU T4 x2**\n",
                "2. **Add Input** -> Carica `aura-data-v5`\n",
                "---\n",
                "\n",
                "### V5 Configuration\n",
                "| Component | Value |\n",
                "|-----------|-------|\n",
                "| Backbone | BERT-base (110M) |\n",
                "| Emotions | 5 (anger, disgust, fear, joy, neutral) |\n",
                "| Toxicity Loss | Focal Loss (Î³=2.0) |\n",
                "| Learning Rate | 1e-5 |\n",
                "| Dropout | 0.3 |\n",
                "| Data | Balanced, single-label, noise-filtered |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
                "from torch.optim.lr_scheduler import OneCycleLR\n",
                "from transformers import BertModel, BertTokenizer\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.metrics import f1_score, classification_report\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"=\"*60)\n",
                "if torch.cuda.is_available():\n",
                "    device = torch.device('cuda')\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    device = torch.device('cpu')\n",
                "    raise RuntimeError(\"ATTIVA LA GPU!\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# V5 CONFIGURATION - OPTIMIZED\n",
                "# ============================================================\n",
                "CONFIG = {\n",
                "    'encoder': 'bert-base-uncased',\n",
                "    'max_length': 128,\n",
                "    'num_emotion_classes': 5,  # V5: 5 emotions only\n",
                "    'dropout': 0.3,            # Increased from 0.1\n",
                "    'batch_size': 16,\n",
                "    'gradient_accumulation': 2,\n",
                "    'epochs': 8,               # Increased from 5\n",
                "    'lr': 1e-5,                # Reduced from 2e-5\n",
                "    'weight_decay': 0.02,      # Increased from 0.01\n",
                "    'patience': 3,             # Increased from 2\n",
                "    'mc_samples': 10,\n",
                "    'focal_gamma': 2.0,\n",
                "    'output_dir': '/kaggle/working'\n",
                "}\n",
                "\n",
                "# V5 Emotion columns (5 only, toxicity-relevant)\n",
                "EMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'neutral']\n",
                "\n",
                "print(\"V5 Configuration:\")\n",
                "print(f\"  Emotions: {EMO_COLS}\")\n",
                "print(f\"  LR: {CONFIG['lr']} | Dropout: {CONFIG['dropout']}\")\n",
                "print(f\"  Focal Gamma: {CONFIG['focal_gamma']}\")\n",
                "\n",
                "# Data paths\n",
                "DATA_DIR = None\n",
                "for path in ['/kaggle/input/aura-data-v5', '/kaggle/input/aura_data_v5', 'data/kaggle_upload_v5']:\n",
                "    if os.path.exists(path):\n",
                "        if os.path.exists(os.path.join(path, 'goemotions_v5.csv')):\n",
                "            DATA_DIR = path\n",
                "            GOEMO_FILE = 'goemotions_v5.csv'\n",
                "            break\n",
                "\n",
                "if DATA_DIR is None:\n",
                "    raise FileNotFoundError(\"V5 Dataset not found! Upload aura-data-v5\")\n",
                "\n",
                "print(f\"\\nDataset: {DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# MODEL: AURA Bayesian (BERT + Kendall Uncertainty)\n",
                "# ============================================================\n",
                "class AURA_Bayesian(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained(config['encoder'])\n",
                "        hidden_size = self.bert.config.hidden_size\n",
                "        self.dropout = nn.Dropout(config['dropout'])\n",
                "        \n",
                "        self.toxicity_head = nn.Linear(hidden_size, 2)\n",
                "        self.emotion_head = nn.Linear(hidden_size, config['num_emotion_classes'])\n",
                "        \n",
                "        # Homoscedastic Uncertainty (Kendall 2018)\n",
                "        self.tox_log_var = nn.Parameter(torch.zeros(1))\n",
                "        self.emo_log_var = nn.Parameter(torch.zeros(1))\n",
                "        \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = self.dropout(outputs.pooler_output)\n",
                "        \n",
                "        tox_logits = self.toxicity_head(pooled)\n",
                "        emo_logits = self.emotion_head(pooled)\n",
                "        \n",
                "        return tox_logits, emo_logits, self.tox_log_var, self.emo_log_var"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# LOSS FUNCTIONS\n",
                "# ============================================================\n",
                "def focal_loss_with_uncertainty(logits, log_var, targets, gamma=2.0, T=10):\n",
                "    \"\"\"\n",
                "    Focal Loss + Kendall Uncertainty (Lin 2017 + Kendall 2018)\n",
                "    \"\"\"\n",
                "    log_var_clamped = torch.clamp(log_var, min=-10, max=10)\n",
                "    std = torch.exp(0.5 * log_var_clamped)\n",
                "    \n",
                "    # Monte Carlo Sampling\n",
                "    logits_expanded = logits.unsqueeze(0).expand(T, -1, -1)\n",
                "    noise = torch.randn_like(logits_expanded)\n",
                "    corrupted_logits = logits_expanded + (noise * std)\n",
                "    \n",
                "    # Average probabilities\n",
                "    probs = F.softmax(corrupted_logits, dim=-1)\n",
                "    avg_probs = torch.mean(probs, dim=0)\n",
                "    \n",
                "    # Focal Loss\n",
                "    p_t = avg_probs[range(len(targets)), targets]\n",
                "    focal_weight = (1 - p_t) ** gamma\n",
                "    ce_loss = -torch.log(p_t + 1e-8)\n",
                "    focal_loss = (focal_weight * ce_loss).mean()\n",
                "    \n",
                "    return focal_loss + 0.5 * log_var_clamped\n",
                "\n",
                "\n",
                "def mc_uncertainty_loss_multilabel(logits, log_var, targets, T=10):\n",
                "    \"\"\"\n",
                "    Monte Carlo Uncertainty for Multi-Label (Emotions)\n",
                "    \"\"\"\n",
                "    log_var_clamped = torch.clamp(log_var, min=-10, max=10)\n",
                "    std = torch.exp(0.5 * log_var_clamped)\n",
                "    \n",
                "    logits_expanded = logits.unsqueeze(0).expand(T, -1, -1)\n",
                "    noise = torch.randn_like(logits_expanded)\n",
                "    corrupted_logits = logits_expanded + (noise * std)\n",
                "    \n",
                "    probs = torch.sigmoid(corrupted_logits)\n",
                "    avg_probs = torch.mean(probs, dim=0)\n",
                "    \n",
                "    bce = F.binary_cross_entropy(avg_probs, targets, reduction='mean')\n",
                "    return bce + 0.5 * log_var_clamped"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# DATASET\n",
                "# ============================================================\n",
                "class AURADataset(Dataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_length, is_toxicity=True, emo_cols=None):\n",
                "        self.df = pd.read_csv(csv_path)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.is_toxicity = is_toxicity\n",
                "        self.emo_cols = emo_cols or EMO_COLS\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = str(row.get('text', row.get('tweet', '')))\n",
                "        \n",
                "        enc = self.tokenizer.encode_plus(\n",
                "            text,\n",
                "            add_special_tokens=True,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        tox_label = -1\n",
                "        emo_label = torch.full((len(self.emo_cols),), -1.0)\n",
                "        \n",
                "        if self.is_toxicity:\n",
                "            label_raw = row['label'] if 'label' in row else row.get('subtask_a', 'NOT')\n",
                "            tox_label = 1 if label_raw in [1, 'OFF'] else 0\n",
                "        else:\n",
                "            emo_label = torch.tensor([float(row[c]) for c in self.emo_cols], dtype=torch.float32)\n",
                "        \n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(),\n",
                "            'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'toxicity_target': torch.tensor(tox_label, dtype=torch.long),\n",
                "            'emotion_target': emo_label,\n",
                "            'is_toxicity_task': torch.tensor(1 if self.is_toxicity else 0, dtype=torch.long)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# DATA LOADING\n",
                "# ============================================================\n",
                "tokenizer = BertTokenizer.from_pretrained(CONFIG['encoder'])\n",
                "\n",
                "# Load V5 datasets\n",
                "olid_train = AURADataset(f\"{DATA_DIR}/olid_train.csv\", tokenizer, CONFIG['max_length'], is_toxicity=True)\n",
                "olid_val = AURADataset(f\"{DATA_DIR}/olid_validation.csv\", tokenizer, CONFIG['max_length'], is_toxicity=True)\n",
                "goemo = AURADataset(f\"{DATA_DIR}/{GOEMO_FILE}\", tokenizer, CONFIG['max_length'], is_toxicity=False, emo_cols=EMO_COLS)\n",
                "\n",
                "# Verify data\n",
                "goemo_df = pd.read_csv(f\"{DATA_DIR}/{GOEMO_FILE}\")\n",
                "print(\"V5 Data Verification:\")\n",
                "print(f\"  OLID Train: {len(olid_train)} samples\")\n",
                "print(f\"  OLID Val: {len(olid_val)} samples\")\n",
                "print(f\"  GoEmotions: {len(goemo)} samples\")\n",
                "print(f\"  Emotions: {EMO_COLS}\")\n",
                "for col in EMO_COLS:\n",
                "    if col in goemo_df.columns:\n",
                "        print(f\"    {col}: {int(goemo_df[col].sum())} samples\")\n",
                "\n",
                "# Combine\n",
                "train_set = ConcatDataset([olid_train, goemo])\n",
                "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
                "val_loader = DataLoader(olid_val, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
                "\n",
                "print(f\"\\nTotal Training: {len(train_set)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# TRAINING FUNCTION\n",
                "# ============================================================\n",
                "def train_epoch(model, loader, optimizer, scheduler, epoch, config):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    tox_preds, tox_labels = [], []\n",
                "    \n",
                "    loop = tqdm(loader, desc=f\"Epoch {epoch}\", leave=True)\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    for step, batch in enumerate(loop):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        tox_targets = batch['toxicity_target'].to(device)\n",
                "        emo_targets = batch['emotion_target'].to(device)\n",
                "        is_tox_task = batch['is_toxicity_task'].to(device)\n",
                "        \n",
                "        tox_logits, emo_logits, tox_log_var, emo_log_var = model(input_ids, attention_mask)\n",
                "        \n",
                "        loss = torch.tensor(0.0, device=device)\n",
                "        \n",
                "        # Toxicity Loss (Focal)\n",
                "        tox_mask = is_tox_task == 1\n",
                "        if tox_mask.sum() > 0:\n",
                "            tox_loss = focal_loss_with_uncertainty(\n",
                "                tox_logits[tox_mask], \n",
                "                tox_log_var, \n",
                "                tox_targets[tox_mask],\n",
                "                gamma=config['focal_gamma'],\n",
                "                T=config['mc_samples']\n",
                "            )\n",
                "            loss = loss + tox_loss\n",
                "            \n",
                "            preds = torch.argmax(tox_logits[tox_mask], dim=1).cpu().numpy()\n",
                "            tox_preds.extend(preds)\n",
                "            tox_labels.extend(tox_targets[tox_mask].cpu().numpy())\n",
                "        \n",
                "        # Emotion Loss\n",
                "        emo_mask = is_tox_task == 0\n",
                "        if emo_mask.sum() > 0:\n",
                "            emo_loss = mc_uncertainty_loss_multilabel(\n",
                "                emo_logits[emo_mask], \n",
                "                emo_log_var, \n",
                "                emo_targets[emo_mask],\n",
                "                T=config['mc_samples']\n",
                "            )\n",
                "            loss = loss + emo_loss\n",
                "        \n",
                "        loss = loss / config['gradient_accumulation']\n",
                "        loss.backward()\n",
                "        \n",
                "        if (step + 1) % config['gradient_accumulation'] == 0:\n",
                "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            optimizer.step()\n",
                "            scheduler.step()\n",
                "            optimizer.zero_grad()\n",
                "        \n",
                "        total_loss += loss.item() * config['gradient_accumulation']\n",
                "        \n",
                "        sigma_tox = torch.exp(0.5 * tox_log_var).item()\n",
                "        sigma_emo = torch.exp(0.5 * emo_log_var).item()\n",
                "        loop.set_postfix(loss=loss.item(), s_tox=f\"{sigma_tox:.3f}\", s_emo=f\"{sigma_emo:.3f}\")\n",
                "    \n",
                "    avg_loss = total_loss / len(loader)\n",
                "    train_f1 = f1_score(tox_labels, tox_preds, average='macro') if tox_labels else 0\n",
                "    \n",
                "    return avg_loss, train_f1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# VALIDATION FUNCTION\n",
                "# ============================================================\n",
                "@torch.no_grad()\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    all_preds, all_labels = [], []\n",
                "    \n",
                "    for batch in tqdm(loader, desc=\"Validating\", leave=False):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        tox_targets = batch['toxicity_target'].to(device)\n",
                "        \n",
                "        tox_logits, _, _, _ = model(input_ids, attention_mask)\n",
                "        \n",
                "        loss = F.cross_entropy(tox_logits, tox_targets)\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        preds = torch.argmax(tox_logits, dim=1).cpu().numpy()\n",
                "        all_preds.extend(preds)\n",
                "        all_labels.extend(tox_targets.cpu().numpy())\n",
                "    \n",
                "    avg_loss = total_loss / len(loader)\n",
                "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
                "    \n",
                "    return avg_loss, val_f1, all_preds, all_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# MAIN TRAINING LOOP\n",
                "# ============================================================\n",
                "model = AURA_Bayesian(CONFIG).to(device)\n",
                "print(f\"Model: BERT (110M params) on {device}\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
                "\n",
                "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation']\n",
                "scheduler = OneCycleLR(optimizer, max_lr=CONFIG['lr'], total_steps=total_steps, pct_start=0.1)\n",
                "\n",
                "best_f1 = 0\n",
                "patience_counter = 0\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"STARTING V5 TRAINING (Balanced Data + Focal Loss + Optimized HP)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    print(f\"\\nEpoch {epoch}/{CONFIG['epochs']}\")\n",
                "    \n",
                "    train_loss, train_f1 = train_epoch(model, train_loader, optimizer, scheduler, epoch, CONFIG)\n",
                "    val_loss, val_f1, preds, labels = validate(model, val_loader)\n",
                "    \n",
                "    sigma_tox = torch.exp(0.5 * model.tox_log_var).item()\n",
                "    sigma_emo = torch.exp(0.5 * model.emo_log_var).item()\n",
                "    gap = abs(train_f1 - val_f1) * 100\n",
                "    \n",
                "    print(f\"   Train Loss: {train_loss:.4f} | Train F1: {train_f1:.4f}\")\n",
                "    print(f\"   Val Loss:   {val_loss:.4f} | Val F1:   {val_f1:.4f}\")\n",
                "    print(f\"   Gap: {gap:.1f}% | sigma_Tox: {sigma_tox:.4f} | sigma_Emo: {sigma_emo:.4f}\")\n",
                "    \n",
                "    if val_f1 > best_f1:\n",
                "        best_f1 = val_f1\n",
                "        patience_counter = 0\n",
                "        torch.save(model.state_dict(), f\"{CONFIG['output_dir']}/aura_v5_best.pt\")\n",
                "        print(f\"   NEW BEST! (F1: {best_f1:.4f})\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        print(f\"   No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
                "    \n",
                "    if patience_counter >= CONFIG['patience']:\n",
                "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
                "        break\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"TRAINING COMPLETE | Best Val F1: {best_f1:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# FINAL EVALUATION\n",
                "# ============================================================\n",
                "model.load_state_dict(torch.load(f\"{CONFIG['output_dir']}/aura_v5_best.pt\"))\n",
                "val_loss, val_f1, preds, labels = validate(model, val_loader)\n",
                "\n",
                "print(\"\\nFINAL CLASSIFICATION REPORT (V5 - Production Model)\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(labels, preds, target_names=['NOT', 'OFF']))\n",
                "\n",
                "print(f\"\\nFinal Macro-F1: {val_f1:.4f}\")\n",
                "print(f\"Model saved: {CONFIG['output_dir']}/aura_v5_best.pt\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
