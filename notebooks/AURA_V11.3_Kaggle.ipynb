{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# AURA V11.3 — GradNorm Multi-Task Gradient Balancing\n\n**Base:** V11 (RoBERTa-base + all 4 fixes retained)\n\n## New in V11.3\n- **GradNorm** (Chen et al., 2018) replaces Kendall Uncertainty Loss\n- Dynamically balances gradient norms across tasks so no single task dominates the shared encoder\n\n## V11 Fixes Retained\n1. Proper Emotion/Sentiment Validation (official dev splits)\n2. Task Mask in Multi-Task Loss\n3. Optimizer State Reset on Unfreeze\n4. No Data Leak in Emotion Evaluation\n\n## Reverted from V11.2\n- Task-Weighted Sampling reverted to uniform shuffle (oversampling caused overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_seed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Seed — IDENTICAL TO V10.2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import (\n    f1_score, classification_report, confusion_matrix, \n    multilabel_confusion_matrix, precision_recall_fscore_support\n)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility — SAME SEED AS V10.2\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'\\U0001f527 Device: {device}')\nif device.type == 'cuda':\n    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration — V11.3: GradNorm params added\nCONFIG = {\n    # Model\n    'encoder': 'roberta-base',\n    'hidden_dim': 768,\n    'n_heads': 8,\n    'num_emotion_classes': 7,\n    'max_length': 128,\n    'dropout': 0.3,\n    \n    # Training\n    'batch_size': 16,\n    'gradient_accumulation': 4,  # Effective batch = 64\n    'epochs': 10,  # SAME AS V10.2 FINAL RUN\n    'lr_encoder': 1e-5,\n    'lr_heads': 5e-5,\n    'weight_decay': 0.01,\n    'max_grad_norm': 1.0,\n    'warmup_ratio': 0.1,\n    \n    # Regularization (Module 3)\n    'focal_gamma': 2.0,\n    'label_smoothing': 0.1,\n    'patience': 5,\n    'freezing_epochs': 1,\n    # V11.3: GradNorm hyperparameters (Chen et al., 2018)\n    'gradnorm_alpha': 1.5,   # Asymmetry: higher = more aggressive rebalancing\n    'gradnorm_lr': 0.025,    # SGD learning rate for task weight updates\n}\n\nDATA_DIR = '/kaggle/input/aura-v11-data'\nEMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n\nprint('\\U0001f4cb V11 Configuration:')\nfor k, v in CONFIG.items():\n    print(f'   {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Functions — IDENTICAL TO V10.2\ndef plot_class_distribution(df, label_col, title, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 4))\n    counts = df[label_col].value_counts().sort_index()\n    bars = ax.bar(counts.index.astype(str), counts.values, color=['#66c2a5', '#fc8d62'])\n    ax.set_title(title)\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Count')\n    for bar, count in zip(bars, counts.values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n                str(count), ha='center', fontsize=10)\n    return ax\n\ndef plot_confusion_matrix_heatmap(y_true, y_pred, labels, title='Confusion Matrix', ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 5))\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=labels, yticklabels=labels, ax=ax,\n                cbar_kws={'label': 'Count'})\n    ax.set_title(title)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    return ax\n\ndef plot_multilabel_confusion_matrices(y_true, y_pred, labels, normalize=True):\n    cms = multilabel_confusion_matrix(y_true, y_pred)\n    n_labels = len(labels)\n    cols = min(4, n_labels)\n    rows = (n_labels + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axes = axes.flatten() if n_labels > 1 else [axes]\n    \n    for i, (cm, label) in enumerate(zip(cms, labels)):\n        ax = axes[i]\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n            fmt = '.2f'\n        else:\n            fmt = 'd'\n        sns.heatmap(cm, annot=True, fmt=fmt, cmap='YlGnBu', ax=ax,\n                    xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'],\n                    vmin=0, vmax=1 if normalize else None, cbar=False)\n        ax.set_title(label, fontsize=10)\n        ax.set_ylabel('Actual')\n        ax.set_xlabel('Predicted')\n    \n    for i in range(n_labels, len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Multilabel Confusion Matrices (Normalized)', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_history(history):\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    axes[0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-o', label='Train')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    axes[1].plot(range(1, len(history['val_f1'])+1), history['val_f1'], 'g-o', label='Val F1')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Macro F1')\n    axes[1].set_title('Validation F1 Score')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    weights = np.array(history['task_weights'])\n    for i, name in enumerate(['Toxicity', 'Emotion', 'Sentiment', 'Reporting']):\n        axes[2].plot(range(1, len(weights)+1), weights[:, i], '-o', label=name)\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Weight')\n    axes[2].set_title('GradNorm Task Weights')\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint('\\U0001f4ca Visualization functions loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mha_module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Task-Specific Multi-Head Attention Module — IDENTICAL TO V10.2\nclass TaskSpecificMHA(nn.Module):\n    \"\"\"Multi-Head Self-Attention per task (Module 2: Redundancy Principle).\n    \n    Each task gets its own attention mechanism to learn WHERE to look.\n    - Toxicity: looks for 'You' + insults\n    - Reporting: looks for 'said', 'claims'\n    - Sentiment: looks for adjectives\n    \"\"\"\n    def __init__(self, hidden_dim, n_heads, dropout=0.1):\n        super().__init__()\n        self.mha = nn.MultiheadAttention(\n            embed_dim=hidden_dim, \n            num_heads=n_heads, \n            batch_first=True, \n            dropout=dropout\n        )\n        self.layernorm = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, hidden_states, attention_mask):\n        key_padding_mask = (attention_mask == 0)\n        attn_output, attn_weights = self.mha(\n            query=hidden_states, \n            key=hidden_states, \n            value=hidden_states,\n            key_padding_mask=key_padding_mask\n        )\n        output = self.layernorm(hidden_states + self.dropout(attn_output))\n        return output, attn_weights\n\nprint('\\U0001f9e0 TaskSpecificMHA module defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: AURA V10 Model — V11.3: added shared_rep for GradNorm\nclass AURA_V10(nn.Module):\n    \"\"\"AURA V10: RoBERTa + 4 Parallel Task-Specific MHSA Blocks.\"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained(config['encoder'])\n        hidden = config['hidden_dim']\n        \n        # 4 Parallel MHSA Blocks (Feature Disentanglement)\n        self.tox_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.emo_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.sent_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        self.report_mha = TaskSpecificMHA(hidden, config['n_heads'], config['dropout'])\n        \n        self.dropout = nn.Dropout(config['dropout'])\n        \n        # Classification Heads\n        self.toxicity_head = nn.Linear(hidden, 2)\n        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n        self.sentiment_head = nn.Linear(hidden, 2)\n        self.reporting_head = nn.Linear(hidden, 1)\n        \n        # Bias Initialization (NB11: Imbalanced Datasets)\n        with torch.no_grad():\n            self.toxicity_head.bias[0] = 2.5   # Non-Toxic\n            self.toxicity_head.bias[1] = -2.5  # Toxic\n\n    def _mean_pool(self, seq, mask):\n        mask_exp = mask.unsqueeze(-1).expand(seq.size()).float()\n        return (seq * mask_exp).sum(dim=1) / mask_exp.sum(dim=1).clamp(min=1e-9)\n\n    def forward(self, input_ids, attention_mask):\n        shared = self.roberta(input_ids, attention_mask).last_hidden_state\n        \n        tox_seq, _ = self.tox_mha(shared, attention_mask)\n        emo_seq, _ = self.emo_mha(shared, attention_mask)\n        sent_seq, _ = self.sent_mha(shared, attention_mask)\n        rep_seq, _ = self.report_mha(shared, attention_mask)\n        \n        return {\n            'toxicity': self.toxicity_head(self.dropout(self._mean_pool(tox_seq, attention_mask))),\n            'emotion': self.emotion_head(self.dropout(self._mean_pool(emo_seq, attention_mask))),\n            'sentiment': self.sentiment_head(self.dropout(self._mean_pool(sent_seq, attention_mask))),\n            'reporting': self.reporting_head(self.dropout(self._mean_pool(rep_seq, attention_mask))).squeeze(-1),\n            'shared_rep': shared  # V11.3: encoder output for GradNorm gradient computation\n        }\n\nprint('\\U0001f985 AURA_V10 model defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Loss Functions\n# V11.3: GradNorm replaces Kendall Uncertainty Loss (Chen et al., 2018)\n\ndef focal_loss(logits, targets, weight=None, gamma=2.0, smoothing=0.0):\n    \"\"\"Focal loss for binary/multiclass classification — IDENTICAL to V10.2.\"\"\"\n    n_classes = logits.size(-1)\n    if smoothing > 0:\n        with torch.no_grad():\n            smooth = torch.full_like(logits, smoothing / (n_classes - 1))\n            smooth.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n        log_probs = F.log_softmax(logits, dim=-1)\n        ce = -(smooth * log_probs).sum(dim=-1)\n    else:\n        ce = F.cross_entropy(logits, targets, weight=weight, reduction='none')\n    \n    pt = torch.exp(-ce)\n    focal = ((1 - pt) ** gamma) * ce\n    return focal.mean()\n\n\nclass GradNormLoss(nn.Module):\n    \"\"\"GradNorm: Gradient Normalization for Adaptive Loss Balancing.\n    \n    Chen et al., 2018 — 'GradNorm: Gradient Normalization for Adaptive \n    Loss Balancing in Deep Multitask Networks'\n    \n    Key idea: balance the gradient norms from each task w.r.t. the shared \n    encoder output. If one task's gradient is too large, reduce its weight;\n    if too small, increase it. This prevents any single task from dominating \n    the shared encoder's learning.\n    \n    Algorithm per step:\n      1. Compute g_i = ||grad(L_i, shared_rep)||_2 for each active task\n      2. Compute weighted norms G_i = w_i * g_i\n      3. Compute relative training rate r_i = L_i(t) / L_i(0)\n      4. Target: G_target_i = mean(G) * (r_i / mean(r))^alpha\n      5. Update w_i via SGD: dLoss/dw_i = g_i * sign(G_i - target_i)\n      6. Renormalize: w_i = w_i * n_tasks / sum(w_i)\n    \"\"\"\n    def __init__(self, n_tasks, alpha=1.5, lr=0.025):\n        super().__init__()\n        self.n_tasks = n_tasks\n        self.alpha = alpha\n        self.lr = lr\n        \n        # Learnable task weights (all start at 1.0)\n        self.task_weights = nn.Parameter(torch.ones(n_tasks))\n        \n        # Track initial losses for relative training rate\n        self.register_buffer('initial_losses', torch.zeros(n_tasks))\n        self.initial_losses_set = False\n        \n    def set_initial_losses(self, losses):\n        \"\"\"Call after first epoch with average per-task losses.\"\"\"\n        self.initial_losses.copy_(losses.detach())\n        self.initial_losses_set = True\n        print(f'   GradNorm initial losses set: {self.initial_losses.cpu().numpy().round(4)}')\n    \n    def forward(self, task_losses, task_mask=None):\n        \"\"\"Weighted sum of task losses.\n        \n        Weights are DETACHED so the main optimizer doesn't update them —\n        only gradnorm_step() modifies the weights.\n        \"\"\"\n        total = torch.tensor(0.0, device=self.task_weights.device)\n        for i in range(self.n_tasks):\n            if task_mask is not None and not task_mask[i]:\n                continue\n            total = total + self.task_weights[i].detach() * task_losses[i]\n        return total\n    \n    def gradnorm_step(self, task_losses, shared_rep, task_mask=None):\n        \"\"\"Perform one GradNorm weight update.\n        \n        Uses analytical gradient: since G_i = w_i * g_i (linearity of gradient),\n        dLoss/dw_i = g_i * sign(G_i - target_i). No create_graph needed.\n        \n        Args:\n            task_losses: list of per-task loss tensors (must be in computation graph)\n            shared_rep: the encoder output tensor (all task losses flow through this)\n            task_mask: list of bools — which tasks are present in this batch\n        \"\"\"\n        if not self.initial_losses_set:\n            return\n        \n        # Step 1: Compute unweighted gradient norms g_i = ||grad(L_i, shared)||\n        g_norms = []\n        active = []\n        for i in range(self.n_tasks):\n            if task_mask is not None and not task_mask[i]:\n                continue\n            # Skip dummy zero-loss tensors (requires_grad=False)\n            if not task_losses[i].requires_grad:\n                continue\n            grad = torch.autograd.grad(\n                task_losses[i], shared_rep, \n                retain_graph=True, create_graph=False\n            )[0]\n            g_norms.append(grad.norm().item())\n            active.append(i)\n        \n        if len(g_norms) < 2:\n            return  # Need at least 2 tasks to balance\n        \n        with torch.no_grad():\n            # Step 2: Weighted gradient norms G_i = w_i * g_i\n            G = [self.task_weights[i].item() * g for i, g in zip(active, g_norms)]\n            G_avg = sum(G) / len(G)\n            \n            # Step 3: Relative inverse training rates\n            r = [task_losses[i].item() / max(self.initial_losses[i].item(), 1e-8)\n                 for i in active]\n            r_avg = sum(r) / len(r)\n            r_tilde = [ri / max(r_avg, 1e-8) for ri in r]\n            \n            # Step 4: Target gradient norms\n            targets = [G_avg * (ri ** self.alpha) for ri in r_tilde]\n            \n            # Step 5: Manual SGD update\n            # Analytical gradient: dLoss/dw_i = g_i * sign(G_i - target_i)\n            for j, i in enumerate(active):\n                sign = 1.0 if G[j] > targets[j] else -1.0\n                self.task_weights.data[i] -= self.lr * g_norms[j] * sign\n            \n            # Step 6: Renormalize weights to sum to n_tasks\n            self.task_weights.data.clamp_(min=0.1)\n            self.task_weights.data *= self.n_tasks / self.task_weights.data.sum()\n    \n    def get_weights(self):\n        return self.task_weights.detach().cpu().numpy()\n\n\nprint('\\u2696\\ufe0f Loss functions defined (Focal + GradNorm).')\nprint('   V11.3: GradNorm replaces Kendall Uncertainty for gradient balancing.')\nprint('   V11 FIX retained: task_mask support (absent tasks skipped).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Dataset Classes — IDENTICAL TO V10.2\nclass BaseDataset(Dataset):\n    def __init__(self, path, tokenizer, max_len):\n        self.df = pd.read_csv(path)\n        self.tok = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self): \n        return len(self.df)\n    \n    def encode(self, text):\n        return self.tok(\n            str(text), max_length=self.max_len, \n            padding='max_length', truncation=True, return_tensors='pt'\n        )\n\nclass ToxicityDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'tox': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 0\n        }\n\nclass EmotionDataset(BaseDataset):\n    def __init__(self, path, tokenizer, max_len, cols):\n        super().__init__(path, tokenizer, max_len)\n        self.cols = cols\n        if 'label_sum' in self.df.columns:\n            self.df = self.df[self.df['label_sum'] > 0].reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'emo': torch.tensor([float(row[c]) for c in self.cols], dtype=torch.float), \n            'task': 1\n        }\n\nclass SentimentDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'sent': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 2\n        }\n\nclass ReportingDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'rep': torch.tensor(int(row['is_reporting']), dtype=torch.long), \n            'task': 3\n        }\n\ndef collate_fn(batch):\n    ids = torch.stack([x['ids'] for x in batch])\n    mask = torch.stack([x['mask'] for x in batch])\n    tasks = torch.tensor([x['task'] for x in batch])\n    \n    tox_items = [x['tox'] for x in batch if x['task'] == 0]\n    emo_items = [x['emo'] for x in batch if x['task'] == 1]\n    sent_items = [x['sent'] for x in batch if x['task'] == 2]\n    rep_items = [x['rep'] for x in batch if x['task'] == 3]\n    \n    return {\n        'ids': ids, 'mask': mask, 'tasks': tasks,\n        'tox': torch.stack(tox_items) if tox_items else None,\n        'emo': torch.stack(emo_items) if emo_items else None,\n        'sent': torch.stack(sent_items) if sent_items else None,\n        'rep': torch.stack(rep_items) if rep_items else None\n    }\n\nprint('\\U0001f4e6 Dataset classes defined \\u2014 identical to V10.2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load Data\n# V11 FIX #1: Load proper held-out validation sets for emotion and sentiment.\n# These come from official GoEmotions dev and SST-2 dev splits,\n# generated by prepare_v11_datasets.py. No data leak.\n\ntokenizer = RobertaTokenizer.from_pretrained(CONFIG['encoder'])\n\n# Training sets\ntox_train = ToxicityDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'])\nemo_train = EmotionDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\nsent_train = SentimentDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'])\nrep_train = ReportingDataset(f'{DATA_DIR}/reporting_examples_augmented.csv', tokenizer, CONFIG['max_length'])\n\n# Validation sets — V11: ALL from official held-out splits\ntox_val = ToxicityDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, CONFIG['max_length'])\nemo_val = EmotionDataset(f'{DATA_DIR}/emotions_val.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\nsent_val = SentimentDataset(f'{DATA_DIR}/sentiment_val.csv', tokenizer, CONFIG['max_length'])\n\n# Combined training loader\ntrain_ds = ConcatDataset([tox_train, emo_train, sent_train, rep_train])\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, \n                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n\n# Separate validation loaders per task\ntox_val_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\nemo_val_loader = DataLoader(emo_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\nsent_val_loader = DataLoader(sent_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\n\nprint('='*60)\nprint('\\U0001f4ca DATASET SUMMARY')\nprint('='*60)\nprint(f'Training Samples: {len(train_ds):,}')\nprint(f'  \\u251c\\u2500 Toxicity:  {len(tox_train):,}')\nprint(f'  \\u251c\\u2500 Emotion:   {len(emo_train):,}')\nprint(f'  \\u251c\\u2500 Sentiment: {len(sent_train):,}')\nprint(f'  \\u2514\\u2500 Reporting: {len(rep_train):,}')\nprint(f'Validation Samples:')\nprint(f'  \\u251c\\u2500 Toxicity:  {len(tox_val):,} (OLID official test)')\nprint(f'  \\u251c\\u2500 Emotion:   {len(emo_val):,} (GoEmotions official dev)')\nprint(f'  \\u2514\\u2500 Sentiment: {len(sent_val):,} (SST-2 official dev)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_dist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Data Distribution Analysis (NB11 Pattern)\nprint('='*60)\nprint('\\U0001f4c8 CLASS DISTRIBUTION ANALYSIS (NB11)')\nprint('='*60)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 1. Toxicity Distribution\ntox_df = pd.read_csv(f'{DATA_DIR}/toxicity_train.csv')\nplot_class_distribution(tox_df, 'label', 'Toxicity: Class Distribution', axes[0, 0])\naxes[0, 0].set_xticklabels(['Non-Toxic (0)', 'Toxic (1)'])\n\n# 2. Task Sample Distribution\ntask_counts = {'Toxicity': len(tox_train), 'Emotion': len(emo_train), \n               'Sentiment': len(sent_train), 'Reporting': len(rep_train)}\ncolors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3']\nbars = axes[0, 1].bar(task_counts.keys(), task_counts.values(), color=colors)\naxes[0, 1].set_title('Task Sample Distribution')\naxes[0, 1].set_ylabel('Count')\nfor bar, count in zip(bars, task_counts.values()):\n    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500, \n                    f'{count:,}', ha='center', fontsize=9)\n\n# 3. Emotion Label Distribution (Multilabel)\nemo_df = pd.read_csv(f'{DATA_DIR}/emotions_train.csv')\nif 'label_sum' in emo_df.columns:\n    emo_df = emo_df[emo_df['label_sum'] > 0]\nemo_counts = emo_df[EMO_COLS].sum().sort_values(ascending=True)\nemo_counts.plot(kind='barh', ax=axes[1, 0], color='#8da0cb')\naxes[1, 0].set_title('Emotion Label Distribution')\naxes[1, 0].set_xlabel('Count')\n\n# 4. # of Labels per Sample\nif 'label_sum' in emo_df.columns:\n    label_counts = emo_df['label_sum'].value_counts().sort_index()\nelse:\n    label_counts = emo_df[EMO_COLS].sum(axis=1).value_counts().sort_index()\nlabel_counts.plot(kind='bar', ax=axes[1, 1], color='#fc8d62')\naxes[1, 1].set_title('Emotion: # of Labels per Sample')\naxes[1, 1].set_xlabel('Number of Emotion Labels')\naxes[1, 1].set_ylabel('Count')\naxes[1, 1].tick_params(axis='x', rotation=0)\n\nplt.tight_layout()\nplt.show()\n\nneg, pos = tox_df['label'].value_counts().sort_index()\nprint(f'\\n\\u26a0\\ufe0f Toxicity Imbalance: {neg:,} Non-Toxic vs {pos:,} Toxic ({pos/(neg+pos)*100:.1f}% minority class)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Model & Optimizer Setup\n# V11.3: GradNormLoss replaces UncertaintyLoss\n\nmodel = AURA_V10(CONFIG).to(device)\n\n# V11.3: GradNorm loss (replaces Kendall UncertaintyLoss)\nloss_fn = GradNormLoss(\n    n_tasks=4, \n    alpha=CONFIG['gradnorm_alpha'],\n    lr=CONFIG['gradnorm_lr']\n).to(device)\n\n# Separate encoder and head parameters\nencoder_params = set(model.roberta.parameters())\nhead_params = [p for p in model.parameters() if p not in encoder_params]\n\noptimizer = torch.optim.AdamW([\n    {'params': model.roberta.parameters(), 'lr': CONFIG['lr_encoder']},\n    {'params': head_params, 'lr': CONFIG['lr_heads']}\n], weight_decay=CONFIG['weight_decay'])\n\n# Toxicity class weights — IDENTICAL to V10.2/V11\ntox_weights = torch.tensor([0.5, 2.0]).to(device)\n\ntotal_steps = (len(train_loader) // CONFIG['gradient_accumulation']) * CONFIG['epochs']\nwarmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n)\n\nn_params = sum(p.numel() for p in model.parameters())\nn_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('='*60)\nprint('\\U0001f3d7\\ufe0f MODEL SETUP')\nprint('='*60)\nprint(f'Encoder: {CONFIG[\"encoder\"]}')\nprint(f'Total parameters:     {n_params:,}')\nprint(f'Trainable parameters: {n_train:,}')\nprint(f'Total optimization steps: {total_steps}')\nprint(f'Warmup steps: {warmup_steps}')\nprint(f'Effective batch size: {CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation\"]}')\nprint(f'V11.3: GradNorm (alpha={CONFIG[\"gradnorm_alpha\"]}, lr={CONFIG[\"gradnorm_lr\"]})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training & Evaluation Functions\n# V11.3: GradNorm gradient balancing integrated into train_epoch.\n# All V11 fixes retained: task_mask, optimizer state reset.\n# GradNorm runs every accumulation cycle from epoch 2 onwards.\n\nepoch_task_losses = None  # Per-epoch loss tracker for initial loss computation\n\ndef train_epoch(epoch):\n    global epoch_task_losses\n    epoch_task_losses = {i: [] for i in range(4)}\n    model.train()\n    \n    # Progressive freezing — IDENTICAL to V11\n    if epoch <= CONFIG['freezing_epochs']:\n        print(f'\\u2744\\ufe0f Epoch {epoch}: RoBERTa FROZEN')\n        for p in model.roberta.parameters(): \n             p.requires_grad = False\n    else:\n        # --- V11 FIX #4: Reset Adam states on first unfreeze ---\n        if epoch == CONFIG['freezing_epochs'] + 1:\n            print(f'\\U0001f525 Epoch {epoch}: RoBERTa UNFROZEN (resetting optimizer states)')\n            for p in model.roberta.parameters():\n                p.requires_grad = True\n                # Clear stale Adam momentum/variance from frozen epochs\n                if p in optimizer.state:\n                    del optimizer.state[p]\n        else:\n            print(f'\\U0001f525 Epoch {epoch}: RoBERTa UNFROZEN')\n            for p in model.roberta.parameters(): \n                 p.requires_grad = True\n    \n    total_loss = 0\n    optimizer.zero_grad()\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', mininterval=10.0)\n    \n    for step, batch in enumerate(pbar):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        tasks = batch['tasks']\n        \n        out = model(ids, mask)\n        shared_rep = out['shared_rep']  # V11.3: encoder output for GradNorm\n        \n        # Compute per-task losses — IDENTICAL to V10.2/V11\n        losses = []\n        task_mask = []  # V11 FIX #3: track which tasks are present\n        \n        # Toxicity\n        if batch['tox'] is not None and (tasks == 0).sum() > 0:\n            losses.append(focal_loss(\n                out['toxicity'][tasks == 0], batch['tox'].to(device), \n                weight=tox_weights, smoothing=CONFIG['label_smoothing']\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Emotion (Multilabel BCE)\n        if batch['emo'] is not None and (tasks == 1).sum() > 0:\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['emotion'][tasks == 1], batch['emo'].to(device)\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Sentiment\n        if batch['sent'] is not None and (tasks == 2).sum() > 0:\n            losses.append(focal_loss(\n                out['sentiment'][tasks == 2], batch['sent'].to(device), \n                smoothing=CONFIG['label_smoothing']\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Reporting\n        if batch['rep'] is not None and (tasks == 3).sum() > 0:\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['reporting'][tasks == 3], batch['rep'].float().to(device)\n            ))\n            task_mask.append(True)\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            task_mask.append(False)\n            \n        # Skip fully empty batches\n        if not any(task_mask):\n            print(f\"\\u26a0\\ufe0f Warning: Empty batch at step {step}, skipping\")\n            optimizer.zero_grad()\n            continue\n\n        # Track per-task losses for GradNorm initial loss computation\n        for i in range(4):\n            if task_mask[i]:\n                epoch_task_losses[i].append(losses[i].item())\n\n        # V11.3: GradNorm step BEFORE main backward (needs computation graph alive)\n        # Only runs after initial losses are set (i.e., from epoch 2 onwards)\n        is_accum_step = (step + 1) % CONFIG['gradient_accumulation'] == 0\n        if is_accum_step and loss_fn.initial_losses_set:\n            loss_fn.gradnorm_step(losses, shared_rep, task_mask=task_mask)\n\n        # GradNorm weighted loss — V11.3: replaces Kendall loss\n        loss = loss_fn(losses, task_mask=task_mask) / CONFIG['gradient_accumulation']\n        \n        # NaN/Inf safety check\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"\\u26a0\\ufe0f Warning: Invalid loss {loss.item():.4f} at step {step}, skipping batch\")\n            optimizer.zero_grad()\n            continue\n\n        loss.backward()\n        \n        # Gradient Accumulation\n        if is_accum_step:\n            nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n        total_loss += loss.item() * CONFIG['gradient_accumulation']\n        if step % 50 == 0: pbar.set_postfix({'loss': f'{loss.item() * CONFIG[\"gradient_accumulation\"]:.3f}'})\n    \n    # V11.3: After first epoch, set GradNorm initial losses\n    if not loss_fn.initial_losses_set:\n        avg_losses = torch.tensor([\n            np.mean(epoch_task_losses[i]) if epoch_task_losses[i] else 1.0\n            for i in range(4)\n        ])\n        loss_fn.set_initial_losses(avg_losses)\n        \n    return total_loss / len(train_loader)\n\n@torch.no_grad()\ndef evaluate_toxicity():\n    \"\"\"Evaluate toxicity on held-out OLID test set.\"\"\"\n    model.eval()\n    preds, trues = [], []\n    for batch in tox_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n    return f1_score(trues, preds, average='macro', zero_division=0)\n\n@torch.no_grad()\ndef evaluate_emotion():\n    \"\"\"Evaluate emotion on held-out GoEmotions dev set (V11 FIX #2).\"\"\"\n    model.eval()\n    all_preds, all_trues = [], []\n    for batch in emo_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        pred = (torch.sigmoid(out['emotion']) > 0.5).cpu().numpy()\n        all_preds.append(pred)\n        all_trues.append(batch['emo'].numpy())\n    all_preds = np.concatenate(all_preds)\n    all_trues = np.concatenate(all_trues)\n    # Per-emotion F1, then average\n    f1s = []\n    for i in range(len(EMO_COLS)):\n        f1s.append(f1_score(all_trues[:, i], all_preds[:, i], average='binary', zero_division=0))\n    return np.mean(f1s)\n\n@torch.no_grad()\ndef evaluate_sentiment():\n    \"\"\"Evaluate sentiment on held-out SST-2 dev set (V11 FIX #2).\"\"\"\n    model.eval()\n    preds, trues = [], []\n    for batch in sent_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['sentiment'].argmax(1).cpu().numpy())\n        trues.extend(batch['sent'].numpy())\n    return f1_score(trues, preds, average='macro', zero_division=0)\n\nprint('\\U0001f3af Training & evaluation functions defined.')\nprint('   V11.3: GradNorm gradient balancing integrated.')\nprint('   V11 FIX #3: task_mask for absent tasks (retained).')\nprint('   V11 FIX #4: Optimizer states reset on RoBERTa unfreeze (retained).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Main Training Loop\n# V11.3: GradNorm weights tracked per epoch.\n\nprint('='*60)\nprint('\\U0001f680 AURA V11.3 \\u2014 TRAINING START (RoBERTa + GradNorm)')\nprint('='*60)\n\nbest_f1 = 0\npatience_counter = 0\nhistory = {\n    'train_loss': [], \n    'val_f1': [],            # Toxicity (primary)\n    'val_emo_f1': [],        # Emotion (V11)\n    'val_sent_f1': [],       # Sentiment (V11)\n    'task_weights': []\n}\n\nfor epoch in range(1, CONFIG['epochs'] + 1):\n    train_loss = train_epoch(epoch)\n    val_f1 = evaluate_toxicity()\n    emo_f1 = evaluate_emotion()\n    sent_f1 = evaluate_sentiment()\n    weights = loss_fn.get_weights()\n    \n    history['train_loss'].append(train_loss)\n    history['val_f1'].append(val_f1)\n    history['val_emo_f1'].append(emo_f1)\n    history['val_sent_f1'].append(sent_f1)\n    history['task_weights'].append(weights.copy())\n    \n    print(f'\\nEpoch {epoch} Summary:')\n    print(f'  Train Loss:     {train_loss:.4f}')\n    print(f'  Toxicity Val F1:  {val_f1:.4f}')\n    print(f'  Emotion Val F1:   {emo_f1:.4f}')\n    print(f'  Sentiment Val F1: {sent_f1:.4f}')\n    print(f'  GradNorm Weights [Tox/Emo/Sent/Rep]: {weights.round(3)}')\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        best_emo_f1 = emo_f1\n        best_sent_f1 = sent_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'aura_v11.3_best.pt')\n        print('  >>> BEST MODEL SAVED <<<')\n    else:\n        patience_counter += 1\n        print(f'  (No improvement. Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n        if patience_counter >= CONFIG['patience']:\n            print(f'\\n\\u26a0\\ufe0f Early stopping at epoch {epoch}')\n            break\n\nprint('\\n' + '='*60)\nprint(f'\\u2705 Training Complete.')\nprint(f'   Best Toxicity F1:  {best_f1:.4f}')\nprint(f'   Best Emotion F1:   {best_emo_f1:.4f}')\nprint(f'   Best Sentiment F1: {best_sent_f1:.4f}')\nprint('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Training History Visualization\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Loss\naxes[0, 0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-o', label='Train')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Toxicity F1\naxes[0, 1].plot(range(1, len(history['val_f1'])+1), history['val_f1'], 'g-o', label='Toxicity Val F1')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('Macro F1')\naxes[0, 1].set_title('Toxicity Validation F1')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Emotion + Sentiment F1\naxes[1, 0].plot(range(1, len(history['val_emo_f1'])+1), history['val_emo_f1'], 'r-o', label='Emotion Val F1')\naxes[1, 0].plot(range(1, len(history['val_sent_f1'])+1), history['val_sent_f1'], 'm-o', label='Sentiment Val F1')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('F1')\naxes[1, 0].set_title('Auxiliary Task Validation F1')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Kendall Weights\nweights = np.array(history['task_weights'])\nfor i, name in enumerate(['Toxicity', 'Emotion', 'Sentiment', 'Reporting']):\n    axes[1, 1].plot(range(1, len(weights)+1), weights[:, i], '-o', label=name)\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Weight')\naxes[1, 1].set_title('GradNorm Task Weights')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_toxicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Final Evaluation — Toxicity\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: TOXICITY')\nprint('='*60)\n\nmodel.load_state_dict(torch.load('aura_v11.3_best.pt'))\nmodel.eval()\n\npreds, trues = [], []\nwith torch.no_grad():\n    for batch in tox_val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n\nprint('\\n--- Classification Report ---')\nprint(classification_report(trues, preds, target_names=['Non-Toxic', 'Toxic']))\n\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_confusion_matrix_heatmap(trues, preds, ['Non-Toxic', 'Toxic'], 'Toxicity Confusion Matrix', ax)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_emotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Final Evaluation — Emotion\n# V11 FIX #2: Uses official GoEmotions dev split instead of training data tail.\n# This eliminates the data leak present in V10.2.\n\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: EMOTION (GoEmotions Dev)')\nprint('='*60)\n\nemo_preds, emo_trues = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(emo_val_loader, desc='Evaluating Emotions'):\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        pred = (torch.sigmoid(out['emotion']) > 0.5).cpu().numpy()\n        emo_preds.append(pred)\n        emo_trues.append(batch['emo'].numpy())\n\nemo_preds = np.concatenate(emo_preds)\nemo_trues = np.concatenate(emo_trues)\n\n# Per-emotion metrics\nprint('\\n--- Per-Emotion Metrics ---')\nfor i, emo in enumerate(EMO_COLS):\n    p, r, f1, _ = precision_recall_fscore_support(emo_trues[:, i], emo_preds[:, i], average='binary', zero_division=0)\n    print(f'{emo:10s}: P={p:.3f}, R={r:.3f}, F1={f1:.3f}')\n\n# Multilabel Confusion Matrices\nprint('\\n--- Multilabel Confusion Matrices ---')\nplot_multilabel_confusion_matrices(emo_trues, emo_preds, EMO_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_sentiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Final Evaluation — Sentiment (V11 NEW)\n# Uses official SST-2 dev split for evaluation.\n\nprint('='*60)\nprint('\\U0001f52c FINAL EVALUATION: SENTIMENT (SST-2 Dev)')\nprint('='*60)\n\nsent_preds, sent_trues = [], []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(sent_val_loader, desc='Evaluating Sentiment'):\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        sent_preds.extend(out['sentiment'].argmax(1).cpu().numpy())\n        sent_trues.extend(batch['sent'].numpy())\n\nprint('\\n--- Classification Report ---')\nprint(classification_report(sent_trues, sent_preds, target_names=['Negative', 'Positive']))\n\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_confusion_matrix_heatmap(sent_trues, sent_preds, ['Negative', 'Positive'], 'Sentiment Confusion Matrix', ax)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stress_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Qualitative Stress Test — IDENTICAL TO V10.2\nprint('='*60)\nprint('\\U0001f9ea QUALITATIVE STRESS TEST')\nprint('='*60)\n\ntest_cases = [\n    (\"I hate rainy Mondays\", \"Non-Toxic\"),\n    (\"This soup is disgusting\", \"Non-Toxic\"),\n    (\"I am so angry at the traffic\", \"Non-Toxic\"),\n    (\"This movie was terrible\", \"Non-Toxic\"),\n    \n    (\"You are an idiot\", \"Toxic\"),\n    (\"I hate you so much\", \"Toxic\"),\n    (\"Go kill yourself\", \"Toxic\"),\n    (\"You are worthless garbage\", \"Toxic\"),\n    \n    (\"He said you are an idiot\", \"Non-Toxic\"),\n    (\"The article discusses hate speech\", \"Non-Toxic\"),\n    (\"Someone wrote 'go die' in the comments\", \"Non-Toxic\"),\n]\n\nprint(f\"{'Text':<50} {'Expected':<12} {'Predicted':<12} {'Status'}\")\nprint('-'*80)\n\ncorrect = 0\nmodel.eval()\nwith torch.no_grad():\n    for text, expected in test_cases:\n        enc = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n        out = model(enc['input_ids'].to(device), enc['attention_mask'].to(device))\n        pred_idx = out['toxicity'].argmax(1).item()\n        pred_label = 'Toxic' if pred_idx == 1 else 'Non-Toxic'\n        status = '\\u2705' if pred_label == expected else '\\u274c'\n        if pred_label == expected:\n            correct += 1\n        print(f\"{text[:48]:<50} {expected:<12} {pred_label:<12} {status}\")\n\nprint('-'*80)\nprint(f'Stress Test Accuracy: {correct}/{len(test_cases)} ({correct/len(test_cases)*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: V11.3 Summary & Comparison\nprint('='*60)\nprint('\\u2697\\ufe0f V11.3 RESULTS SUMMARY')\nprint('='*60)\n\n# Known previous results\nV10_TOX_F1 = 0.7572\nBASELINE_TOX_F1 = 0.7378\nV11_TOX_F1 = 0.7418\nV112_TOX_F1 = 0.7368\n\nV11_EMO_F1 = 0.6202\nV11_SENT_F1 = 0.9403\nV112_EMO_F1 = 0.6095\nV112_SENT_F1 = 0.9357\n\nV113_TOX_F1 = best_f1\n\nprint(f'\\n{\"Metric\":<30} {\"V10.2\":<10} {\"Base\":<10} {\"V11\":<10} {\"V11.2\":<10} {\"V11.3\":<10}')\nprint('-'*80)\nprint(f'{\"Toxicity Val F1\":<30} {V10_TOX_F1:<10.4f} {BASELINE_TOX_F1:<10.4f} {V11_TOX_F1:<10.4f} {V112_TOX_F1:<10.4f} {V113_TOX_F1:<10.4f}')\nprint(f'{\"Emotion Val F1\":<30} {\"N/A\":<10} {\"N/A\":<10} {V11_EMO_F1:<10.4f} {V112_EMO_F1:<10.4f} {best_emo_f1:<10.4f}')\nprint(f'{\"Sentiment Val F1\":<30} {\"N/A\":<10} {\"N/A\":<10} {V11_SENT_F1:<10.4f} {V112_SENT_F1:<10.4f} {best_sent_f1:<10.4f}')\nprint(f'{\"Gradient Balance\":<30} {\"Kendall\":<10} {\"Kendall\":<10} {\"Kendall\":<10} {\"Kendall\":<10} {\"GradNorm\":<10}')\nprint(f'{\"Sampling\":<30} {\"Uniform\":<10} {\"Uniform\":<10} {\"Uniform\":<10} {\"Weighted\":<10} {\"Uniform\":<10}')\nprint('-'*80)\n\ndelta_v10 = V113_TOX_F1 - V10_TOX_F1\ndelta_v11 = V113_TOX_F1 - V11_TOX_F1\ndelta_base = V113_TOX_F1 - BASELINE_TOX_F1\nprint(f'\\n\\u0394 vs V10.2:    {delta_v10:+.4f} F1')\nprint(f'\\u0394 vs V11:      {delta_v11:+.4f} F1')\nprint(f'\\u0394 vs Baseline: {delta_base:+.4f} F1')\n\nprint('\\n--- V11.3 Changes ---')\nprint('  V11 fixes retained:')\nprint('    1. \\u2705 Proper emotion/sentiment validation')\nprint('    2. \\u2705 Emotion eval data leak fixed')\nprint('    3. \\u2705 Task mask in multi-task loss')\nprint('    4. \\u2705 Optimizer state reset on unfreeze')\nprint('  V11.3 new:')\nprint('    5. \\u2705 GradNorm gradient balancing (replaces Kendall)')\nprint('  V11.2 reverted:')\nprint('    \\u274c Task-Weighted Sampling (back to uniform shuffle)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Save Artifacts\nprint('='*60)\nprint('\\U0001f4be SAVING V11.3 ARTIFACTS')\nprint('='*60)\n\nimport json as json_save\nhistory_serializable = {\n    'train_loss': history['train_loss'],\n    'val_f1': history['val_f1'],\n    'val_emo_f1': history['val_emo_f1'],\n    'val_sent_f1': history['val_sent_f1'],\n    'task_weights': [w.tolist() for w in history['task_weights']],\n    'best_f1': best_f1,\n    'best_emo_f1': best_emo_f1,\n    'best_sent_f1': best_sent_f1,\n    'config': CONFIG,\n    'model_type': 'aura_v11.3_gradnorm',\n    'v11_fixes': [\n        'proper_emotion_sentiment_validation',\n        'task_mask_in_gradnorm_loss',\n        'optimizer_state_reset_on_unfreeze',\n        'emotion_eval_data_leak_fixed'\n    ],\n    'v113_improvements': [\n        'gradnorm_gradient_balancing'\n    ],\n    'v112_reverted': [\n        'task_weighted_sampling_removed'\n    ]\n}\nwith open('aura_v11.3_history.json', 'w') as f:\n    json_save.dump(history_serializable, f, indent=2)\n\nprint('\\u2705 Model saved: aura_v11.3_best.pt')\nprint('\\u2705 History saved: aura_v11.3_history.json')\nprint(f'\\n\\U0001f3c6 Best Toxicity F1:  {best_f1:.4f}')\nprint(f'\\U0001f3c6 Best Emotion F1:   {best_emo_f1:.4f}')\nprint(f'\\U0001f3c6 Best Sentiment F1: {best_sent_f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}