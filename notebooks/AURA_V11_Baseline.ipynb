{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# AURA V11 ‚Äî RoBERTa Baseline (Ablation Study)\n\n**Purpose**: Controlled ablation to measure the contribution of Task-Specific Multi-Head Attention.\n\nThis notebook is **identical** to `AURA_V10.2_Kaggle.ipynb` in every respect except the model:\n\n| | AURA V10.2 | This Baseline |\n|---|---|---|\n| Encoder | RoBERTa-base | RoBERTa-base |\n| Task Attention | 4√ó MHSA (8 heads each) | ‚ùå None |\n| Pooling | Mean pool per-task attention output | Mean pool shared encoder output |\n| Heads | 4 linear classifiers | 4 linear classifiers (identical) |\n| Loss | Focal + Kendall (Softplus variant) | Focal + Kendall (Softplus variant) |\n| Data | aura-v11-data | aura-v11-data |\n| Config | Identical | Identical |\n| Seed | 42 | 42 |\n\nThe **only independent variable** is the presence of Task-Specific MHSA.\n\n---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_seed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Seed ‚Äî IDENTICAL TO V10.2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom transformers import RobertaModel, RobertaTokenizer, get_linear_schedule_with_warmup\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import (\n    f1_score, classification_report, confusion_matrix, \n    multilabel_confusion_matrix, precision_recall_fscore_support\n)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility ‚Äî SAME SEED AS V10.2\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'üîß Device: {device}')\nif device.type == 'cuda':\n    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration ‚Äî IDENTICAL TO V10.2\nCONFIG = {\n    # Model\n    'encoder': 'roberta-base',\n    'hidden_dim': 768,\n    'n_heads': 8,\n    'num_emotion_classes': 7,\n    'max_length': 128,\n    'dropout': 0.3,\n    \n    # Training\n    'batch_size': 16,\n    'gradient_accumulation': 4,  # Effective batch = 64\n    'epochs': 10,  # SAME AS V10.2 FINAL RUN\n    'lr_encoder': 1e-5,\n    'lr_heads': 5e-5,\n    'weight_decay': 0.01,\n    'max_grad_norm': 1.0,\n    'warmup_ratio': 0.1,\n    \n    # Regularization (Module 3)\n    'focal_gamma': 2.0,\n    'label_smoothing': 0.1,\n    'patience': 5,\n    'freezing_epochs': 1,\n}\n\nDATA_DIR = '/kaggle/input/aura-v11-data'\nEMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n\nprint('üìã Baseline Configuration (identical to V10.2):')\nfor k, v in CONFIG.items():\n    print(f'   {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Visualization Functions ‚Äî IDENTICAL TO V10.2\ndef plot_class_distribution(df, label_col, title, ax=None):\n    \"\"\"Plot class distribution (NB11 pattern).\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 4))\n    counts = df[label_col].value_counts().sort_index()\n    bars = ax.bar(counts.index.astype(str), counts.values, color=['#66c2a5', '#fc8d62'])\n    ax.set_title(title)\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Count')\n    for bar, count in zip(bars, counts.values):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n                str(count), ha='center', fontsize=10)\n    return ax\n\ndef plot_confusion_matrix_heatmap(y_true, y_pred, labels, title='Confusion Matrix', ax=None):\n    \"\"\"Plot confusion matrix heatmap (NB10 pattern).\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(6, 5))\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=labels, yticklabels=labels, ax=ax,\n                cbar_kws={'label': 'Count'})\n    ax.set_title(title)\n    ax.set_ylabel('Actual')\n    ax.set_xlabel('Predicted')\n    return ax\n\ndef plot_multilabel_confusion_matrices(y_true, y_pred, labels, normalize=True):\n    \"\"\"Plot confusion matrix for each label in multilabel task (NB06 pattern).\"\"\"\n    cms = multilabel_confusion_matrix(y_true, y_pred)\n    n_labels = len(labels)\n    cols = min(4, n_labels)\n    rows = (n_labels + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n    axes = axes.flatten() if n_labels > 1 else [axes]\n    \n    for i, (cm, label) in enumerate(zip(cms, labels)):\n        ax = axes[i]\n        if normalize:\n            cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n            fmt = '.2f'\n        else:\n            fmt = 'd'\n        sns.heatmap(cm, annot=True, fmt=fmt, cmap='YlGnBu', ax=ax,\n                    xticklabels=['Neg', 'Pos'], yticklabels=['Neg', 'Pos'],\n                    vmin=0, vmax=1 if normalize else None, cbar=False)\n        ax.set_title(label, fontsize=10)\n        ax.set_ylabel('Actual')\n        ax.set_xlabel('Predicted')\n    \n    # Hide unused axes\n    for i in range(n_labels, len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Multilabel Confusion Matrices (Normalized)', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_history(history):\n    \"\"\"Plot training history (NB10 pattern).\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    \n    # Loss\n    axes[0].plot(range(1, len(history['train_loss'])+1), history['train_loss'], 'b-o', label='Train')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # F1 Score\n    axes[1].plot(range(1, len(history['val_f1'])+1), history['val_f1'], 'g-o', label='Val F1')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Macro F1')\n    axes[1].set_title('Validation F1 Score')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    # Task Weights (Kendall)\n    weights = np.array(history['task_weights'])\n    for i, name in enumerate(['Toxicity', 'Emotion', 'Sentiment', 'Reporting']):\n        axes[2].plot(range(1, len(weights)+1), weights[:, i], '-o', label=name)\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Weight (1/œÉ¬≤)')\n    axes[2].set_title('Kendall Task Weights')\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint('üìä Visualization functions loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_doc",
   "metadata": {},
   "source": [
    "## ‚öóÔ∏è Ablation: Baseline Model (No Task-Specific Attention)\n\nThe baseline model removes **all** Task-Specific Multi-Head Attention blocks.\n\nArchitecture comparison:\n\n```\nAURA V10.2:   RoBERTa ‚Üí [tox_mha, emo_mha, sent_mha, rep_mha] ‚Üí mean_pool ‚Üí dropout ‚Üí heads\nBaseline:     RoBERTa ‚Üí mean_pool ‚Üí dropout ‚Üí heads\n```\n\nAll 4 task heads receive the **same pooled representation** from the shared encoder. This isolates whether the MHSA blocks provide any benefit over a simple shared-encoder MTL setup.\n\n**Everything else is identical**: same loss functions, optimizer, scheduler, data, seed, freezing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Baseline Model (No Task-Specific MHA)\n#\n# ABLATION VARIABLE: This replaces AURA_V10's 4 TaskSpecificMHA blocks\n# with a single shared mean-pool representation fed to all 4 heads.\n#\n# Everything else (dropout, bias init, head dimensions) is IDENTICAL to V10.2.\n\nclass AURA_Baseline(nn.Module):\n    \"\"\"RoBERTa + 4 Linear Heads (No Task-Specific Attention).\n    \n    Ablation baseline for AURA V10.2. Removes all TaskSpecificMHA modules.\n    All task heads share the same pooled encoder representation.\n    \"\"\"\n    \n    def __init__(self, config):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained(config['encoder'])\n        hidden = config['hidden_dim']\n        \n        # NO TaskSpecificMHA blocks ‚Äî this is the ablation variable\n        \n        self.dropout = nn.Dropout(config['dropout'])\n        \n        # Classification Heads ‚Äî IDENTICAL to V10.2\n        self.toxicity_head = nn.Linear(hidden, 2)\n        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n        self.sentiment_head = nn.Linear(hidden, 2)\n        self.reporting_head = nn.Linear(hidden, 1)\n        \n        # Bias Initialization ‚Äî IDENTICAL to V10.2\n        with torch.no_grad():\n            self.toxicity_head.bias[0] = 2.5   # Non-Toxic\n            self.toxicity_head.bias[1] = -2.5  # Toxic\n\n    def _mean_pool(self, seq, mask):\n        \"\"\"Masked mean pooling ‚Äî IDENTICAL to V10.2.\"\"\"\n        mask_exp = mask.unsqueeze(-1).expand(seq.size()).float()\n        return (seq * mask_exp).sum(dim=1) / mask_exp.sum(dim=1).clamp(min=1e-9)\n\n    def forward(self, input_ids, attention_mask):\n        # Shared encoder ‚Äî IDENTICAL to V10.2\n        shared = self.roberta(input_ids, attention_mask).last_hidden_state\n        \n        # ABLATION DIFFERENCE: single shared pooling, no task-specific attention\n        pooled = self.dropout(self._mean_pool(shared, attention_mask))\n        \n        # All heads receive the SAME representation\n        return {\n            'toxicity': self.toxicity_head(pooled),\n            'emotion': self.emotion_head(pooled),\n            'sentiment': self.sentiment_head(pooled),\n            'reporting': self.reporting_head(pooled).squeeze(-1)\n        }\n\nprint('üß† AURA_Baseline model defined (No MHSA).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Loss Functions ‚Äî IDENTICAL TO V10.2\ndef focal_loss(logits, targets, gamma=2.0, weight=None, smoothing=0.0):\n    \"\"\"Focal Loss (NB11): focuses on hard examples.\n    \n    FL(p_t) = -(1 - p_t)^gamma * log(p_t)\n    \"\"\"\n    ce = F.cross_entropy(logits, targets, weight=weight, reduction='none', label_smoothing=smoothing)\n    pt = torch.exp(-ce)\n    return ((1 - pt) ** gamma * ce).mean()\n\nclass UncertaintyLoss(nn.Module):\n    \"\"\"Kendall et al. (2018) Homoscedastic Uncertainty ‚Äî Softplus variant.\n    \n    IDENTICAL to V10.2. Uses Softplus instead of exp() for numerical stability.\n    \n    L_total = sum_i [precision_i * L_i + softplus(s_i)/2]\n    where precision_i = 1 / softplus(s_i)\n    \"\"\"\n    def __init__(self, n_tasks=4):\n        super().__init__()\n        self.log_vars = nn.Parameter(torch.zeros(n_tasks))\n    \n    def forward(self, losses):\n        total = 0\n        for i, loss in enumerate(losses):\n            # SoftPlus variant for better numerical stability\n            precision = 1.0 / (F.softplus(self.log_vars[i]) + 1e-8)  # FIXED: Correct inverse formula\n            total += precision * loss + F.softplus(self.log_vars[i]) * 0.5\n        return total\n    \n    def get_weights(self):\n        return (1.0 / (F.softplus(self.log_vars) + 1e-8)).detach().cpu().numpy()  # FIXED\n\nprint('‚öñÔ∏è Loss functions defined (Focal + Kendall Softplus) ‚Äî identical to V10.2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset Classes ‚Äî IDENTICAL TO V10.2\nclass BaseDataset(Dataset):\n    def __init__(self, path, tokenizer, max_len):\n        self.df = pd.read_csv(path)\n        self.tok = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self): \n        return len(self.df)\n    \n    def encode(self, text):\n        return self.tok(\n            str(text), max_length=self.max_len, \n            padding='max_length', truncation=True, return_tensors='pt'\n        )\n\nclass ToxicityDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'tox': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 0\n        }\n\nclass EmotionDataset(BaseDataset):\n    def __init__(self, path, tokenizer, max_len, cols):\n        super().__init__(path, tokenizer, max_len)\n        self.cols = cols\n        # FIX: Filter samples with no labels + reset_index\n        if 'label_sum' in self.df.columns:\n            self.df = self.df[self.df['label_sum'] > 0].reset_index(drop=True)\n            \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'emo': torch.tensor([float(row[c]) for c in self.cols], dtype=torch.float), \n            'task': 1\n        }\n\nclass SentimentDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'sent': torch.tensor(int(row['label']), dtype=torch.long), \n            'task': 2\n        }\n\nclass ReportingDataset(BaseDataset):\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        enc = self.encode(row['text'])\n        return {\n            'ids': enc['input_ids'].flatten(), \n            'mask': enc['attention_mask'].flatten(),\n            'rep': torch.tensor(int(row['is_reporting']), dtype=torch.long), \n            'task': 3\n        }\n\ndef collate_fn(batch):\n    \"\"\"Custom collate: handle mixed-task batches gracefully.\"\"\"\n    ids = torch.stack([x['ids'] for x in batch])\n    mask = torch.stack([x['mask'] for x in batch])\n    tasks = torch.tensor([x['task'] for x in batch])\n    \n    tox_items = [x['tox'] for x in batch if x['task'] == 0]\n    emo_items = [x['emo'] for x in batch if x['task'] == 1]\n    sent_items = [x['sent'] for x in batch if x['task'] == 2]\n    rep_items = [x['rep'] for x in batch if x['task'] == 3]\n    \n    return {\n        'ids': ids, 'mask': mask, 'tasks': tasks,\n        'tox': torch.stack(tox_items) if tox_items else None,\n        'emo': torch.stack(emo_items) if emo_items else None,\n        'sent': torch.stack(sent_items) if sent_items else None,\n        'rep': torch.stack(rep_items) if rep_items else None\n    }\n\nprint('üì¶ Dataset classes defined ‚Äî identical to V10.2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load Data ‚Äî IDENTICAL TO V10.2\ntokenizer = RobertaTokenizer.from_pretrained(CONFIG['encoder'])\n\n# Load all datasets\ntox_train = ToxicityDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'])\nemo_train = EmotionDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\nsent_train = SentimentDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'])\nrep_train = ReportingDataset(f'{DATA_DIR}/reporting_examples_augmented.csv', tokenizer, CONFIG['max_length'])\ntox_val = ToxicityDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, CONFIG['max_length'])\n\ntrain_ds = ConcatDataset([tox_train, emo_train, sent_train, rep_train])\ntrain_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, \n                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], collate_fn=collate_fn)\n\nprint('='*60)\nprint('üìä DATASET SUMMARY')\nprint('='*60)\nprint(f'Training Samples: {len(train_ds):,}')\nprint(f'  ‚îú‚îÄ Toxicity:  {len(tox_train):,}')\nprint(f'  ‚îú‚îÄ Emotion:   {len(emo_train):,}')\nprint(f'  ‚îú‚îÄ Sentiment: {len(sent_train):,}')\nprint(f'  ‚îî‚îÄ Reporting: {len(rep_train):,}')\nprint(f'Validation Samples: {len(tox_val):,} (Toxicity only)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model & Optimizer Setup\n#\n# ABLATION NOTE: Uses AURA_Baseline instead of AURA_V10.\n# Optimizer structure is adapted to match: encoder params get lr_encoder,\n# all head params + loss params get lr_heads ‚Äî same differential LR strategy.\n\nmodel = AURA_Baseline(CONFIG).to(device)\nloss_fn = UncertaintyLoss().to(device)\ntox_weights = torch.tensor([0.5, 2.0], device=device)  # Class weights ‚Äî IDENTICAL\n\n# Optimizer with differential LR ‚Äî SAME STRATEGY as V10.2\n# V10.2 groups: [roberta params @ lr_encoder] + [MHA + heads + loss @ lr_heads]\n# Baseline groups: [roberta params @ lr_encoder] + [heads + loss @ lr_heads]\noptimizer = torch.optim.AdamW([\n    {'params': model.roberta.parameters(), 'lr': CONFIG['lr_encoder']},\n    {'params': list(model.toxicity_head.parameters()) + list(model.emotion_head.parameters()) +\n               list(model.sentiment_head.parameters()) + list(model.reporting_head.parameters()) +\n               list(loss_fn.parameters()), 'lr': CONFIG['lr_heads']}\n], weight_decay=CONFIG['weight_decay'])\n\n# Scheduler with warmup ‚Äî IDENTICAL to V10.2\ntotal_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation']\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=int(total_steps * CONFIG['warmup_ratio']), \n    num_training_steps=total_steps\n)\n\n# Model summary\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint('='*60)\nprint('üèóÔ∏è BASELINE MODEL SETUP')\nprint('='*60)\nprint(f'Total parameters:     {total_params:,}')\nprint(f'Trainable parameters: {trainable_params:,}')\nprint(f'Total optimization steps: {total_steps}')\nprint(f'Warmup steps: {int(total_steps * CONFIG[\"warmup_ratio\"])}')\nprint(f'Effective batch size: {CONFIG[\"batch_size\"] * CONFIG[\"gradient_accumulation\"]}')\nprint()\nprint('‚öóÔ∏è ABLATION NOTE:')\nprint(f'   AURA V10.2 params: ~128.7M (with 4√ó MHSA blocks)')\nprint(f'   Baseline params:   {total_params:,} (no MHSA)')\nprint(f'   Œî (MHSA overhead): ~{128_700_000 - total_params:,} parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training Functions ‚Äî IDENTICAL TO V10.2\ndef train_epoch(epoch):\n    model.train()\n    \n    # Progressive Freezing ‚Äî IDENTICAL to V10.2\n    if epoch <= CONFIG['freezing_epochs']:\n        print(f'‚ùÑÔ∏è Epoch {epoch}: RoBERTa FROZEN')\n        for p in model.roberta.parameters(): \n             p.requires_grad = False\n    else:\n        print(f'üî• Epoch {epoch}: RoBERTa UNFROZEN')\n        for p in model.roberta.parameters(): \n             p.requires_grad = True\n    \n    total_loss = 0\n    optimizer.zero_grad()\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', mininterval=10.0)\n    \n    for step, batch in enumerate(pbar):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        tasks = batch['tasks']\n        \n        # Forward pass\n        out = model(ids, mask)\n        \n        # Compute per-task losses ‚Äî IDENTICAL to V10.2\n        losses = []\n        \n        # Toxicity\n        if batch['tox'] is not None and (tasks == 0).sum() > 0:\n            losses.append(focal_loss(\n                out['toxicity'][tasks == 0], batch['tox'].to(device), \n                weight=tox_weights, smoothing=CONFIG['label_smoothing']\n            ))\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            \n        # Emotion (Multilabel BCE)\n        if batch['emo'] is not None and (tasks == 1).sum() > 0:\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['emotion'][tasks == 1], batch['emo'].to(device)\n            ))\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            \n        # Sentiment\n        if batch['sent'] is not None and (tasks == 2).sum() > 0:\n            losses.append(focal_loss(\n                out['sentiment'][tasks == 2], batch['sent'].to(device), \n                smoothing=CONFIG['label_smoothing']\n            ))\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            \n        # Reporting\n        if batch['rep'] is not None and (tasks == 3).sum() > 0:\n            # Use BCE with logits on float target\n            losses.append(F.binary_cross_entropy_with_logits(\n                out['reporting'][tasks == 3], batch['rep'].float().to(device)\n            ))\n        else: \n            losses.append(torch.tensor(0., device=device, requires_grad=False))\n            \n        # Check for empty batch ‚Äî IDENTICAL to V10.2\n        if all((tasks == i).sum() == 0 for i in range(4)):\n            print(f\"‚ö†Ô∏è Warning: Empty batch at step {step}, skipping\")\n            optimizer.zero_grad()\n            continue\n\n        # Kendall weighted loss ‚Äî IDENTICAL to V10.2\n        loss = loss_fn(losses) / CONFIG['gradient_accumulation']\n        \n        # NaN/Inf safety check ‚Äî IDENTICAL to V10.2\n        if torch.isnan(loss) or torch.isinf(loss):\n            print(f\"‚ö†Ô∏è Warning: Invalid loss {loss.item():.4f} at step {step}, skipping batch\")\n            optimizer.zero_grad()\n            continue\n\n        # Backward pass\n        loss.backward()\n        \n        # Gradient Accumulation ‚Äî IDENTICAL to V10.2\n        if (step + 1) % CONFIG['gradient_accumulation'] == 0:\n            nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n        total_loss += loss.item() * CONFIG['gradient_accumulation']\n        if step % 50 == 0: pbar.set_postfix({'loss': f'{loss.item() * CONFIG[\"gradient_accumulation\"]:.3f}'})\n        \n    return total_loss / len(train_loader)\n\n@torch.no_grad()\ndef evaluate():\n    model.eval()\n    preds, trues = [], []\n    for batch in val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n    return f1_score(trues, preds, average='macro', zero_division=0)\n\nprint('üéØ Training functions defined ‚Äî identical to V10.2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Main Training Loop ‚Äî IDENTICAL TO V10.2\nprint('='*60)\nprint('üöÄ BASELINE ‚Äî TRAINING START')\nprint('='*60)\n\nbest_f1 = 0\npatience_counter = 0\nhistory = {'train_loss': [], 'val_f1': [], 'task_weights': []}\n\nfor epoch in range(1, CONFIG['epochs'] + 1):\n    train_loss = train_epoch(epoch)\n    val_f1 = evaluate()\n    weights = loss_fn.get_weights()\n    \n    history['train_loss'].append(train_loss)\n    history['val_f1'].append(val_f1)\n    history['task_weights'].append(weights.copy())\n    \n    print(f'\\nEpoch {epoch} Summary:')\n    print(f'  Train Loss: {train_loss:.4f}')\n    print(f'  Val F1:     {val_f1:.4f}')\n    print(f'  Task Weights [Tox/Emo/Sent/Rep]: {weights.round(3)}')\n    \n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        patience_counter = 0\n        torch.save(model.state_dict(), 'baseline_best.pt')\n        print('  >>> BEST MODEL SAVED <<<')\n    else:\n        patience_counter += 1\n        print(f'  (No improvement. Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n        if patience_counter >= CONFIG['patience']:\n            print(f'\\n‚ö†Ô∏è Early stopping at epoch {epoch}')\n            break\n\nprint('\\n' + '='*60)\nprint(f'‚úÖ Baseline Training Complete. Best Val F1: {best_f1:.4f}')\nprint('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Training History Visualization\nplot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Evaluation ‚Äî Toxicity (IDENTICAL evaluation to V10.2)\nprint('='*60)\nprint('üî¨ BASELINE FINAL EVALUATION: TOXICITY')\nprint('='*60)\n\nmodel.load_state_dict(torch.load('baseline_best.pt'))\nmodel.eval()\n\npreds, trues = [], []\nwith torch.no_grad():\n    for batch in val_loader:\n        out = model(batch['ids'].to(device), batch['mask'].to(device))\n        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n        trues.extend(batch['tox'].numpy())\n\n# Classification Report\nprint('\\n--- Classification Report ---')\nprint(classification_report(trues, preds, target_names=['Non-Toxic', 'Toxic']))\n\n# Confusion Matrix\nfig, ax = plt.subplots(figsize=(6, 5))\nplot_confusion_matrix_heatmap(trues, preds, ['Non-Toxic', 'Toxic'], 'Baseline: Toxicity Confusion Matrix', ax)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stress_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Qualitative Stress Test ‚Äî IDENTICAL test cases to V10.2\nprint('='*60)\nprint('üß™ BASELINE QUALITATIVE STRESS TEST')\nprint('='*60)\n\ntest_cases = [\n    # Should be NON-TOXIC (The Traffic Paradox)\n    (\"I hate rainy Mondays\", \"Non-Toxic\"),\n    (\"This soup is disgusting\", \"Non-Toxic\"),\n    (\"I am so angry at the traffic\", \"Non-Toxic\"),\n    (\"This movie was terrible\", \"Non-Toxic\"),\n    \n    # Should be TOXIC\n    (\"You are an idiot\", \"Toxic\"),\n    (\"I hate you so much\", \"Toxic\"),\n    (\"Go kill yourself\", \"Toxic\"),\n    (\"You are worthless garbage\", \"Toxic\"),\n    \n    # Should be NON-TOXIC (Reporting)\n    (\"He said you are an idiot\", \"Non-Toxic\"),\n    (\"The article discusses hate speech\", \"Non-Toxic\"),\n    (\"Someone wrote 'go die' in the comments\", \"Non-Toxic\"),\n]\n\nprint(f\"{'Text':<50} {'Expected':<12} {'Predicted':<12} {'Status'}\")\nprint('-'*80)\n\ncorrect = 0\nmodel.eval()\nwith torch.no_grad():\n    for text, expected in test_cases:\n        enc = tokenizer(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n        out = model(enc['input_ids'].to(device), enc['attention_mask'].to(device))\n        pred_idx = out['toxicity'].argmax(1).item()\n        pred_label = 'Toxic' if pred_idx == 1 else 'Non-Toxic'\n        status = '‚úÖ' if pred_label == expected else '‚ùå'\n        if pred_label == expected:\n            correct += 1\n        print(f\"{text[:48]:<50} {expected:<12} {pred_label:<12} {status}\")\n\nprint('-'*80)\nprint(f'Stress Test Accuracy: {correct}/{len(test_cases)} ({correct/len(test_cases)*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Ablation Comparison ‚Äî AURA V10.2 vs Baseline\nprint('='*60)\nprint('‚öóÔ∏è ABLATION COMPARISON: AURA V10.2 vs BASELINE')\nprint('='*60)\n\n# V10.2 known results (from AURA_V10.2_Kaggle.ipynb run)\nAURA_V10_F1 = 0.7572\nBASELINE_F1 = best_f1\nDELTA = BASELINE_F1 - AURA_V10_F1\n\nprint(f'\\n{\"Metric\":<30} {\"AURA V10.2 (MHSA)\":<20} {\"Baseline (No MHSA)\":<20} {\"Œî\":<10}')\nprint('-'*80)\nprint(f'{\"Toxicity Val F1 (macro)\":<30} {AURA_V10_F1:<20.4f} {BASELINE_F1:<20.4f} {DELTA:+.4f}')\nprint(f'{\"Architecture\":<30} {\"RoBERTa + 4√óMHSA\":<20} {\"RoBERTa only\":<20} {\"\":<10}')\nprint(f'{\"Extra Params (MHSA)\":<30} {\"~7M\":<20} {\"0\":<20} {\"\":<10}')\nprint('-'*80)\n\nif DELTA > 0:\n    print(f'\\nüìä Result: Baseline is BETTER by {abs(DELTA):.4f} F1 points.')\n    print('   ‚Üí Task-Specific MHSA may be HURTING performance.')\nelif DELTA < -0.01:\n    print(f'\\nüìä Result: AURA V10.2 is BETTER by {abs(DELTA):.4f} F1 points.')\n    print('   ‚Üí Task-Specific MHSA provides a measurable improvement.')\nelse:\n    print(f'\\nüìä Result: Difference is NEGLIGIBLE ({abs(DELTA):.4f} F1 points).')\n    print('   ‚Üí Task-Specific MHSA adds ~7M parameters for marginal/no gain.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save Artifacts\nprint('='*60)\nprint('üíæ SAVING BASELINE ARTIFACTS')\nprint('='*60)\n\nimport json\nhistory_serializable = {\n    'train_loss': history['train_loss'],\n    'val_f1': history['val_f1'],\n    'task_weights': [w.tolist() for w in history['task_weights']],\n    'best_f1': best_f1,\n    'config': CONFIG,\n    'model_type': 'baseline_no_mhsa'\n}\nwith open('baseline_history.json', 'w') as f:\n    json.dump(history_serializable, f, indent=2)\n\nprint('‚úÖ Model saved: baseline_best.pt')\nprint('‚úÖ History saved: baseline_history.json')\nprint(f'\\nüèÜ Baseline Best F1: {best_f1:.4f}')\nprint(f'üèÜ AURA V10.2 F1:    {AURA_V10_F1:.4f}')\nprint(f'üèÜ Œî (Baseline - AURA): {DELTA:+.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}