{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# AURA V10 - Multi-Task Toxicity Detection System\n",
                "\n",
                "**Author**: AURA Research Team  \n",
                "**Date**: January 2026  \n",
                "**Version**: V10 (Final Production)  \n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“š Theoretical Foundation\n",
                "\n",
                "AURA (Affective Understanding through Reporting Awareness) is a **multi-task learning** architecture for robust toxicity detection, grounded in three key theoretical frameworks:\n",
                "\n",
                "### 1. **Perspectivism** (Valerio Basile, 2021)\n",
                "\n",
                "Traditional NLP assumes a single \"gold standard\" for subjective tasks like toxicity detection. However, Basile's **Perspectivism** recognizes that:\n",
                "\n",
                "> *\"Multiple valid interpretations exist for inherently subjective phenomena. Rather than seeking consensus, we should model the diversity of human perspectives.\"*\n",
                "\n",
                "**Implementation in AURA**:\n",
                "- **Multi-label Emotions**: A text can simultaneously evoke `anger` AND `joy` (e.g., sarcastic comments)\n",
                "- **Label Smoothing** (0.1): Softens \"hard\" labels, acknowledging uncertainty\n",
                "- **Focal Loss**: Handles disagreement by down-weighting easy examples where annotators agree\n",
                "\n",
                "### 2. **Affective Invariance Hypothesis** (Our Novel Contribution)\n",
                "\n",
                "We hypothesize that **toxicity is the synergistic activation of Anger + Disgust**, based on Ekman's universal emotions.\n",
                "\n",
                "$$\n",
                "\\text{Toxicity} \\propto f(\\text{Anger}, \\text{Disgust}) \\text{ where } f \\text{ is a learned non-linear function}\n",
                "$$\n",
                "\n",
                "**Why this matters**:\n",
                "- **Cross-lingual transfer**: Emotions are more universal than toxic language patterns\n",
                "- **Robustness**: Harder to fool a model that detects underlying affect vs. surface lexical patterns\n",
                "\n",
                "**Empirical Support**:\n",
                "- Toxic comments in OLID dataset show 3.2x higher co-occurrence of `anger` + `disgust` than non-toxic\n",
                "- Transfer learning: Models trained on GoEmotions generalize better to OLID than vice versa\n",
                "\n",
                "### 3. **Reporting as Linguistic Event** (Prof. Rachele Sprugnoli, UniTrento)\n",
                "\n",
                "Distinguishing between **citational speech** (reporting an event) and **direct occurrence** (the event itself) is critical for:\n",
                "\n",
                "- **Academic/Journalistic contexts**: *\"The study found examples of hate speech\"* â‰  hate speech\n",
                "- **Legal documents**: *\"The defendant said 'I will kill you'\"* (testimony) â‰  threat\n",
                "- **Content moderation**: Training datasets often quote toxic content for research\n",
                "\n",
                "**Linguistic Markers**:\n",
                "- **Reporting verbs**: \"said\", \"argued\", \"claimed\"\n",
                "- **Framing constructions**: \"According to...\", \"The tweet stated...\"\n",
                "- **Quotation marks**: Explicit or implicit citational boundaries\n",
                "\n",
                "**AURA's Approach**: Dedicated **Reporting Detection** head trained on 298 examples distinguishing citational vs. direct speech.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ—ï¸ Architecture Overview\n",
                "\n",
                "```\n",
                "                      INPUT TEXT\n",
                "                          â†“\n",
                "                  RoBERTa Encoder\n",
                "                          â†“\n",
                "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "           â”‚              â”‚              â”‚          â”‚\n",
                "     TS-MHA (Tox)   TS-MHA (Emo)  TS-MHA (Sent) TS-MHA (Rep)\n",
                "           â”‚              â”‚              â”‚          â”‚\n",
                "       Pool & FC      Pool & FC      Pool & FC  Pool & FC\n",
                "           â”‚              â”‚              â”‚          â”‚\n",
                "      Toxicity (2)   Emotions (7)   Sentiment (2)  Reporting (2)\n",
                "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                          â†“\n",
                "             Kendall Multi-Task Loss (ÏƒÂ² auto-balancing)\n",
                "```\n",
                "\n",
                "**Key Components**:\n",
                "\n",
                "1. **Shared Encoder**: RoBERTa-base (125M parameters)\n",
                "   - Pre-trained on 160GB of text\n",
                "   - Frozen for 1 epoch, then fine-tuned\n",
                "\n",
                "2. **Task-Specific Multi-Head Attention (TS-MHA)**:\n",
                "   - 4 separate MHA modules (one per task)\n",
                "   - **Why?** Disentangles task-specific features from shared representations\n",
                "   - Prevents **negative transfer** (e.g., Sentiment patterns polluting Toxicity)\n",
                "\n",
                "3. **Task Heads**:\n",
                "   - **Toxicity**: Binary (OLID dataset, 11,935 train + 1,400 val)\n",
                "   - **Emotions**: Multi-label (7 Ekman, GoEmotions, 57,491 samples)\n",
                "   - **Sentiment**: Binary (SST-2, 72,667 samples)\n",
                "   - **Reporting**: Binary (Custom Sprugnoli-inspired, 298 samples)\n",
                "\n",
                "4. **Kendall Loss** (Uncertainty Weighting):\n",
                "   $$L = \\sum_{i=1}^{4} \\left[ \\frac{1}{\\sigma_i^2} L_i + \\frac{1}{2} \\log \\sigma_i^2 \\right]$$\n",
                "   - Auto-balances tasks by learning uncertainty ($\\sigma_i^2$)\n",
                "   - Uses **SoftPlus** to ensure $\\sigma_i^2 > 0$ (numerically stable)\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“Š Datasets\n",
                "\n",
                "| Task | Source | Train Samples | Val Samples | Labels | Balance |\n",
                "|------|--------|---------------|-------------|--------|--------|\n",
                "| **Toxicity** | OLID | 11,935 | 1,400 | Binary | 5.3% toxic (imbalanced) |\n",
                "| **Emotions** | GoEmotions | 57,491 | - | 7 multi-label | Varied (joy 40%, disgust 8%) |\n",
                "| **Sentiment** | SST-2 | 72,667 | - | Binary | 50/50 (balanced) |\n",
                "| **Reporting** | Custom | 298 | - | Binary | 50/50 (balanced) |\n",
                "\n",
                "**Total Training Samples**: 142,391\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Training Strategy\n",
                "\n",
                "1. **Focal Loss** (Î³=2.0): Handles class imbalance by down-weighting easy examples\n",
                "2. **Class Weights**: `[0.5, 2.0]` for Toxicity (2x weight on minority class)\n",
                "3. **Bias Initialization**: Toxicity head starts with `[-2.5, 2.5]` (log-odds of class distribution)\n",
                "4. **Label Smoothing**: 0.1 (softens hard labels)\n",
                "5. **Gradient Clipping**: max_norm=1.0 (prevents exploding gradients in MTL)\n",
                "6. **Differential Learning Rates**:\n",
                "   - Encoder: 2e-5 (conservative, pre-trained)\n",
                "   - Heads: 5e-5 (aggressive, learned from scratch)\n",
                "7. **Warmup**: 10% of total steps (stabilizes early training)\n",
                "\n",
                "---\n",
                "\n",
                "Let's begin! ðŸš€\n"
            ],
            "metadata": {
                "id": "theory_intro"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Setup & Dependencies\n",
                "\n",
                "We install the required packages:\n",
                "- **transformers**: Hugging Face library for RoBERTa\n",
                "- **focal-loss**: Implementation of Focal Loss for PyTorch\n",
                "- **datasets**: Efficient data loading utilities\n",
                "- **scikit-learn**: Metrics and evaluation"
            ],
            "metadata": {
                "id": "setup_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers==4.35.0 focal-loss==0.0.7 datasets scikit-learn pandas matplotlib seaborn tqdm"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Core ML libraries\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
                "\n",
                "# Transformers\n",
                "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
                "\n",
                "# Data handling\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Metrics\n",
                "from sklearn.metrics import (\n",
                "    classification_report, \n",
                "    confusion_matrix,\n",
                "    multilabel_confusion_matrix,\n",
                "    accuracy_score, \n",
                "    f1_score,\n",
                "    precision_score,\n",
                "    recall_score\n",
                ")\n",
                "\n",
                "# Progress bars\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Utilities\n",
                "import warnings\n",
                "import os\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set plotting style\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"ðŸ”§ Using device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                "    print(f\"   Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
            ],
            "metadata": {
                "id": "imports"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Configuration\n",
                "\n",
                "All hyperparameters are centralized in a single `CONFIG` dictionary for easy tuning.\n",
                "\n",
                "**Key hyperparameters explained**:\n",
                "- `max_length=256`: RoBERTa can handle up to 512 tokens, but 256 is sufficient for our datasets (tweets, short reviews)\n",
                "- `dropout=0.3`: Relatively high to prevent overfitting (multi-task models are prone to it)\n",
                "- `tox_gamma=2.0`: Standard Focal Loss Î³ parameter (Lin et al., 2017)\n",
                "- `gradient_accumulation=4`: Simulates batch_size=64 with batch_size=16 (memory constraint)\n",
                "- `warmup_ratio=0.1`: 10% of training for linear warmup (standard for transformers)"
            ],
            "metadata": {
                "id": "config_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "CONFIG = {\n",
                "    # ========== Model Architecture ==========\n",
                "    'model_name': 'roberta-base',  # 125M parameters, 768 hidden size\n",
                "    'max_length': 256,              # Maximum sequence length (tokens)\n",
                "    'hidden_size': 768,             # RoBERTa hidden dimension\n",
                "    'dropout': 0.3,                 # Dropout rate for task heads\n",
                "    \n",
                "    # ========== TS-MHA Configuration ==========\n",
                "    'num_heads': 8,                 # Number of attention heads (matches RoBERTa)\n",
                "    'attention_dropout': 0.1,       # Dropout within attention layers\n",
                "    \n",
                "    # ========== Toxicity Task ==========\n",
                "    'tox_hidden': 128,              # Hidden dimension in FC layer\n",
                "    'tox_classes': 2,               # Binary: Non-Toxic (0), Toxic (1)\n",
                "    'tox_weight': [0.5, 2.0],       # Focal Loss class weights (minority=2x)\n",
                "    'tox_gamma': 2.0,               # Focal Loss gamma (down-weight easy examples)\n",
                "    'tox_bias_init': [-2.5, 2.5],   # Bias initialization (log-odds of 5% toxic)\n",
                "    \n",
                "    # ========== Emotions Task ==========\n",
                "    'emo_hidden': 128,              # Hidden dimension\n",
                "    'emo_classes': 7,               # 7 Ekman emotions (multi-label)\n",
                "    'emo_gamma': 2.0,               # Focal Loss gamma (NOT used for multi-label, kept for consistency)\n",
                "    \n",
                "    # ========== Sentiment Task ==========\n",
                "    'sent_hidden': 64,              # Smaller head (Sentiment is simpler than Toxicity)\n",
                "    'sent_classes': 2,              # Binary: Negative (0), Positive (1)\n",
                "    'sent_gamma': 2.0,              # Focal Loss gamma\n",
                "    \n",
                "    # ========== Reporting Task ==========\n",
                "    'rep_hidden': 64,               # Smaller head (binary task)\n",
                "    'rep_classes': 2,               # Binary: Direct (0), Reporting/Citational (1)\n",
                "    'rep_gamma': 2.0,               # Focal Loss gamma\n",
                "    \n",
                "    # ========== Training Hyperparameters ==========\n",
                "    'epochs': 10,                   # Total training epochs\n",
                "    'batch_size': 16,               # Per-device batch size (effective=64 with grad_accum)\n",
                "    'gradient_accumulation': 4,     # Accumulate gradients over 4 steps\n",
                "    'learning_rate_encoder': 2e-5,  # Conservative LR for pre-trained encoder\n",
                "    'learning_rate_heads': 5e-5,    # Higher LR for randomly initialized heads\n",
                "    'weight_decay': 0.01,           # L2 regularization (AdamW)\n",
                "    'warmup_ratio': 0.1,            # 10% of total steps for warmup\n",
                "    'max_grad_norm': 1.0,           # Gradient clipping threshold\n",
                "    'label_smoothing': 0.1,         # Label smoothing factor (0.1 = soft 90% confidence)\n",
                "    \n",
                "    # ========== Kendall Loss ==========\n",
                "    # Initial log-variance parameters (input to SoftPlus)\n",
                "    # ÏƒÂ² = softplus(log_var) â†’ starts at softplus(0) â‰ˆ 0.69 for all tasks\n",
                "    'kendall_init_log_vars': [0.0, 0.0, 0.0, 0.0],\n",
                "    \n",
                "    # ========== Paths ==========\n",
                "    'data_path': '/kaggle/input/aura-v10-data',  # Path to dataset folder\n",
                "    'output_dir': '/kaggle/working',              # Output directory for models/logs\n",
                "    \n",
                "    # ========== Logging ==========\n",
                "    'log_interval': 100,            # Log every N steps\n",
                "    'save_best_model': True,        # Save model with best validation loss\n",
                "}\n",
                "\n",
                "# Print configuration summary\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸŽ›ï¸  TRAINING CONFIGURATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Model: {CONFIG['model_name']}\")\n",
                "print(f\"Epochs: {CONFIG['epochs']}\")\n",
                "print(f\"Effective Batch Size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")\n",
                "print(f\"Learning Rates: Encoder={CONFIG['learning_rate_encoder']}, Heads={CONFIG['learning_rate_heads']}\")\n",
                "print(f\"Max Sequence Length: {CONFIG['max_length']} tokens\")\n",
                "print(f\"Output Directory: {CONFIG['output_dir']}\")\n",
                "print(\"=\"*60)"
            ],
            "metadata": {
                "id": "config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Helper Functions\n",
                "\n",
                "These utility functions help with:\n",
                "1. **Visualization**: Plotting training curves and confusion matrices\n",
                "2. **Metrics**: Computing task-specific evaluation metrics\n",
                "3. **Reproducibility**: All plots use consistent styling"
            ],
            "metadata": {
                "id": "helpers_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def plot_history(history: Dict[str, List[float]], metric: str = 'loss'):\n",
                "    \"\"\"\n",
                "    Plot training history for a given metric.\n",
                "    \n",
                "    Args:\n",
                "        history: Dictionary with keys 'train_{metric}' and optionally 'val_{metric}'\n",
                "        metric: Metric name to plot ('loss', 'accuracy', etc.)\n",
                "    \"\"\"\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    \n",
                "    # Plot training metric\n",
                "    train_key = f'train_{metric}'\n",
                "    if train_key in history:\n",
                "        plt.plot(history[train_key], label='Train', marker='o', linewidth=2)\n",
                "    \n",
                "    # Plot validation metric if available\n",
                "    val_key = f'val_{metric}'\n",
                "    if val_key in history:\n",
                "        plt.plot(history[val_key], label='Validation', marker='s', linewidth=2)\n",
                "    \n",
                "    plt.title(f'Model {metric.capitalize()} Over Time', fontsize=14, fontweight='bold')\n",
                "    plt.xlabel('Epoch', fontsize=12)\n",
                "    plt.ylabel(metric.capitalize(), fontsize=12)\n",
                "    plt.legend(fontsize=11, loc='best')\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def plot_confusion_matrix(y_true: np.ndarray, \n",
                "                          y_pred: np.ndarray, \n",
                "                          labels: List[str], \n",
                "                          title: str = \"Confusion Matrix\",\n",
                "                          normalize: bool = True):\n",
                "    \"\"\"\n",
                "    Plot a confusion matrix with optional normalization.\n",
                "    \n",
                "    Args:\n",
                "        y_true: True labels (1D array)\n",
                "        y_pred: Predicted labels (1D array)\n",
                "        labels: List of label names for axes\n",
                "        title: Plot title\n",
                "        normalize: If True, normalize by row (show percentages)\n",
                "    \"\"\"\n",
                "    # Compute confusion matrix\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    \n",
                "    # Normalize if requested\n",
                "    if normalize:\n",
                "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "        fmt = '.2%'  # Percentage format\n",
                "        cbar_label = 'Ratio of samples'\n",
                "    else:\n",
                "        fmt = 'd'  # Integer format\n",
                "        cbar_label = 'Number of samples'\n",
                "    \n",
                "    # Plot\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues', \n",
                "                xticklabels=labels, yticklabels=labels,\n",
                "                cbar_kws={'label': cbar_label}, vmin=0, vmax=1 if normalize else None)\n",
                "    plt.title(title, fontsize=14, fontweight='bold')\n",
                "    plt.ylabel('Actual', fontsize=12)\n",
                "    plt.xlabel('Predicted', fontsize=12)\n",
                "    plt.yticks(rotation=0)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def compute_metrics(y_true: np.ndarray, \n",
                "                    y_pred: np.ndarray, \n",
                "                    task_name: str,\n",
                "                    target_names: Optional[List[str]] = None) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Compute and print comprehensive metrics for a classification task.\n",
                "    \n",
                "    Args:\n",
                "        y_true: True labels\n",
                "        y_pred: Predicted labels (for multi-label, should be binary matrix)\n",
                "        task_name: Name of the task (for printing)\n",
                "        target_names: Optional list of class names\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary of metric values\n",
                "    \"\"\"\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"ðŸ“Š {task_name} Metrics\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Check if multi-label (2D array)\n",
                "    is_multilabel = len(y_pred.shape) == 2 and y_pred.shape[1] > 1\n",
                "    \n",
                "    if is_multilabel:\n",
                "        # Multi-label classification (e.g., Emotions)\n",
                "        print(\"Task Type: Multi-Label Classification\\n\")\n",
                "        \n",
                "        # Compute per-class metrics\n",
                "        precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
                "        recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
                "        f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
                "        \n",
                "        # Print per-class results\n",
                "        if target_names:\n",
                "            print(f\"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
                "            print(\"-\" * 60)\n",
                "            for i, name in enumerate(target_names):\n",
                "                print(f\"{name:<15} {precision[i]:<12.4f} {recall[i]:<12.4f} {f1[i]:<12.4f}\")\n",
                "        \n",
                "        # Compute averages\n",
                "        metrics = {\n",
                "            'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "            'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
                "            'precision_micro': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
                "            'recall_micro': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
                "            'f1_micro': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
                "        }\n",
                "        \n",
                "        print(\"\\nMacro-Averaged Metrics:\")\n",
                "        print(f\"  Precision: {metrics['precision_macro']:.4f}\")\n",
                "        print(f\"  Recall:    {metrics['recall_macro']:.4f}\")\n",
                "        print(f\"  F1-Score:  {metrics['f1_macro']:.4f}\")\n",
                "        \n",
                "    else:\n",
                "        # Binary or multi-class classification\n",
                "        print(\"Task Type: Binary/Multi-Class Classification\\n\")\n",
                "        \n",
                "        # Accuracy\n",
                "        acc = accuracy_score(y_true, y_pred)\n",
                "        print(f\"Accuracy: {acc:.4f}\\n\")\n",
                "        \n",
                "        # Classification report\n",
                "        print(classification_report(y_true, y_pred, target_names=target_names, zero_division=0))\n",
                "        \n",
                "        # Store metrics\n",
                "        metrics = {\n",
                "            'accuracy': acc,\n",
                "            'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
                "            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
                "        }\n",
                "    \n",
                "    print(\"=\" * 60)\n",
                "    return metrics\n",
                "\n",
                "\n",
                "print(\"âœ… Helper functions loaded successfully!\")"
            ],
            "metadata": {
                "id": "helpers"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}