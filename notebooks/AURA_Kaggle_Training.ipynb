{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåü AURA - Multi-Task Learning for Cross-Domain Toxicity Detection\n",
                "\n",
                "**AURA** = **A**ffective **U**nderstanding for **R**obust **A**buse Detection\n",
                "\n",
                "---\n",
                "## ‚ö†Ô∏è PRIMA DI ESEGUIRE:\n",
                "1. **Settings** (‚öôÔ∏è) ‚Üí **Accelerator** ‚Üí **GPU T4 x2**\n",
                "2. **Add Input** ‚Üí Carica `aura-data` (deve contenere `goemotions_processed.csv` e `olid_train.csv`)\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
                "from torch.optim.lr_scheduler import OneCycleLR\n",
                "from transformers import BertModel, BertTokenizer\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.metrics import f1_score\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"=\"*50)\n",
                "if torch.cuda.is_available():\n",
                "    device = torch.device('cuda')\n",
                "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    device = torch.device('cpu')\n",
                "    print(\"‚ùå NO GPU! Vai su Settings ‚Üí Accelerator ‚Üí GPU T4\")\n",
                "    raise RuntimeError(\"Attiva la GPU prima di procedere!\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CONFIG = {\n",
                "    'encoder': 'bert-base-uncased',\n",
                "    'max_length': 128,\n",
                "    'num_emotion_classes': 7,\n",
                "    'dropout': 0.1,\n",
                "    'batch_size': 16,\n",
                "    'gradient_accumulation': 2,\n",
                "    'epochs': 8,\n",
                "    'lr_bert': 2e-5,\n",
                "    'lr_heads': 1e-4,\n",
                "    'weight_decay': 0.01,\n",
                "    'patience': 3,\n",
                "    'focal_gamma': 2.0,\n",
                "    'output_dir': '/kaggle/working'\n",
                "}\n",
                "\n",
                "DATA_DIR = None\n",
                "for path in ['/kaggle/input/aura-data', '/kaggle/input/aura-data/kaggle_upload', 'data/processed']:\n",
                "    if os.path.exists(path) and 'olid_train.csv' in os.listdir(path):\n",
                "        DATA_DIR = path\n",
                "        break\n",
                "\n",
                "if DATA_DIR is None:\n",
                "    raise FileNotFoundError(\"Dataset non trovato! Assicurati di aver collegato 'aura-data'.\")\n",
                "\n",
                "print(f\"‚úÖ Dataset trovato: {DATA_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Model Architectures (Theoretical Alignment)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AURA(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained(config['encoder'])\n",
                "        hidden_size = self.bert.config.hidden_size\n",
                "        self.dropout = nn.Dropout(config['dropout'])\n",
                "        self.toxicity_head = nn.Linear(hidden_size, 2)\n",
                "        self.emotion_head = nn.Linear(hidden_size, config['num_emotion_classes'])\n",
                "        \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = self.dropout(outputs.pooler_output)\n",
                "        return self.toxicity_head(pooled), self.emotion_head(pooled)\n",
                "\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, gamma=2.0):\n",
                "        super().__init__()\n",
                "        self.gamma = gamma\n",
                "        \n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        return (((1 - pt) ** self.gamma) * ce_loss).mean()\n",
                "\n",
                "class UncertaintyLoss(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.log_vars = nn.Parameter(torch.zeros(2))\n",
                "        \n",
                "    def forward(self, loss_tox, loss_emo):\n",
                "        p1 = torch.exp(-self.log_vars[0])\n",
                "        p2 = torch.exp(-self.log_vars[1])\n",
                "        return p1 * loss_tox + self.log_vars[0] + p2 * loss_emo + self.log_vars[1]\n",
                "\n",
                "def load_weights(path):\n",
                "    if os.path.exists(path):\n",
                "        with open(path) as f:\n",
                "            data = json.load(f)\n",
                "            return torch.tensor(data.get('weights', [1.0]*7), dtype=torch.float32)\n",
                "    return torch.ones(7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Multitask Dataset Handler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AURADataset(Dataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_length, is_toxicity=True):\n",
                "        self.df = pd.read_csv(csv_path)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "        self.is_toxicity = is_toxicity\n",
                "        self.emo_cols = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        text = str(row.get('text', row.get('tweet', '')))\n",
                "        enc = self.tokenizer.encode_plus(\n",
                "            text, add_special_tokens=True, max_length=self.max_length,\n",
                "            padding='max_length', truncation=True, return_tensors='pt'\n",
                "        )\n",
                "        tox_label = -1\n",
                "        emo_label = torch.full((7,), -1.0)\n",
                "        if self.is_toxicity:\n",
                "            label_raw = row['label'] if 'label' in row else row.get('subtask_a', 'NOT')\n",
                "            tox_label = 1 if label_raw in [1, 'OFF'] else 0\n",
                "        else:\n",
                "            emo_label = torch.tensor([float(row[c]) for c in self.emo_cols], dtype=torch.float32)\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(),\n",
                "            'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'toxicity_target': torch.tensor(tox_label, dtype=torch.long),\n",
                "            'emotion_target': emo_label\n",
                "        }\n",
                "\n",
                "tokenizer = BertTokenizer.from_pretrained(CONFIG['encoder'])\n",
                "olid_train = AURADataset(f\"{DATA_DIR}/olid_train.csv\", tokenizer, CONFIG['max_length'], is_toxicity=True)\n",
                "olid_val = AURADataset(f\"{DATA_DIR}/olid_validation.csv\", tokenizer, CONFIG['max_length'], is_toxicity=True)\n",
                "goemo_full = AURADataset(f\"{DATA_DIR}/goemotions_processed.csv\", tokenizer, CONFIG['max_length'], is_toxicity=False)\n",
                "indices = np.random.choice(len(goemo_full), 30000, replace=False)\n",
                "goemo_subset = torch.utils.data.Subset(goemo_full, indices)\n",
                "train_set = ConcatDataset([olid_train, goemo_subset])\n",
                "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
                "val_loader = DataLoader(olid_val, batch_size=CONFIG['batch_size'], num_workers=2)\n",
                "print(f\"‚úÖ Training set: {len(train_set)} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ MTL Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.cuda.amp import GradScaler, autocast\n",
                "model = AURA(CONFIG).to(device)\n",
                "uncertainty_loss = UncertaintyLoss().to(device)\n",
                "focal_loss = FocalLoss(CONFIG['focal_gamma']).to(device)\n",
                "scaler = GradScaler()\n",
                "optimizer = optim.AdamW([\n",
                "    {'params': model.bert.parameters(), 'lr': CONFIG['lr_bert']},\n",
                "    {'params': model.toxicity_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': model.emotion_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': uncertainty_loss.parameters(), 'lr': CONFIG['lr_heads']}\n",
                "], weight_decay=CONFIG['weight_decay'])\n",
                "emo_weights = load_weights(f\"{DATA_DIR}/class_weights.json\").to(device)\n",
                "criterion_emo = nn.BCEWithLogitsLoss(pos_weight=emo_weights)\n",
                "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation']\n",
                "scheduler = OneCycleLR(optimizer, max_lr=[CONFIG['lr_bert'], CONFIG['lr_heads'], CONFIG['lr_heads'], CONFIG['lr_heads']], total_steps=total_steps, pct_start=0.1)\n",
                "\n",
                "def train_step(batch, i):\n",
                "    input_ids = batch['input_ids'].to(device)\n",
                "    attention_mask = batch['attention_mask'].to(device)\n",
                "    tox_targets = batch['toxicity_target'].to(device)\n",
                "    emo_targets = batch['emotion_target'].to(device)\n",
                "    with autocast():\n",
                "        tox_logits, emo_logits = model(input_ids, attention_mask)\n",
                "        tox_mask = tox_targets != -1\n",
                "        loss_tox = focal_loss(tox_logits[tox_mask], tox_targets[tox_mask]) if tox_mask.sum() > 0 else torch.tensor(0.0, device=device)\n",
                "        emo_mask = emo_targets[:, 0] != -1\n",
                "        loss_emo = criterion_emo(emo_logits[emo_mask], emo_targets[emo_mask]) if emo_mask.sum() > 0 else torch.tensor(0.0, device=device)\n",
                "        loss = (uncertainty_loss(loss_tox, loss_emo)) / CONFIG['gradient_accumulation']\n",
                "    scaler.scale(loss).backward()\n",
                "    if (i + 1) % CONFIG['gradient_accumulation'] == 0:\n",
                "        scaler.unscale_(optimizer)\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        scheduler.step()\n",
                "        optimizer.zero_grad()\n",
                "    return loss.item() * CONFIG['gradient_accumulation']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_f1 = 0\n",
                "patience = 0\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
                "    for i, batch in enumerate(pbar):\n",
                "        loss = train_step(batch, i)\n",
                "        total_loss += loss\n",
                "        if i % 100 == 0: pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
                "    model.eval()\n",
                "    preds, labels = [], []\n",
                "    with torch.no_grad():\n",
                "        for batch in val_loader:\n",
                "            tox_logits, _ = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
                "            preds.extend(torch.argmax(tox_logits, dim=1).cpu().numpy())\n",
                "            labels.extend(batch['toxicity_target'].cpu().numpy())\n",
                "    val_f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
                "    print(f\"Epoch {epoch}: Avg Loss={total_loss/len(train_loader):.4f}, Val F1={val_f1:.4f}\")\n",
                "    if val_f1 > best_f1:\n",
                "        best_f1 = val_f1\n",
                "        torch.save(model.state_dict(), \"aura_mtl_best.pt\")\n",
                "        patience = 0\n",
                "    else:\n",
                "        patience += 1\n",
                "        if patience >= CONFIG['patience']: break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Stress Testing (Cross-Domain Robustness)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.load_state_dict(torch.load(\"aura_mtl_best.pt\"))\n",
                "model.eval()\n",
                "class StressDataset(Dataset):\n",
                "    def __init__(self, path, tokenizer):\n",
                "        if path.endswith('.json'):\n",
                "            with open(path) as f: self.data = json.load(f)\n",
                "        else:\n",
                "            self.data = pd.read_csv(path).to_dict('records')\n",
                "        self.tokenizer = tokenizer\n",
                "    def __len__(self): return len(self.data)\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.data[idx]\n",
                "        text = str(item.get('text', item.get('tweet', '')))\n",
                "        label_raw = item.get('label', item.get('subtask_a', 'NOT'))\n",
                "        label = 1 if label_raw in ['OFF', 1] else 0\n",
                "        enc = self.tokenizer.encode_plus(text, max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
                "        return {'ids': enc['input_ids'].flatten(), 'mask': enc['attention_mask'].flatten(), 'label': label}\n",
                "def stress_test(name, path):\n",
                "    ds = StressDataset(path, tokenizer)\n",
                "    dl = DataLoader(ds, batch_size=32)\n",
                "    preds, labels = [], []\n",
                "    for batch in dl:\n",
                "        with torch.no_grad():\n",
                "            logits, _ = model(batch['ids'].to(device), batch['mask'].to(device))\n",
                "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
                "            labels.extend(batch['label'].numpy())\n",
                "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
                "    print(f\"üéØ {name}: Macro-F1 = {f1:.4f}\")\n",
                "    return f1\n",
                "f1_l1 = stress_test(\"Level 1 (OLID)\", f\"{DATA_DIR}/olid_test.csv\")\n",
                "f1_l2 = stress_test(\"Level 2 (Jigsaw)\", f\"{DATA_DIR}/jigsaw_test.json\")\n",
                "f1_l3 = stress_test(\"Level 3 (ToxiGen)\", f\"{DATA_DIR}/toxigen_test.json\")\n",
                "print(f\"\\nüöÄ Cross-Domain Delta (ŒîF1): {f1_l1 - f1_l3:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
