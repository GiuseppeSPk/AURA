{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AURA V12 CERBERUS - \"OMEGA\" Protocol (Grand Unified)\n",
                "\n",
                "**Components**:\n",
                "1. **Hard Negatives**: Training data augmented with emotionally-charged but non-toxic examples.\n",
                "2. **Soft Labels (Perspectivist NLP)**: Label Smoothing to model annotator uncertainty.\n",
                "3. **Bias Initialization**: Toxicity head starts biased towards Non-Toxic (Module 3).\n",
                "4. **Progressive Freezing**: Ice Age protocol from V10.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Imports & Setup\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
                "from torch.optim.lr_scheduler import OneCycleLR\n",
                "from transformers import BertModel, BertTokenizer\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.metrics import f1_score\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "SEED = 42\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.backends.cudnn.deterministic = True\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Configuration V12 (OMEGA)\n",
                "CONFIG = {\n",
                "    'encoder': 'bert-base-uncased',\n",
                "    'max_length': 128,\n",
                "    'num_emotion_classes': 7,\n",
                "    'dropout': 0.5,\n",
                "    'weight_decay': 0.1,\n",
                "    'batch_size': 32,\n",
                "    'gradient_accumulation': 2,\n",
                "    'epochs': 5,\n",
                "    'lr_bert': 2e-5,\n",
                "    'lr_heads': 3e-5,\n",
                "    'warmup_ratio': 0.1,\n",
                "    'patience': 5,\n",
                "    'focal_gamma': 2.0,\n",
                "    'freezing_epochs': 1,\n",
                "    # V12 SPECIFIC\n",
                "    'label_smoothing': 0.1,  # Perspectivist NLP: Soft Labels\n",
                "}\n",
                "\n",
                "EMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
                "DATA_DIR = '/kaggle/input/aura-v12-data'  # <-- Updated for V12 dataset with Hard Negatives\n",
                "print(\"V12 OMEGA Loaded: Hard Negatives + Soft Labels Active ðŸŽ¯\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: AURA V12 Model (Same as V10, with Bias Init)\n",
                "class AURA_CERBERUS(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained(config['encoder'])\n",
                "        hidden = self.bert.config.hidden_size\n",
                "        self.dropout = nn.Dropout(config['dropout'])\n",
                "        \n",
                "        self.toxicity_head = nn.Linear(hidden, 2)\n",
                "        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n",
                "        self.sentiment_head = nn.Linear(hidden, 2)\n",
                "        \n",
                "        # Uncertainty parameters\n",
                "        self.log_var_tox = nn.Parameter(torch.tensor(0.0))\n",
                "        self.log_var_emo = nn.Parameter(torch.tensor(0.0))\n",
                "        self.log_var_sent = nn.Parameter(torch.tensor(0.0))\n",
                "\n",
                "        # Bias Initialization (Module 3 - Imbalanced Datasets)\n",
                "        with torch.no_grad():\n",
                "             self.toxicity_head.bias[0] = 2.5  # Non-Toxic (Majority)\n",
                "             self.toxicity_head.bias[1] = -2.5 # Toxic (Minority)\n",
                "        \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = self.dropout(outputs.pooler_output)\n",
                "        \n",
                "        return {\n",
                "            'toxicity': self.toxicity_head(pooled),\n",
                "            'emotion': self.emotion_head(pooled),\n",
                "            'sentiment': self.sentiment_head(pooled),\n",
                "            'log_var_tox': self.log_var_tox,\n",
                "            'log_var_emo': self.log_var_emo,\n",
                "            'log_var_sent': self.log_var_sent\n",
                "        }\n",
                "\n",
                "print('Model AURA V12 defined (with Bias Init).')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Loss Functions (WITH LABEL SMOOTHING)\n",
                "def focal_loss_smooth(logits, targets, gamma=2.0, weight=None, smoothing=0.0):\n",
                "    \"\"\"Focal Loss with optional Label Smoothing (Perspectivist NLP).\"\"\"\n",
                "    # Label Smoothing: instead of [0, 1], use [smoothing/2, 1 - smoothing/2]\n",
                "    ce = F.cross_entropy(logits, targets, weight=weight, reduction='none', label_smoothing=smoothing)\n",
                "    p_t = torch.exp(-ce)\n",
                "    return ((1 - p_t) ** gamma * ce).mean()\n",
                "\n",
                "def kendall_loss(task_loss, log_var):\n",
                "    log_var = torch.clamp(log_var, min=-5.0, max=5.0)\n",
                "    precision = torch.exp(-log_var)\n",
                "    return precision * task_loss + log_var\n",
                "\n",
                "def compute_mtl_loss(outputs, batch, tox_weights, gamma, smoothing):\n",
                "    total_loss = torch.tensor(0.0, device=outputs['toxicity'].device)\n",
                "    \n",
                "    if batch['task_mask_tox'].sum() > 0:\n",
                "        # Toxicity with Focal Loss + Label Smoothing\n",
                "        l = focal_loss_smooth(outputs['toxicity'][batch['task_mask_tox']], \n",
                "                              batch['tox_label'][batch['task_mask_tox']], \n",
                "                              gamma, tox_weights, smoothing)\n",
                "        total_loss += kendall_loss(l, outputs['log_var_tox'])\n",
                "        \n",
                "    if batch['task_mask_emo'].sum() > 0:\n",
                "        l = F.binary_cross_entropy_with_logits(outputs['emotion'][batch['task_mask_emo']], \n",
                "                                                batch['emo_label'][batch['task_mask_emo']])\n",
                "        total_loss += kendall_loss(l, outputs['log_var_emo'])\n",
                "        \n",
                "    if batch['task_mask_sent'].sum() > 0:\n",
                "        l = focal_loss_smooth(outputs['sentiment'][batch['task_mask_sent']], \n",
                "                              batch['sent_label'][batch['task_mask_sent']], \n",
                "                              gamma, smoothing=smoothing)  # No weights for sentiment\n",
                "        total_loss += kendall_loss(l, outputs['log_var_sent'])\n",
                "        \n",
                "    return total_loss\n",
                "\n",
                "print('V12 Loss defined (Focal + Label Smoothing).')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Datasets (Standard from V10)\n",
                "class BaseDataset(Dataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_len):\n",
                "        self.df = pd.read_csv(csv_path)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_len = max_len\n",
                "    def __len__(self): return len(self.df)\n",
                "    def encode(self, text):\n",
                "        return self.tokenizer.encode_plus(str(text), max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
                "\n",
                "class ToxicityDataset(BaseDataset):\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(), 'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'tox_label': torch.tensor(int(row['label']), dtype=torch.long), \n",
                "            'emo_label': torch.zeros(len(EMO_COLS)), \n",
                "            'sent_label': torch.tensor(-1, dtype=torch.long),\n",
                "            'task': 'toxicity'\n",
                "        }\n",
                "\n",
                "class EmotionDataset(BaseDataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_len, emo_cols):\n",
                "        super().__init__(csv_path, tokenizer, max_len)\n",
                "        self.emo_cols = emo_cols\n",
                "        if 'label_sum' in self.df.columns: self.df = self.df[self.df['label_sum'] > 0].reset_index(drop=True)\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(), 'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'tox_label': torch.tensor(-1, dtype=torch.long), \n",
                "            'emo_label': torch.tensor([float(row[c]) for c in self.emo_cols]), \n",
                "            'sent_label': torch.tensor(-1, dtype=torch.long),\n",
                "            'task': 'emotion'\n",
                "        }\n",
                "\n",
                "class SentimentDataset(BaseDataset):\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(), 'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'tox_label': torch.tensor(-1, dtype=torch.long), \n",
                "            'emo_label': torch.zeros(len(EMO_COLS)), \n",
                "            'sent_label': torch.tensor(int(row['label']), dtype=torch.long),\n",
                "            'task': 'sentiment'\n",
                "        }\n",
                "\n",
                "def collate_fn(batch):\n",
                "    tasks = [x['task'] for x in batch]\n",
                "    return {\n",
                "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
                "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
                "        'tox_label': torch.stack([x['tox_label'] for x in batch]),\n",
                "        'emo_label': torch.stack([x['emo_label'] for x in batch]),\n",
                "        'sent_label': torch.stack([x['sent_label'] for x in batch]),\n",
                "        'task_mask_tox': torch.tensor([t == 'toxicity' for t in tasks], dtype=torch.bool), \n",
                "        'task_mask_emo': torch.tensor([t == 'emotion' for t in tasks], dtype=torch.bool),\n",
                "        'task_mask_sent': torch.tensor([t == 'sentiment' for t in tasks], dtype=torch.bool)\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Loaders (Using V12 Dataset with Hard Negatives)\n",
                "tokenizer = BertTokenizer.from_pretrained(CONFIG['encoder'])\n",
                "\n",
                "# NOTE: toxicity_train.csv now includes Hard Negatives!\n",
                "tox_train = ToxicityDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'])\n",
                "emo_train = EmotionDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\n",
                "sent_train = SentimentDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'])\n",
                "tox_val = ToxicityDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, CONFIG['max_length'])\n",
                "\n",
                "train_set = ConcatDataset([tox_train, emo_train, sent_train])\n",
                "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
                "val_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
                "tox_weights = torch.tensor([0.75, 1.5], device=device)\n",
                "\n",
                "print(f\"Toxicity Train (with Hard Negatives): {len(tox_train)} samples\")\n",
                "print(\"Datasets loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Training Engine (with Freezing)\n",
                "\n",
                "def train_epoch(model, loader, optimizer, scheduler, config, tox_weights, epoch):\n",
                "    model.train()\n",
                "    \n",
                "    if epoch <= config['freezing_epochs']:\n",
                "        print(f\"â„ï¸ EPOCH {epoch}: ICE AGE. Backbone Frozen.\")\n",
                "        for param in model.bert.parameters(): param.requires_grad = False\n",
                "    else:\n",
                "        print(f\"ðŸ”¥ EPOCH {epoch}: FIRE. Backbone Unfrozen.\")\n",
                "        for param in model.bert.parameters(): param.requires_grad = True\n",
                "            \n",
                "    total_loss = 0\n",
                "    optimizer.zero_grad()\n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    \n",
                "    for step, batch in enumerate(pbar):\n",
                "        for k, v in batch.items(): \n",
                "            if isinstance(v, torch.Tensor): batch[k] = v.to(device)\n",
                "            \n",
                "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
                "        loss = compute_mtl_loss(outputs, batch, tox_weights, config['focal_gamma'], config['label_smoothing'])\n",
                "        \n",
                "        (loss / config['gradient_accumulation']).backward()\n",
                "        \n",
                "        if (step + 1) % config['gradient_accumulation'] == 0:\n",
                "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "            optimizer.step()\n",
                "            scheduler.step()\n",
                "            optimizer.zero_grad()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        if step % 50 == 0:\n",
                "            Ïƒ_t = torch.exp(0.5 * model.log_var_tox).item()\n",
                "            pbar.set_postfix({'loss': loss.item(), 'Ïƒ_tox': f'{Ïƒ_t:.2f}'})\n",
                "            \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    preds, labels = [], []\n",
                "    for batch in tqdm(loader, desc='Validating', leave=False):\n",
                "        ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
                "        out = model(ids, mask)\n",
                "        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n",
                "        labels.extend(batch['tox_label'].to(device).cpu().numpy())\n",
                "    return f1_score(labels, preds, average='macro')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Main Loop V12\n",
                "print(\"=\"*60)\n",
                "print(\"STARTING CERBERUS V12 OMEGA TRAINING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "model = AURA_CERBERUS(CONFIG).to(device)\n",
                "optimizer = torch.optim.AdamW([\n",
                "    {'params': model.bert.parameters(), 'lr': CONFIG['lr_bert']},\n",
                "    {'params': model.toxicity_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': model.emotion_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': model.sentiment_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': [model.log_var_tox, model.log_var_emo, model.log_var_sent], 'lr': CONFIG['lr_heads']}\n",
                "], weight_decay=CONFIG['weight_decay'])\n",
                "\n",
                "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation']\n",
                "scheduler = OneCycleLR(optimizer, max_lr=[CONFIG['lr_bert']] + [CONFIG['lr_heads']]*4, total_steps=total_steps, pct_start=CONFIG['warmup_ratio'])\n",
                "\n",
                "best_val_f1 = 0.0\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, CONFIG, tox_weights, epoch)\n",
                "    val_f1 = validate(model, val_loader)\n",
                "    Ïƒ_t = torch.exp(0.5 * model.log_var_tox).item()\n",
                "    print(f'Epoch {epoch}: Loss={avg_loss:.4f}, Val F1={val_f1:.4f}, Ïƒ_tox={Ïƒ_t:.2f}')\n",
                "    if val_f1 > best_val_f1:\n",
                "        best_val_f1 = val_f1\n",
                "        torch.save(model.state_dict(), 'aura_v12_omega.pt')\n",
                "        print('>>> NEW BEST MODEL <<<')\n",
                "\n",
                "print(f\"\\nTraining Complete. Best Val F1: {best_val_f1:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}