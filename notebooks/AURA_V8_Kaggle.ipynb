{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AURA V8: 4-Task Engine (Kaggle Edition)\n\n---\n## Instructions\n1. **Runtime** -> Make sure you have GPU enabled\n2. **Add Data** -> Search and add your dataset \u0007ura-mega-data\n3. The CSV files should be directly in /kaggle/input/aura-v8-data/\n4. Run all cells below\n---\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kaggle Environment already has transformers installed\n# dataset is mounted at /kaggle/input/aura-v8-data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup & Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom transformers import BertModel, BertTokenizer\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, classification_report\nimport pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif device.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n\ntorch.manual_seed(42)\nnp.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Configuration\nCONFIG = {\n    'encoder': 'bert-base-uncased',\n    'max_length': 128,\n    'num_emotion_classes': 5,\n    'dropout': 0.4,\n    'batch_size': 32,\n    'gradient_accumulation': 2,\n    'epochs': 4,\n    'lr': 5e-6,\n    'weight_decay': 0.03,\n    'patience': 3,\n    'mc_samples': 10,\n    'focal_gamma': 2.0,\n    'output_dir': '.'\n}\n\nEMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'neutral']\nDATA_DIR = '/kaggle/input/aura-v8-data'  # Unzipped folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Model Class (4-Head)\nclass AURA_MultiTask(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(config['encoder'])\n        hidden = self.bert.config.hidden_size\n        self.dropout = nn.Dropout(config['dropout'])\n        \n        self.toxicity_head = nn.Linear(hidden, 2)\n        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n        self.sentiment_head = nn.Linear(hidden, 2)\n        self.hate_head = nn.Linear(hidden, 2)\n        \n        self.tox_log_var = nn.Parameter(torch.zeros(1))\n        self.emo_log_var = nn.Parameter(torch.zeros(1))\n        self.sent_log_var = nn.Parameter(torch.zeros(1))\n        self.hate_log_var = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = self.dropout(outputs.pooler_output)\n        \n        return {\n            'toxicity': self.toxicity_head(pooled),\n            'emotion': self.emotion_head(pooled),\n            'sentiment': self.sentiment_head(pooled),\n            'hate': self.hate_head(pooled),\n            'log_vars': {\n                'toxicity': self.tox_log_var,\n                'emotion': self.emo_log_var,\n                'sentiment': self.sent_log_var,\n                'hate': self.hate_log_var\n            }\n        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Loss Functions\ndef focal_loss_with_uncertainty(logits, log_var, targets, gamma=2.0, T=10, label_smoothing=0.1):\n    log_var = torch.clamp(log_var, -10, 10).squeeze()  # Squeeze to scalar\n    std = torch.exp(0.5 * log_var)\n    \n    # Label Smoothing: soften targets to prevent overconfidence\n    # This is equivalent to mixing with uniform distribution\n    logits_exp = logits.unsqueeze(0).expand(T, -1, -1)\n    noise = torch.randn_like(logits_exp)\n    corrupted = logits_exp + noise * std\n    probs = F.softmax(corrupted, dim=-1)\n    avg_probs = probs.mean(dim=0)\n    p_t = avg_probs[range(len(targets)), targets]\n    focal_weight = (1 - p_t) ** gamma\n    loss = (focal_weight * (-torch.log(p_t + 1e-8))).mean()\n    return loss + 0.5 * log_var\n\ndef mc_bce_loss(logits, log_var, targets, T=10):\n    log_var = torch.clamp(log_var, -10, 10).squeeze()  # Squeeze to scalar\n    std = torch.exp(0.5 * log_var)\n    \n    # Label Smoothing: soften targets to prevent overconfidence\n    # This is equivalent to mixing with uniform distribution\n    logits_exp = logits.unsqueeze(0).expand(T, -1, -1)\n    noise = torch.randn_like(logits_exp)\n    corrupted = logits_exp + noise * std\n    probs = torch.sigmoid(corrupted)\n    avg_probs = probs.mean(dim=0)\n    return F.binary_cross_entropy(avg_probs, targets, reduction='mean') + 0.5 * log_var\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Data Loading with Custom Collate\ndef custom_collate(batch):\n    \"\"\"Handle mixed label shapes (scalar for binary, [5] for emotions)\"\"\"\n    input_ids = torch.stack([x['input_ids'] for x in batch])\n    attention_mask = torch.stack([x['attention_mask'] for x in batch])\n    tasks = [x['task'] for x in batch]\n    \n    # Pad labels to same size (max 5 for emotions)\n    labels = []\n    for x in batch:\n        lbl = x['label']\n        if lbl.dim() == 0:  # scalar -> pad to [5]\n            padded = torch.zeros(5)\n            padded[0] = lbl.item()\n            labels.append(padded)\n        else:\n            labels.append(lbl)\n    labels = torch.stack(labels)\n    \n    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': labels, 'task': tasks}\n\nclass TaskDataset(Dataset):\n    def __init__(self, csv_path, tokenizer, max_len, task_type, emo_cols=None):\n        self.df = pd.read_csv(csv_path)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.task_type = task_type\n        self.emo_cols = emo_cols or EMO_COLS\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = str(row['text'])\n        enc = self.tokenizer.encode_plus(text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n        item = {'input_ids': enc['input_ids'].flatten(), 'attention_mask': enc['attention_mask'].flatten(), 'task': self.task_type}\n        if self.task_type == 'emotion':\n            item['label'] = torch.tensor([float(row[c]) for c in self.emo_cols])\n        else:\n            item['label'] = torch.tensor(int(row['label']))\n        return item\n\ntokenizer = BertTokenizer.from_pretrained(CONFIG['encoder'])\ntox_train = TaskDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'], 'toxicity')\nemo_train = TaskDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], 'emotion')\nsent_train = TaskDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'], 'sentiment')\nhate_train = TaskDataset(f'{DATA_DIR}/hate_train.csv', tokenizer, CONFIG['max_length'], 'hate')\ntox_val = TaskDataset(f'{DATA_DIR}/toxicity_validation.csv', tokenizer, CONFIG['max_length'], 'toxicity')\n\n# --- BALANCED TASK SAMPLING ---\n# Cap sentiment (73k) to match other tasks (~15k each)\n# This prevents sentiment gradients from dominating toxicity learning\n\nfrom torch.utils.data import Subset\nimport random\n\ndef balance_dataset(ds, max_samples=15000):\n    if len(ds) > max_samples:\n        indices = random.sample(range(len(ds)), max_samples)\n        return Subset(ds, indices)\n    return ds\n\n# Apply balancing (critical for preventing overfit)\ntox_train_bal = balance_dataset(tox_train, 12000)  # Keep all toxicity\nemo_train_bal = balance_dataset(emo_train, 15000)  # Keep most emotions\nsent_train_bal = balance_dataset(sent_train, 15000)  # CAP sentiment!\nhate_train_bal = balance_dataset(hate_train, 12000)  # Keep all hate\n\ntrain_set = ConcatDataset([tox_train_bal, emo_train_bal, sent_train_bal, hate_train_bal])\nprint(f'Balanced Training Set: {len(train_set)} samples (was 113k)')\ntrain_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=custom_collate)\nval_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=custom_collate)\n\nprint(f'Total Samples: {len(train_set)}')\nprint(f'Train Loader Batches: {len(train_loader)}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Training\ndef train_epoch(model, loader, optimizer, scheduler, config):\n    model.train()\n    total_loss = 0\n    loop = tqdm(loader, desc='Training')\n    optimizer.zero_grad()\n    tox_preds, tox_labels = [], []\n    \n    for step, batch in enumerate(loop):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        tasks = batch['task']\n        \n        outputs = model(input_ids, attention_mask)\n        loss = torch.tensor(0.0, device=device)\n        \n        for task in ['toxicity', 'sentiment', 'hate']:\n            mask = [t == task for t in tasks]\n            if sum(mask) > 0:\n                task_logits = outputs[task][mask]\n                task_labels = labels[mask][:, 0].long()  # Extract first element\n                loss += focal_loss_with_uncertainty(task_logits, outputs['log_vars'][task], task_labels, config['focal_gamma'], config['mc_samples'])\n                if task == 'toxicity':\n                    tox_preds.extend(torch.argmax(task_logits, dim=1).cpu().numpy())\n                    tox_labels.extend(task_labels.cpu().numpy())\n        \n        emo_mask = [t == 'emotion' for t in tasks]\n        if sum(emo_mask) > 0:\n            loss += mc_bce_loss(outputs['emotion'][emo_mask], outputs['log_vars']['emotion'], labels[emo_mask].float(), config['mc_samples'])\n        \n        loss = loss / config['gradient_accumulation']\n        loss.backward()\n        \n        if (step + 1) % config['gradient_accumulation'] == 0:\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        total_loss += loss.item() * config['gradient_accumulation']\n        loop.set_postfix(loss=loss.item())\n        \n    return total_loss / len(loader), f1_score(tox_labels, tox_preds, average='macro') if tox_labels else 0\n\n@torch.no_grad()\ndef validate(model, loader):\n    model.eval()\n    preds, labels = [], []\n    for batch in tqdm(loader, desc='Validating', leave=False):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        tox_labels = batch['label'][:, 0].long().to(device)  # Extract first element\n        outputs = model(input_ids, attention_mask)\n        preds.extend(torch.argmax(outputs['toxicity'], dim=1).cpu().numpy())\n        labels.extend(tox_labels.cpu().numpy())\n    return f1_score(labels, preds, average='macro')\n\n# --- MAIN LOOP ---\nmodel = AURA_MultiTask(CONFIG).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\nscheduler = OneCycleLR(optimizer, max_lr=CONFIG['lr'], total_steps=len(train_loader)*CONFIG['epochs']//CONFIG['gradient_accumulation'])\n\nbest_f1 = 0\nno_improve_count = 0\nPATIENCE = 2  # Stop if no improvement for 2 epochs\nhistory = {'train_loss': [], 'train_f1': [], 'val_f1': []}\nprint('STARTING V8 TRAINING')\nfor epoch in range(1, CONFIG['epochs'] + 1):\n    loss, train_f1 = train_epoch(model, train_loader, optimizer, scheduler, CONFIG)\n    val_f1 = validate(model, val_loader)\n    print(f'Epoch {epoch}: Train Loss={loss:.4f}, Train F1={train_f1:.4f}, Val F1={val_f1:.4f}')\n    history['train_loss'].append(loss)\n    history['train_f1'].append(train_f1)\n    history['val_f1'].append(val_f1)\n    if val_f1 > best_f1:\n        best_f1 = val_f1\n        no_improve_count = 0\n        torch.save(model.state_dict(), 'aura_v8_best.pt')\n        print('  NEW BEST!')\n    else:\n        no_improve_count += 1\n        if no_improve_count >= PATIENCE:\n            print(f'Early stopping triggered at epoch {epoch}!')\n            break\nprint(f'COMPLETE. Best: {best_f1:.4f}')\n\n\n# Save history\nimport pickle\nwith open('history_v8.pkl', 'wb') as f:\n    pickle.dump(history, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_training_history(history):\n    epochs = range(1, len(history['train_loss']) + 1)\n    plt.figure(figsize=(15, 5))\n    \n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], 'b-o', label='Train Loss')\n    plt.title('Training Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot F1\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_f1'], 'b-o', label='Train F1')\n    plt.plot(epochs, history['val_f1'], 'r-s', label='Validation F1')\n    plt.title('F1 Score')\n    plt.xlabel('Epochs')\n    plt.ylabel('F1')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('aura_v8_training_curves.png')\n    plt.show()\n\nif 'history' in locals():\n    plot_training_history(history)\nelif os.path.exists('history_v8.pkl'):\n    with open('history_v8.pkl', 'rb') as f:\n        history = pickle.load(f)\n    plot_training_history(history)\nelse:\n    print('No history found. Training must complete first.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}