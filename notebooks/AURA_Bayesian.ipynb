{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ›ï¸ AURA: The Bayesian Correction (Monte Carlo Uncertainty)\n",
                "\n",
                "This notebook implements the theoretically correct version of Kendall's Uncertainty Loss for **Classification** tasks.\n",
                "Unlike the Regression approximation (1/2sigma^2), we use **Monte Carlo Integration** over the logits before Softmax.\n",
                "\n",
                "Paper: *Multi-Task Learning Using Uncertainty to Weigh Losses* (Kendall et al., 2018), Equation 12."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "from transformers import BertModel, BertTokenizer\n",
                "\n",
                "CONFIG = {\n",
                "    'model_name': 'bert-base-uncased',\n",
                "    'max_length': 128,\n",
                "    'batch_size': 32,\n",
                "    'mc_samples': 10, # Number of Monte Carlo samples (T)\n",
                "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Bayesian Model\n",
                "The model now outputs two values for each head: **Logits** ($f(x)$) and **Log Variance** ($s = \\log \\sigma^2$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AURA_Bayesian(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained(CONFIG['model_name'])\n",
                "        self.dropout = nn.Dropout(0.1)\n",
                "        \n",
                "        # Toxicity Head (2 classes)\n",
                "        self.tox_linear = nn.Linear(768, 2)\n",
                "        self.tox_var = nn.Linear(768, 1) # Outputs log(sigma^2)\n",
                "        \n",
                "        # Emotion Head (7 classes)\n",
                "        self.emo_linear = nn.Linear(768, 7)\n",
                "        self.emo_var = nn.Linear(768, 1) # Outputs log(sigma^2)\n",
                "        \n",
                "    def forward(self, ids, mask):\n",
                "        o = self.bert(ids, attention_mask=mask).pooler_output\n",
                "        o = self.dropout(o)\n",
                "        \n",
                "        tox_logits = self.tox_linear(o)\n",
                "        tox_log_var = self.tox_var(o)\n",
                "        \n",
                "        emo_logits = self.emo_linear(o)\n",
                "        emo_log_var = self.emo_var(o)\n",
                "        \n",
                "        return tox_logits, tox_log_var, emo_logits, emo_log_var"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Monte Carlo Uncertainty Loss\n",
                "Implementation of Eq. 12 from Kendall (2018).\n",
                "We sample minimal noise $\\epsilon \\sim \\mathcal{N}(0, I)$ scaled by $\\sigma$ and average the resultant probabilities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BayesianClassificationLoss(nn.Module):\n",
                "    def __init__(self, T=10):\n",
                "        super().__init__()\n",
                "        self.T = T\n",
                "        \n",
                "    def forward(self, logits, log_var, targets):\n",
                "        # logits: [Batch, Classes]\n",
                "        # log_var: [Batch, 1]\n",
                "        # targets: [Batch]\n",
                "        \n",
                "        std = torch.exp(0.5 * log_var) # sigma = sqrt(exp(log_var))\n",
                "        \n",
                "        loss = 0\n",
                "        # Monte Carlo Integration\n",
                "        for t in range(self.T):\n",
                "            # Sample epsilon\n",
                "            epsilon = torch.randn_like(logits) * std # [Batch, Classes]\n",
                "            \n",
                "            # Corrupted Logits: f(x) + sigma * epsilon\n",
                "            sampled_logits = logits + epsilon\n",
                "            \n",
                "            # Compute Cross Entropy for this sample\n",
                "            # We use NLLLoss because we need to work with Log Softmax\n",
                "            # Actually, standard CrossEntropy includes Softmax. \n",
                "            # But we need sum(exp) inside log.\n",
                "            \n",
                "            # Eq 12: - log ( 1/T sum ( exp( x_hat_t,c - log sum exp x_hat_t,c' ) ) )\n",
                "            # This is hard to implement numerically stable with CrossEntropy class.\n",
                "            # Simplification: Average the CrossEntropy losses? No, Average Probabilities.\n",
                "            \n",
                "            # Let's trust PyTorch's logsumexp for stability\n",
                "            pass\n",
                "        \n",
                "        # Approximate implementation (Standard in repos):\n",
                "        # Sample T logits, compute Softmax, Average Probabilities, then NLL.\n",
                "        \n",
                "        # shape: [T, Batch, Classes]\n",
                "        sampled_logits = torch.distributions.Normal(logits, std).rsample((self.T,))\n",
                "        \n",
                "        # Softmax over classes\n",
                "        sampled_probs = F.softmax(sampled_logits, dim=-1)\n",
                "        \n",
                "        # Average over T samples\n",
                "        avg_probs = torch.mean(sampled_probs, dim=0)\n",
                "        \n",
                "        # NLL Loss\n",
                "        loss = F.nll_loss(torch.log(avg_probs + 1e-8), targets)\n",
                "        return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop Integration\n",
                "We instantiate two Bayesian Losses (one for Toxicity, one for Emotion) and sum them directly.\n",
                "Note: Since the loss itself is self-regulating (high uncertainty -> high sigma -> spread probabilities -> high entopy but handled by NLL), we sum them.\n",
                "\n",
                "Actually, Kendall's Eq 12 describes the loss for *one* task. For MTL, we sum the Bayesian losses: $L_{total} = L_{tox\_bayes} + L_{emo\_bayes}$."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
