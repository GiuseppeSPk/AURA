{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016caebd",
   "metadata": {},
   "source": [
    "# üß™ AURA BASELINE - Professor's Control Experiment\n",
    "\n",
    "## Hypothesis to Test\n",
    "**Professor's Counter-Hypothesis**: \n",
    "> *\"A large enough model could disentangle the four channels by virtue of training only\"*\n",
    "\n",
    "## Baseline Architecture\n",
    "- **Model**: RoBERTa-base (standard, no modifications)\n",
    "- **Training**: Fine-tune on concatenation of all 4 datasets\n",
    "- **No Task-Specific Attention**\n",
    "- **No Kendall Loss** (simple cross-entropy)\n",
    "\n",
    "## Goal\n",
    "Compare with AURA V10 to prove Task-Specific MHA adds value.\n",
    "\n",
    "**Expected**: Baseline F1 < AURA V10 F1 (0.7536)  \n",
    "**If True**: Task-specific architecture is justified ‚úÖ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28182503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üîß Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (Same as AURA V10 where applicable)\n",
    "CONFIG = {\n",
    "    'encoder': 'roberta-base',\n",
    "    'max_length': 128,\n",
    "    'dropout': 0.3,\n",
    "    'batch_size': 32,  # L40S can handle it\n",
    "    'epochs': 15,      # Enough for convergence\n",
    "    'lr': 2e-5,        # Standard BERT fine-tuning LR\n",
    "    'weight_decay': 0.01,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'focal_gamma': 2.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    'patience': 5\n",
    "}\n",
    "\n",
    "DATA_DIR = './aura-v10-data'  # Lightning.ai path\n",
    "print('üìã BASELINE Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'   {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Dataset (All tasks treated as Toxicity binary)\n",
    "class UnifiedDataset(Dataset):\n",
    "    \"\"\"Concatenate all datasets and use only toxicity labels.\n",
    "    \n",
    "    This is the BASELINE approach: expose RoBERTa to diverse data\n",
    "    but without task-specific architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, tokenizer, max_len, task_name):\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.task = task_name\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tok(\n",
    "            str(text),\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc = self.encode(row['text'])\n",
    "        \n",
    "        # Extract toxicity label (0 or 1)\n",
    "        if self.task == 'toxicity':\n",
    "            label = int(row['label'])\n",
    "        elif self.task == 'emotion':\n",
    "            # For emotion, consider ANY emotion as \"non-toxic\" (label=0)\n",
    "            label = 0\n",
    "        elif self.task == 'sentiment':\n",
    "            # Sentiment: negative=0 (non-toxic), positive=0 (non-toxic)\n",
    "            label = 0\n",
    "        elif self.task == 'reporting':\n",
    "            # Reporting: all are non-toxic examples\n",
    "            label = 0\n",
    "        \n",
    "        return {\n",
    "            'ids': enc['input_ids'].flatten(),\n",
    "            'mask': enc['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print('üì¶ Unified dataset class defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ed420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE Model: Standard RoBERTa + Linear Classifier\n",
    "class BaselineModel(nn.Module):\n",
    "    \"\"\"Simple RoBERTa ‚Üí [CLS] ‚Üí Linear(768, 2)\n",
    "    \n",
    "    NO task-specific attention\n",
    "    NO Kendall weighting\n",
    "    Just pure fine-tuning on concatenated data\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(config['encoder'])\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.classifier = nn.Linear(768, 2)  # Binary: Non-Toxic / Toxic\n",
    "        \n",
    "        # Bias initialization for imbalanced data\n",
    "        with torch.no_grad():\n",
    "            self.classifier.bias[0] = 2.5   # Non-Toxic prior\n",
    "            self.classifier.bias[1] = -2.5  # Toxic prior\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get [CLS] token representation\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [batch, 768]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(self.dropout(cls_output))  # [batch, 2]\n",
    "        return logits\n",
    "\n",
    "print('ü¶Ö Baseline model defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss (Same as AURA V10)\n",
    "def focal_loss(logits, targets, gamma=2.0, weight=None, smoothing=0.0):\n",
    "    \"\"\"Focal Loss for handling class imbalance.\"\"\"\n",
    "    ce = F.cross_entropy(logits, targets, weight=weight, \n",
    "                         reduction='none', label_smoothing=smoothing)\n",
    "    pt = torch.exp(-ce)\n",
    "    return ((1 - pt) ** gamma * ce).mean()\n",
    "\n",
    "print('‚öñÔ∏è Focal loss defined.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Datasets and Concatenate\n",
    "tokenizer = RobertaTokenizer.from_pretrained(CONFIG['encoder'])\n",
    "\n",
    "# Load each dataset\n",
    "tox_train = UnifiedDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, \n",
    "                           CONFIG['max_length'], 'toxicity')\n",
    "emo_train = UnifiedDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, \n",
    "                           CONFIG['max_length'], 'emotion')\n",
    "sent_train = UnifiedDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, \n",
    "                            CONFIG['max_length'], 'sentiment')\n",
    "rep_train = UnifiedDataset(f'{DATA_DIR}/reporting_examples.csv', tokenizer, \n",
    "                           CONFIG['max_length'], 'reporting')\n",
    "tox_val = UnifiedDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, \n",
    "                         CONFIG['max_length'], 'toxicity')\n",
    "\n",
    "# CONCATENATE all training data (Professor's baseline approach)\n",
    "train_ds = ConcatDataset([tox_train, emo_train, sent_train, rep_train])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], \n",
    "                          shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'])\n",
    "\n",
    "print('='*60)\n",
    "print('üìä BASELINE DATASET')\n",
    "print('='*60)\n",
    "print(f'Training: {len(train_ds):,} samples (concatenated)')\n",
    "print(f'  - Toxicity:  {len(tox_train):,}')\n",
    "print(f'  - Emotion:   {len(emo_train):,}')\n",
    "print(f'  - Sentiment: {len(sent_train):,}')\n",
    "print(f'  - Reporting: {len(rep_train):,}')\n",
    "print(f'Validation: {len(tox_val):,} (Toxicity only)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Optimizer Setup\n",
    "model = BaselineModel(CONFIG).to(device)\n",
    "tox_weights = torch.tensor([0.5, 2.0], device=device)  # Class weights\n",
    "\n",
    "# Optimizer (same as AURA V10 encoder LR)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=CONFIG['lr'], \n",
    "                              weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "# Scheduler with warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Optimization steps: {total_steps:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch in pbar:\n",
    "        ids = batch['ids'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(ids, mask)\n",
    "        loss = focal_loss(logits, labels, \n",
    "                         gamma=CONFIG['focal_gamma'], \n",
    "                         weight=tox_weights, \n",
    "                         smoothing=CONFIG['label_smoothing'])\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}'})\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for batch in val_loader:\n",
    "        logits = model(batch['ids'].to(device), batch['mask'].to(device))\n",
    "        preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        trues.extend(batch['label'].numpy())\n",
    "    return f1_score(trues, preds, average='macro', zero_division=0)\n",
    "\n",
    "print('üéØ Training functions ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE Training\n",
    "print('='*60)\n",
    "print('üöÄ BASELINE TRAINING START')\n",
    "print('='*60)\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    train_loss = train_epoch(epoch)\n",
    "    val_f1 = evaluate()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f'\\nEpoch {epoch}:')\n",
    "    print(f'  Train Loss: {train_loss:.4f}')\n",
    "    print(f'  Val F1:     {val_f1:.4f}')\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'baseline_best.pt')\n",
    "        print('  >>> BEST MODEL SAVED <<<')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'  (No improvement. Patience: {patience_counter}/{CONFIG[\"patience\"]})')\n",
    "        \n",
    "        if patience_counter >= CONFIG['patience']:\n",
    "            print(f'\\n‚ö†Ô∏è Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print(f'‚úÖ BASELINE Training Complete')\n",
    "print(f'üèÜ Best Val F1: {best_f1:.4f}')\n",
    "print('='*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation & Comparison\n",
    "model.load_state_dict(torch.load('baseline_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "preds, trues = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        logits = model(batch['ids'].to(device), batch['mask'].to(device))\n",
    "        preds.extend(logits.argmax(1).cpu().numpy())\n",
    "        trues.extend(batch['label'].numpy())\n",
    "\n",
    "print('--- BASELINE Classification Report ---')\n",
    "print(classification_report(trues, preds, target_names=['Non-Toxic', 'Toxic']))\n",
    "\n",
    "# Comparison with AURA V10\n",
    "print('\\n' + '='*60)\n",
    "print('üìä COMPARISON: BASELINE vs AURA V10')\n",
    "print('='*60)\n",
    "baseline_f1 = best_f1\n",
    "aura_f1 = 0.7536  # From AURA V10 run\n",
    "\n",
    "print(f'BASELINE F1:  {baseline_f1:.4f}')\n",
    "print(f'AURA V10 F1:  {aura_f1:.4f}')\n",
    "print(f'Improvement:  {((aura_f1 - baseline_f1) / baseline_f1 * 100):+.2f}%')\n",
    "print('='*60)\n",
    "\n",
    "if aura_f1 > baseline_f1:\n",
    "    print('‚úÖ CONCLUSION: Task-Specific MHA architecture is JUSTIFIED!')\n",
    "    print('   The explicit feature disentanglement provides measurable benefit.')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Professor was right: RoBERTa can disentangle without explicit architecture.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
