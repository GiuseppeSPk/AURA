{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ AURA V9 CERBERUS - Study Edition (Deep Dive)\n",
                "\n",
                "Questa edizione contiene il codice completo del protocollo **AURA V9 CERBERUS**, arricchito con spiegazioni teoriche avanzate.\n",
                "Ãˆ pensato per essere letto come un libro di testo interattivo.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ§  Teoria Avanzata: PerchÃ© CERBERUS Funziona?\n",
                "\n",
                "### 1. Hard Parameter Sharing (L'Architettura)\n",
                "Non stiamo addestrando 3 modelli separati. Stiamo addestrando un **unico cervello centrale (Encoder)** che condivide i suoi pesi ('hard sharing') per risolvere 3 problemi diversi. \n",
                "- **Vantaggio**: Il cervello Ã¨ costretto a trovare pattern universali. Non puÃ² \"barare\" memorizzando scorciatoie per un solo task.\n",
                "- **Effetto**: Questo funge da potente regolarizzatore, riducendo drasticamente l'overfitting.\n",
                "\n",
                "### 2. Homoscedastic Uncertainty (La Matematica)\n",
                "Usiamo la **Kendall Loss** (`1/ÏƒÂ² * L + log(ÏƒÂ²)`) per bilanciare i task.\n",
                "- Non Ã¨ un'incertezza sui dati (Aleatoric Heteroscedastic), ma un'incertezza sul **COMPITO** (Aleatoric Homoscedastic).\n",
                "- Il modello impara un parametro $\\sigma$ fisso per ogni task che rappresenta quanto quel compito sia \"rumoroso\" o difficile in generale.\n",
                "- **Risultato**: Il modello impara da solo a ignorare i task troppo rumorosi nelle prime fasi, per concentrarsi su quelli puliti, e poi integrare i difficili gradualmente.\n",
                "\n",
                "### 3. Differential Learning Rates (L'Ottimizzazione)\n",
                "Trattiamo gli strati della rete in modo gerarchico:\n",
                "- **BERT (Il Saggio)**: Ha giÃ  letto tutta Wikipedia. Usiamo un LR basso (`2e-5`) per non distruggere la sua conoscenza (evitando il *Catastrophic Forgetting*).\n",
                "- **Teste (Gli Apprendisti)**: Sono inizializzate a caso. Usiamo un LR alto (`5e-5`) perchÃ© devono imparare tutto da zero rapidamente.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. SETUP E RIPRODUCIBILITÃ€\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
                "from torch.optim.lr_scheduler import OneCycleLR\n",
                "from transformers import BertModel, BertTokenizer\n",
                "from tqdm.notebook import tqdm\n",
                "from sklearn.metrics import f1_score\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# DETERMINISMO SCIENTIFICO\n",
                "# Per pubblicare un paper, i risultati devono essere riproducibili.\n",
                "# Impostando questi seed, blocchiamo il caos: la sequenza di numeri casuali sarÃ  identica a ogni run.\n",
                "SEED = 42\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "torch.backends.cudnn.deterministic = True  # Disabilita algoritmi di convoluzione non deterministici sulla GPU\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device attivo: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. HYPERPARAMETERS\n",
                "\n",
                "CONFIG = {\n",
                "    'encoder': 'bert-base-uncased',\n",
                "    'max_length': 128,\n",
                "    'num_emotion_classes': 7,\n",
                "    'dropout': 0.2,\n",
                "    'batch_size': 32,\n",
                "    \n",
                "    # GRADIENT ACCUMULATION\n",
                "    # Permette di simulare un batch size enorme (es. 64 o 128) anche con poca VRAM.\n",
                "    # Invece di fare 'step()' ogni 32 esempi, accumuliamo gli errori per 2 cicli (32*2 = 64 esempi)\n",
                "    # e poi facciamo un solo passo di aggiornamento preciso.\n",
                "    'gradient_accumulation': 2,    \n",
                "\n",
                "    'epochs': 4,\n",
                "    'lr_bert': 2e-5,    # Basso per il backbone\n",
                "    'lr_heads': 5e-5,   # Alto per i classificatori\n",
                "    'weight_decay': 0.01,\n",
                "    'warmup_ratio': 0.1,\n",
                "    'patience': 3,\n",
                "    'focal_gamma': 2.0,\n",
                "}\n",
                "\n",
                "EMO_COLS = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
                "DATA_DIR = '/kaggle/input/aura-v9-data' \n",
                "print(\"Configurazione pronta.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. ARCHITETTURA (Hard Parameter Sharing)\n",
                "\n",
                "class AURA_CERBERUS(nn.Module):\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        # ENCODER CONDIVISO\n",
                "        # Questo Ã¨ il collo di bottiglia dell'informazione.\n",
                "        # Tutto (TossicitÃ , Emozioni, Sentiment) deve passare da qui.\n",
                "        self.bert = BertModel.from_pretrained(config['encoder'])\n",
                "        hidden = self.bert.config.hidden_size\n",
                "        self.dropout = nn.Dropout(config['dropout'])\n",
                "        \n",
                "        # TASK-SPECIFIC HEADS\n",
                "        # Layer leggeri che 'interpetano' il pensiero di BERT per scopi diversi.\n",
                "        self.toxicity_head = nn.Linear(hidden, 2)\n",
                "        self.emotion_head = nn.Linear(hidden, config['num_emotion_classes'])\n",
                "        self.sentiment_head = nn.Linear(hidden, 2)\n",
                "        \n",
                "        # PARAMETRI INCERTEZZA (Homoscedastic Aleatoric Uncertainty)\n",
                "        # Inizializzati come scalari logaritmici.\n",
                "        # log_var = 0.0 corrisponde a sigma = exp(0) = 1.0 (incertezza neutra).\n",
                "        # Se diventano positivi -> incertezza sale -> loss viene penalizzata.\n",
                "        # Se diventano negativi -> incertezza scende -> loss viene amplificata.\n",
                "        self.log_var_tox = nn.Parameter(torch.tensor(0.0))\n",
                "        self.log_var_emo = nn.Parameter(torch.tensor(0.0))\n",
                "        self.log_var_sent = nn.Parameter(torch.tensor(0.0))\n",
                "        \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = self.dropout(outputs.pooler_output)\n",
                "        \n",
                "        return {\n",
                "            'toxicity': self.toxicity_head(pooled),\n",
                "            'emotion': self.emotion_head(pooled),\n",
                "            'sentiment': self.sentiment_head(pooled),\n",
                "            'log_var_tox': self.log_var_tox,\n",
                "            'log_var_emo': self.log_var_emo,\n",
                "            'log_var_sent': self.log_var_sent\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. MATEMATICA E LOSS FUNCTIONS\n",
                "\n",
                "# FOCAL LOSS (Lin et al., 2017)\n",
                "# Risolve lo sbilanciamento delle classi in modo dinamico.\n",
                "# Invece di pesare le classi a priori (es: 'Tossico vale doppio'), pesa gli ESEMPI.\n",
                "# - Esempio Facile (p=0.9): Focal Loss â‰ˆ 0. Non sprecare gradienti qui.\n",
                "# - Esempio Difficile (p=0.4): Focal Loss Alta. Concentrati qui.\n",
                "def focal_loss(logits, targets, gamma=2.0, weight=None):\n",
                "    ce = F.cross_entropy(logits, targets, weight=weight, reduction='none')\n",
                "    p_t = torch.exp(-ce)\n",
                "    loss = ((1 - p_t) ** gamma * ce).mean()\n",
                "    return loss\n",
                "\n",
                "# KENDALL LOSS (Kendall et al., 2018)\n",
                "# La formula magica per Multi-Task Learning.\n",
                "# Loss = (TaskLoss / 2ÏƒÂ²) + log(Ïƒ)\n",
                "# - Il primo termine premia la precisione ma divide per l'incertezza.\n",
                "# - Il secondo termine impedisce al modello di barare aumentando infinitamente l'incertezza (Ïƒ) per azzerare la loss.\n",
                "def kendall_loss(task_loss, log_var):\n",
                "    log_var = torch.clamp(log_var, min=-5.0, max=5.0) # Clip per stabilitÃ  numerica\n",
                "    precision = torch.exp(-log_var)\n",
                "    return precision * task_loss + log_var\n",
                "\n",
                "# CALCOLO LOSS CON MASCHERE\n",
                "# Qui gestiamo i \"Dati Finti\" (Padding Labels).\n",
                "def compute_mtl_loss(outputs, batch, tox_weights, gamma):\n",
                "    total_loss = torch.tensor(0.0, device=outputs['toxicity'].device)\n",
                "    \n",
                "    # TASK 1: TOSSICITÃ€\n",
                "    tox_mask = batch['task_mask_tox']\n",
                "    if tox_mask.sum() > 0: # Se ci sono esempi tossici veri nel batch...\n",
                "        loss = focal_loss(outputs['toxicity'][tox_mask], batch['tox_label'][tox_mask], gamma, tox_weights)\n",
                "        total_loss += kendall_loss(loss, outputs['log_var_tox'])\n",
                "        \n",
                "    # TASK 2: EMOZIONI\n",
                "    # Usiamo Binary Cross Entropy (BCE) perchÃ© Ã¨ un problema Multi-Label (una frase puÃ² essere sia Triste che Arrabbiata)\n",
                "    emo_mask = batch['task_mask_emo']\n",
                "    if emo_mask.sum() > 0:\n",
                "        loss = F.binary_cross_entropy_with_logits(outputs['emotion'][emo_mask], batch['emo_label'][emo_mask])\n",
                "        total_loss += kendall_loss(loss, outputs['log_var_emo'])\n",
                "        \n",
                "    # TASK 3: SENTIMENT\n",
                "    sent_mask = batch['task_mask_sent']\n",
                "    if sent_mask.sum() > 0:\n",
                "        loss = focal_loss(outputs['sentiment'][sent_mask], batch['sent_label'][sent_mask], gamma)\n",
                "        total_loss += kendall_loss(loss, outputs['log_var_sent'])\n",
                "        \n",
                "    return total_loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. DATASET & COLLATOR\n",
                "# Gestione avanzata dei tensori.\n",
                "\n",
                "class BaseDataset(Dataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_len):\n",
                "        self.df = pd.read_csv(csv_path)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_len = max_len\n",
                "    def __len__(self): return len(self.df)\n",
                "    def encode(self, text):\n",
                "        # TEXT PADDING: Tronca a 128 o riempie con 0 fino a 128.\n",
                "        return self.tokenizer.encode_plus(\n",
                "            str(text), max_length=self.max_len, padding='max_length', \n",
                "            truncation=True, return_tensors='pt'\n",
                "        )\n",
                "\n",
                "class ToxicityDataset(BaseDataset):\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(),\n",
                "            'attention_mask': enc['attention_mask'].flatten(),\n",
                "            # DATI REALI\n",
                "            'tox_label': torch.tensor(int(row['label']), dtype=torch.long),\n",
                "            # DATI FINTI (Label Padding) - verranno ignorati dalla mask\n",
                "            'emo_label': torch.zeros(len(EMO_COLS)), \n",
                "            'sent_label': torch.tensor(-1, dtype=torch.long),\n",
                "            # PASSAPORTO\n",
                "            'task': 'toxicity'\n",
                "        }\n",
                "\n",
                "# (Emotion e Sentiment dataset omessi per brevitÃ , stessa logica...)\n",
                "# ...\n",
                "class EmotionDataset(BaseDataset):\n",
                "    def __init__(self, csv_path, tokenizer, max_len, emo_cols):\n",
                "        super().__init__(csv_path, tokenizer, max_len)\n",
                "        self.emo_cols = emo_cols\n",
                "        if 'label_sum' in self.df.columns:\n",
                "             self.df = self.df[self.df['label_sum'] > 0].reset_index(drop=True)\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(),\n",
                "            'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'tox_label': torch.tensor(-1, dtype=torch.long),\n",
                "            'emo_label': torch.tensor([float(row[c]) for c in self.emo_cols]),\n",
                "            'sent_label': torch.tensor(-1, dtype=torch.long),\n",
                "            'task': 'emotion'\n",
                "        }\n",
                "\n",
                "class SentimentDataset(BaseDataset):\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        enc = self.encode(row['text'])\n",
                "        return {\n",
                "            'input_ids': enc['input_ids'].flatten(),\n",
                "            'attention_mask': enc['attention_mask'].flatten(),\n",
                "            'tox_label': torch.tensor(-1, dtype=torch.long),\n",
                "            'emo_label': torch.zeros(len(EMO_COLS)),\n",
                "            'sent_label': torch.tensor(int(row['label']), dtype=torch.long),\n",
                "            'task': 'sentiment'\n",
                "        }\n",
                "\n",
                "def collate_fn(batch):\n",
                "    # Questo Ã¨ il 'vigile urbano' che smista il traffico nel batch\n",
                "    tasks = [x['task'] for x in batch]\n",
                "    return {\n",
                "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
                "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
                "        'tox_label': torch.stack([x['tox_label'] for x in batch]),\n",
                "        'emo_label': torch.stack([x['emo_label'] for x in batch]),\n",
                "        'sent_label': torch.stack([x['sent_label'] for x in batch]),\n",
                "        # CREAZIONE MASCHERE\n",
                "        # Trasforma i passaporti stringa in array booleani per la GPU\n",
                "        'task_mask_tox': torch.tensor([t == 'toxicity' for t in tasks], dtype=torch.bool), \n",
                "        'task_mask_emo': torch.tensor([t == 'emotion' for t in tasks], dtype=torch.bool),\n",
                "        'task_mask_sent': torch.tensor([t == 'sentiment' for t in tasks], dtype=torch.bool)\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. LOADERS\n",
                "tokenizer = BertTokenizer.from_pretrained(CONFIG['encoder'])\n",
                "\n",
                "# ConcatDataset crea un flusso misto (Interleaved Dataset).\n",
                "# Questo assicura che il modello non dimentichi un task mentre ne impara un altro.\n",
                "tox_train = ToxicityDataset(f'{DATA_DIR}/toxicity_train.csv', tokenizer, CONFIG['max_length'])\n",
                "emo_train = EmotionDataset(f'{DATA_DIR}/emotions_train.csv', tokenizer, CONFIG['max_length'], EMO_COLS)\n",
                "sent_train = SentimentDataset(f'{DATA_DIR}/sentiment_train.csv', tokenizer, CONFIG['max_length'])\n",
                "tox_val = ToxicityDataset(f'{DATA_DIR}/toxicity_val.csv', tokenizer, CONFIG['max_length'])\n",
                "\n",
                "train_set = ConcatDataset([tox_train, emo_train, sent_train])\n",
                "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
                "val_loader = DataLoader(tox_val, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
                "\n",
                "tox_weights = torch.tensor([0.75, 1.5], device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. TRAINING LOOP (The Engine)\n",
                "\n",
                "def train_epoch(model, loader, optimizer, scheduler, config, tox_weights):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    optimizer.zero_grad()\n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    \n",
                "    for step, batch in enumerate(pbar):\n",
                "        # SPOSTAMENTO SU GPU\n",
                "        for k, v in batch.items(): \n",
                "            if isinstance(v, torch.Tensor): batch[k] = v.to(device)\n",
                "        \n",
                "        # FORWARD\n",
                "        outputs = model(batch['input_ids'], batch['attention_mask'])\n",
                "        \n",
                "        # LOSS Calculation\n",
                "        loss = compute_mtl_loss(outputs, batch, tox_weights, config['focal_gamma'])\n",
                "        \n",
                "        # BACKWARD (Accumulato)\n",
                "        (loss / config['gradient_accumulation']).backward()\n",
                "        \n",
                "        # OPTIMIZER STEP (ogni N batch)\n",
                "        if (step + 1) % config['gradient_accumulation'] == 0:\n",
                "            nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Evita exploding gradients\n",
                "            optimizer.step()\n",
                "            scheduler.step()\n",
                "            optimizer.zero_grad()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        # LOGGING SIGMA (Incertezza)\n",
                "        if step % 50 == 0:\n",
                "            Ïƒ_t = torch.exp(0.5 * model.log_var_tox).item()\n",
                "            Ïƒ_e = torch.exp(0.5 * model.log_var_emo).item()\n",
                "            pbar.set_postfix({'loss': loss.item(), 'Ïƒ_tox': f'{Ïƒ_t:.2f}', 'Ïƒ_emo': f'{Ïƒ_e:.2f}'})\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    preds, labels = [], []\n",
                "    for batch in tqdm(loader, desc='Validating', leave=False):\n",
                "        ids = batch['input_ids'].to(device)\n",
                "        mask = batch['attention_mask'].to(device)\n",
                "        lbl = batch['tox_label'].to(device)\n",
                "        out = model(ids, mask)\n",
                "        preds.extend(out['toxicity'].argmax(1).cpu().numpy())\n",
                "        labels.extend(lbl.cpu().numpy())\n",
                "    \n",
                "    # MACRO F1: La metrica piÃ¹ onesta per classi sbilanciate.\n",
                "    # Calcola F1 per 'Non Tossico' e F1 per 'Tossico' separatamente e ne fa la media.\n",
                "    return f1_score(labels, preds, average='macro')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. OPTIMIZER: Differential Learning Rates\n",
                "\n",
                "model = AURA_CERBERUS(CONFIG).to(device)\n",
                "\n",
                "optimizer = torch.optim.AdamW([\n",
                "    # LAYER 1: BERT BACKBONE\n",
                "    # LR molto basso (2e-5). Deve solo raffinare le sue conoscenze.\n",
                "    {'params': model.bert.parameters(), 'lr': CONFIG['lr_bert']},\n",
                "    \n",
                "    # LAYER 2: HEADS E SIGMA\n",
                "    # LR piÃ¹ alto (5e-5). Devono imparare velocemente da zero.\n",
                "    {'params': model.toxicity_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': model.emotion_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': model.sentiment_head.parameters(), 'lr': CONFIG['lr_heads']},\n",
                "    {'params': [model.log_var_tox, model.log_var_emo, model.log_var_sent], 'lr': CONFIG['lr_heads']}\n",
                "], weight_decay=CONFIG['weight_decay'])\n",
                "\n",
                "total_steps = len(train_loader) * CONFIG['epochs'] // CONFIG['gradient_accumulation']\n",
                "scheduler = OneCycleLR(optimizer,\n",
                "    max_lr=[CONFIG['lr_bert']] + [CONFIG['lr_heads']]*4,\n",
                "    total_steps=total_steps, pct_start=CONFIG['warmup_ratio'], anneal_strategy='cos')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. EXECUTION\n",
                "\n",
                "best_val_f1 = 0.0\n",
                "history = {'loss': [], 'val_f1': [], 'sigma_tox': [], 'sigma_emo': [], 'sigma_sent': []}\n",
                "\n",
                "for epoch in range(1, CONFIG['epochs'] + 1):\n",
                "    avg_loss = train_epoch(model, train_loader, optimizer, scheduler, CONFIG, tox_weights)\n",
                "    val_f1 = validate(model, val_loader)\n",
                "    \n",
                "    # Monitoriamo le Incertezze (Sigma)\n",
                "    # Se Sigma sale -> Il modello trova il task difficile.\n",
                "    # Se Sigma scende -> Il modello sta imparando.\n",
                "    Ïƒ_tox = torch.exp(0.5 * model.log_var_tox).item()\n",
                "    Ïƒ_emo = torch.exp(0.5 * model.log_var_emo).item()\n",
                "    Ïƒ_sent = torch.exp(0.5 * model.log_var_sent).item()\n",
                "    \n",
                "    history['loss'].append(avg_loss)\n",
                "    history['val_f1'].append(val_f1)\n",
                "    history['sigma_tox'].append(Ïƒ_tox)\n",
                "    history['sigma_emo'].append(Ïƒ_emo)\n",
                "    history['sigma_sent'].append(Ïƒ_sent)\n",
                "    \n",
                "    print(f'Epoch {epoch}: Loss={avg_loss:.4f}, Val F1={val_f1:.4f}')\n",
                "    print(f'Uncertainties: Tox={Ïƒ_tox:.2f}, Emo={Ïƒ_emo:.2f}, Sent={Ïƒ_sent:.2f}')\n",
                "    \n",
                "    if val_f1 > best_val_f1:\n",
                "        best_val_f1 = val_f1\n",
                "        torch.save(model.state_dict(), 'aura_cerberus_best.pt')\n",
                "        print('>>> NEW BEST MODEL <<<')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. VISUALIZZAZIONE\n",
                "import matplotlib.pyplot as plt\n",
                "plt.figure(figsize=(15, 5))\n",
                "\n",
                "# Loss\n",
                "plt.subplot(1, 3, 1)\n",
                "plt.plot(history['loss'], marker='o')\n",
                "plt.title('Total Loss (Weighted)')\n",
                "plt.grid()\n",
                "\n",
                "# Validation F1\n",
                "plt.subplot(1, 3, 2)\n",
                "plt.plot(history['val_f1'], marker='s', color='orange')\n",
                "plt.title('Toxicity F1 Score (Macro)')\n",
                "plt.grid()\n",
                "\n",
                "# Task Uncertainties\n",
                "plt.subplot(1, 3, 3)\n",
                "plt.plot(history['sigma_tox'], label='Tox')\n",
                "plt.plot(history['sigma_emo'], label='Emo')\n",
                "plt.plot(history['sigma_sent'], label='Sent')\n",
                "plt.title('Homoscedastic Uncertainty (Sigma)')\n",
                "plt.legend()\n",
                "plt.grid()\n",
                "\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}